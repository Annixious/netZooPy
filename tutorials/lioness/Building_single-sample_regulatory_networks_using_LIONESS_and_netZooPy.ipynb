{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building single-sample regulatory networks using LIONESS and netZooPy\n",
    "### Author: \n",
    "Qi (Alex) Song*.\n",
    "\n",
    "*Channing division of network medicine, Brigham's and Women hospital and Harvard Medical School, Boston, MA. (qi.song@channing.harvard.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this tutorial, we will briefly walk through the steps to perform analysis with Lioness algorithm using netZooPy package. Lioness is an algorithm for estimating sample-specific gene regualtory networks in a population.  LIONESS infers individual sample networks by applying linear interpolation to the predictions made by existing aggregate network inference approaches [1]. In this tutorial, we will use Panda as our basic network inference apporach to build sample-specific networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installation of netZooPy.\n",
    "netZooPy comes with full support for Lioness algorithm. netZooPy can be installed through `pip` command. For more details, please refer to the installation guide at netZooPy documentation site [here](https://netzoopy.readthedocs.io/en/latest/install/index.html).    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load required modules\n",
    "We will need `Panda` and `Lioness` python classes from netZooPy package. We will also need `read_csv()` function from `pandas` package for demonstrating the input data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netZooPy.panda import Panda\n",
    "from netZooPy.lioness import Lioness\n",
    "from netZooPy.lioness.analyze_lioness import AnalyzeLioness\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the three data sets to get a sense about what the inputs look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyExpressionData.txt',header=None, index_col = 0, sep = \"\\t\")\n",
    "motif_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyMotifData.txt',header=None, sep = \"\\t\")\n",
    "ppi_data = pd.read_csv('~/netZooPy/tests/puma/ToyData/ToyPPIData.txt',header=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expression data is a matrix where rows are genes and columns are samples.There are 1000 genes and 50 samples in this expression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AACSL</th>\n",
       "      <td>0.141431</td>\n",
       "      <td>-4.153056</td>\n",
       "      <td>2.854971</td>\n",
       "      <td>0.413670</td>\n",
       "      <td>1.082093</td>\n",
       "      <td>1.882361</td>\n",
       "      <td>1.450223</td>\n",
       "      <td>2.130209</td>\n",
       "      <td>0.548923</td>\n",
       "      <td>0.583043</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.551870</td>\n",
       "      <td>-6.645621</td>\n",
       "      <td>-3.970460</td>\n",
       "      <td>-2.041915</td>\n",
       "      <td>0.811989</td>\n",
       "      <td>0.979641</td>\n",
       "      <td>0.063161</td>\n",
       "      <td>3.652624</td>\n",
       "      <td>-2.387639</td>\n",
       "      <td>0.929521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAK1</th>\n",
       "      <td>3.528478</td>\n",
       "      <td>-0.949701</td>\n",
       "      <td>1.039986</td>\n",
       "      <td>-1.618816</td>\n",
       "      <td>-1.228012</td>\n",
       "      <td>-0.171763</td>\n",
       "      <td>-2.822020</td>\n",
       "      <td>0.047464</td>\n",
       "      <td>-0.456019</td>\n",
       "      <td>1.134087</td>\n",
       "      <td>...</td>\n",
       "      <td>2.808942</td>\n",
       "      <td>-0.035529</td>\n",
       "      <td>0.473384</td>\n",
       "      <td>-1.971053</td>\n",
       "      <td>1.759803</td>\n",
       "      <td>3.151289</td>\n",
       "      <td>-5.189503</td>\n",
       "      <td>-0.233187</td>\n",
       "      <td>0.349614</td>\n",
       "      <td>0.704183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCA17P</th>\n",
       "      <td>-2.597842</td>\n",
       "      <td>3.970710</td>\n",
       "      <td>-2.809212</td>\n",
       "      <td>0.474679</td>\n",
       "      <td>-2.714377</td>\n",
       "      <td>-0.474146</td>\n",
       "      <td>-6.738092</td>\n",
       "      <td>-2.811364</td>\n",
       "      <td>-1.017466</td>\n",
       "      <td>-1.646993</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.324379</td>\n",
       "      <td>-0.999586</td>\n",
       "      <td>0.987566</td>\n",
       "      <td>2.591347</td>\n",
       "      <td>-1.445705</td>\n",
       "      <td>-2.788339</td>\n",
       "      <td>2.295727</td>\n",
       "      <td>0.953828</td>\n",
       "      <td>-1.094031</td>\n",
       "      <td>-2.104951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCB8</th>\n",
       "      <td>0.352052</td>\n",
       "      <td>-1.866545</td>\n",
       "      <td>-0.007765</td>\n",
       "      <td>3.289632</td>\n",
       "      <td>2.675149</td>\n",
       "      <td>3.819294</td>\n",
       "      <td>0.668285</td>\n",
       "      <td>2.608310</td>\n",
       "      <td>3.342104</td>\n",
       "      <td>-2.792534</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.666569</td>\n",
       "      <td>-0.074404</td>\n",
       "      <td>4.630231</td>\n",
       "      <td>0.883074</td>\n",
       "      <td>-1.573444</td>\n",
       "      <td>4.909868</td>\n",
       "      <td>0.866853</td>\n",
       "      <td>2.374492</td>\n",
       "      <td>1.410069</td>\n",
       "      <td>-3.828003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCC1</th>\n",
       "      <td>-4.638927</td>\n",
       "      <td>2.440799</td>\n",
       "      <td>-1.655580</td>\n",
       "      <td>0.506424</td>\n",
       "      <td>3.289914</td>\n",
       "      <td>2.460479</td>\n",
       "      <td>-1.003678</td>\n",
       "      <td>1.537393</td>\n",
       "      <td>-1.342323</td>\n",
       "      <td>-1.003316</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.375289</td>\n",
       "      <td>-3.214583</td>\n",
       "      <td>5.531917</td>\n",
       "      <td>-1.693335</td>\n",
       "      <td>1.506472</td>\n",
       "      <td>1.020980</td>\n",
       "      <td>4.933972</td>\n",
       "      <td>2.268159</td>\n",
       "      <td>-0.734398</td>\n",
       "      <td>-2.618825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCF3</th>\n",
       "      <td>3.822458</td>\n",
       "      <td>0.241117</td>\n",
       "      <td>-0.629730</td>\n",
       "      <td>-6.448074</td>\n",
       "      <td>-2.221022</td>\n",
       "      <td>0.559189</td>\n",
       "      <td>-0.817507</td>\n",
       "      <td>0.404023</td>\n",
       "      <td>-2.105254</td>\n",
       "      <td>6.397036</td>\n",
       "      <td>...</td>\n",
       "      <td>0.416917</td>\n",
       "      <td>-1.061398</td>\n",
       "      <td>4.559200</td>\n",
       "      <td>-1.014232</td>\n",
       "      <td>0.372768</td>\n",
       "      <td>1.563113</td>\n",
       "      <td>2.331843</td>\n",
       "      <td>7.556858</td>\n",
       "      <td>1.705383</td>\n",
       "      <td>-1.658708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABCG1</th>\n",
       "      <td>0.562681</td>\n",
       "      <td>0.348409</td>\n",
       "      <td>-1.800319</td>\n",
       "      <td>-3.869393</td>\n",
       "      <td>6.949607</td>\n",
       "      <td>-0.700023</td>\n",
       "      <td>0.613762</td>\n",
       "      <td>-0.194774</td>\n",
       "      <td>0.477825</td>\n",
       "      <td>1.010311</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.687904</td>\n",
       "      <td>-2.232982</td>\n",
       "      <td>-3.412368</td>\n",
       "      <td>-2.943314</td>\n",
       "      <td>-2.985734</td>\n",
       "      <td>1.551215</td>\n",
       "      <td>1.133610</td>\n",
       "      <td>0.454167</td>\n",
       "      <td>-2.928591</td>\n",
       "      <td>2.399636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACOT2</th>\n",
       "      <td>2.411224</td>\n",
       "      <td>-4.461962</td>\n",
       "      <td>2.932685</td>\n",
       "      <td>-3.482673</td>\n",
       "      <td>1.622149</td>\n",
       "      <td>1.970574</td>\n",
       "      <td>3.783328</td>\n",
       "      <td>2.018564</td>\n",
       "      <td>-4.157278</td>\n",
       "      <td>-0.454352</td>\n",
       "      <td>...</td>\n",
       "      <td>3.496161</td>\n",
       "      <td>-4.542483</td>\n",
       "      <td>1.584941</td>\n",
       "      <td>-0.167026</td>\n",
       "      <td>-1.091618</td>\n",
       "      <td>0.266757</td>\n",
       "      <td>0.242896</td>\n",
       "      <td>-1.986338</td>\n",
       "      <td>1.453353</td>\n",
       "      <td>-3.237589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACSF2</th>\n",
       "      <td>0.452929</td>\n",
       "      <td>2.932288</td>\n",
       "      <td>2.580745</td>\n",
       "      <td>0.576073</td>\n",
       "      <td>1.568796</td>\n",
       "      <td>-3.090774</td>\n",
       "      <td>-2.057883</td>\n",
       "      <td>-5.726421</td>\n",
       "      <td>-5.013427</td>\n",
       "      <td>1.913956</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.264008</td>\n",
       "      <td>0.828671</td>\n",
       "      <td>0.139710</td>\n",
       "      <td>-0.038387</td>\n",
       "      <td>1.705630</td>\n",
       "      <td>5.832881</td>\n",
       "      <td>-0.075640</td>\n",
       "      <td>-3.998545</td>\n",
       "      <td>-0.008504</td>\n",
       "      <td>-0.974825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAM2</th>\n",
       "      <td>0.355641</td>\n",
       "      <td>-0.011333</td>\n",
       "      <td>3.167627</td>\n",
       "      <td>-2.292000</td>\n",
       "      <td>0.309439</td>\n",
       "      <td>1.242176</td>\n",
       "      <td>-0.447120</td>\n",
       "      <td>0.028113</td>\n",
       "      <td>-3.598405</td>\n",
       "      <td>1.442208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352784</td>\n",
       "      <td>2.091205</td>\n",
       "      <td>-1.578376</td>\n",
       "      <td>5.417486</td>\n",
       "      <td>1.012646</td>\n",
       "      <td>-2.214839</td>\n",
       "      <td>-2.154928</td>\n",
       "      <td>2.582198</td>\n",
       "      <td>-0.176870</td>\n",
       "      <td>1.551844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAMTS10</th>\n",
       "      <td>-6.435335</td>\n",
       "      <td>-3.484197</td>\n",
       "      <td>-1.334098</td>\n",
       "      <td>-6.257098</td>\n",
       "      <td>1.414799</td>\n",
       "      <td>1.847578</td>\n",
       "      <td>-0.854010</td>\n",
       "      <td>-1.351184</td>\n",
       "      <td>-5.122214</td>\n",
       "      <td>2.551296</td>\n",
       "      <td>...</td>\n",
       "      <td>4.033988</td>\n",
       "      <td>3.940538</td>\n",
       "      <td>3.105636</td>\n",
       "      <td>-0.830551</td>\n",
       "      <td>0.912196</td>\n",
       "      <td>-0.126224</td>\n",
       "      <td>-2.345661</td>\n",
       "      <td>7.605495</td>\n",
       "      <td>1.028245</td>\n",
       "      <td>-3.296487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADAMTS8</th>\n",
       "      <td>-1.500518</td>\n",
       "      <td>-1.781252</td>\n",
       "      <td>0.390679</td>\n",
       "      <td>-4.498557</td>\n",
       "      <td>-2.950740</td>\n",
       "      <td>2.187399</td>\n",
       "      <td>-4.911543</td>\n",
       "      <td>-2.987357</td>\n",
       "      <td>1.188654</td>\n",
       "      <td>-4.559955</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.554750</td>\n",
       "      <td>-1.501601</td>\n",
       "      <td>-4.605737</td>\n",
       "      <td>-2.175497</td>\n",
       "      <td>3.599596</td>\n",
       "      <td>2.921266</td>\n",
       "      <td>3.366518</td>\n",
       "      <td>-3.623441</td>\n",
       "      <td>6.498009</td>\n",
       "      <td>0.601931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCY6</th>\n",
       "      <td>0.239552</td>\n",
       "      <td>3.663878</td>\n",
       "      <td>-2.478119</td>\n",
       "      <td>-1.249731</td>\n",
       "      <td>-4.508629</td>\n",
       "      <td>-1.559120</td>\n",
       "      <td>-1.018709</td>\n",
       "      <td>0.820547</td>\n",
       "      <td>-0.839912</td>\n",
       "      <td>-2.340473</td>\n",
       "      <td>...</td>\n",
       "      <td>4.726817</td>\n",
       "      <td>-2.105939</td>\n",
       "      <td>2.066263</td>\n",
       "      <td>1.598242</td>\n",
       "      <td>-4.352578</td>\n",
       "      <td>2.937616</td>\n",
       "      <td>0.838953</td>\n",
       "      <td>0.873717</td>\n",
       "      <td>2.164306</td>\n",
       "      <td>-7.228367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADCYAP1R1</th>\n",
       "      <td>0.458076</td>\n",
       "      <td>5.646495</td>\n",
       "      <td>1.054934</td>\n",
       "      <td>2.420345</td>\n",
       "      <td>1.272796</td>\n",
       "      <td>2.603719</td>\n",
       "      <td>1.006353</td>\n",
       "      <td>5.376351</td>\n",
       "      <td>-0.302532</td>\n",
       "      <td>6.419028</td>\n",
       "      <td>...</td>\n",
       "      <td>4.236462</td>\n",
       "      <td>-1.691084</td>\n",
       "      <td>-0.661785</td>\n",
       "      <td>0.803437</td>\n",
       "      <td>-0.274287</td>\n",
       "      <td>-0.872806</td>\n",
       "      <td>1.080127</td>\n",
       "      <td>-1.647711</td>\n",
       "      <td>-7.314351</td>\n",
       "      <td>-0.454197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADH1B</th>\n",
       "      <td>-2.498004</td>\n",
       "      <td>4.363269</td>\n",
       "      <td>3.047640</td>\n",
       "      <td>-2.887588</td>\n",
       "      <td>-0.382138</td>\n",
       "      <td>-2.791821</td>\n",
       "      <td>-3.554774</td>\n",
       "      <td>-4.072156</td>\n",
       "      <td>0.171069</td>\n",
       "      <td>-4.810303</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.070164</td>\n",
       "      <td>-2.778129</td>\n",
       "      <td>-0.110080</td>\n",
       "      <td>-3.957719</td>\n",
       "      <td>-1.410711</td>\n",
       "      <td>0.695792</td>\n",
       "      <td>2.330447</td>\n",
       "      <td>-3.053955</td>\n",
       "      <td>-4.306005</td>\n",
       "      <td>1.839237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADH6</th>\n",
       "      <td>-0.224751</td>\n",
       "      <td>-4.080510</td>\n",
       "      <td>3.996458</td>\n",
       "      <td>2.259859</td>\n",
       "      <td>-0.291146</td>\n",
       "      <td>0.586628</td>\n",
       "      <td>3.458583</td>\n",
       "      <td>3.730949</td>\n",
       "      <td>5.144894</td>\n",
       "      <td>-0.546617</td>\n",
       "      <td>...</td>\n",
       "      <td>3.345719</td>\n",
       "      <td>1.916228</td>\n",
       "      <td>4.300311</td>\n",
       "      <td>-4.651193</td>\n",
       "      <td>0.775120</td>\n",
       "      <td>-3.972906</td>\n",
       "      <td>-0.698086</td>\n",
       "      <td>-0.430629</td>\n",
       "      <td>-0.816764</td>\n",
       "      <td>0.763499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFF1</th>\n",
       "      <td>1.267301</td>\n",
       "      <td>-1.743443</td>\n",
       "      <td>-0.558082</td>\n",
       "      <td>0.711610</td>\n",
       "      <td>-1.599487</td>\n",
       "      <td>-4.036190</td>\n",
       "      <td>1.768361</td>\n",
       "      <td>3.132970</td>\n",
       "      <td>0.851968</td>\n",
       "      <td>-0.930023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079065</td>\n",
       "      <td>-7.419378</td>\n",
       "      <td>3.747206</td>\n",
       "      <td>-2.912467</td>\n",
       "      <td>-3.456261</td>\n",
       "      <td>2.962799</td>\n",
       "      <td>-3.951777</td>\n",
       "      <td>-4.874321</td>\n",
       "      <td>-0.339520</td>\n",
       "      <td>3.577241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGR3</th>\n",
       "      <td>2.689198</td>\n",
       "      <td>0.551704</td>\n",
       "      <td>4.595840</td>\n",
       "      <td>5.758756</td>\n",
       "      <td>2.143647</td>\n",
       "      <td>6.210038</td>\n",
       "      <td>3.802123</td>\n",
       "      <td>4.112670</td>\n",
       "      <td>3.055105</td>\n",
       "      <td>-2.853347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480619</td>\n",
       "      <td>-1.733525</td>\n",
       "      <td>-0.008265</td>\n",
       "      <td>4.462258</td>\n",
       "      <td>-8.505583</td>\n",
       "      <td>-0.109861</td>\n",
       "      <td>-1.752762</td>\n",
       "      <td>-0.309343</td>\n",
       "      <td>-0.280030</td>\n",
       "      <td>-3.342774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGRN</th>\n",
       "      <td>-3.803666</td>\n",
       "      <td>3.013388</td>\n",
       "      <td>0.094045</td>\n",
       "      <td>-3.100119</td>\n",
       "      <td>3.327980</td>\n",
       "      <td>-3.495524</td>\n",
       "      <td>3.172164</td>\n",
       "      <td>0.686780</td>\n",
       "      <td>1.848163</td>\n",
       "      <td>-4.764568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.059656</td>\n",
       "      <td>1.874475</td>\n",
       "      <td>2.730239</td>\n",
       "      <td>2.368229</td>\n",
       "      <td>0.895757</td>\n",
       "      <td>1.090120</td>\n",
       "      <td>3.247198</td>\n",
       "      <td>3.194017</td>\n",
       "      <td>1.101338</td>\n",
       "      <td>2.587900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AHR</th>\n",
       "      <td>-0.206353</td>\n",
       "      <td>5.031601</td>\n",
       "      <td>-6.772378</td>\n",
       "      <td>0.658805</td>\n",
       "      <td>-4.815799</td>\n",
       "      <td>2.131688</td>\n",
       "      <td>-0.616780</td>\n",
       "      <td>2.924277</td>\n",
       "      <td>-0.459491</td>\n",
       "      <td>1.096779</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.079071</td>\n",
       "      <td>7.787621</td>\n",
       "      <td>4.173739</td>\n",
       "      <td>-4.853682</td>\n",
       "      <td>1.384796</td>\n",
       "      <td>-2.567371</td>\n",
       "      <td>-4.304014</td>\n",
       "      <td>-2.440764</td>\n",
       "      <td>2.933068</td>\n",
       "      <td>3.258371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AIM1</th>\n",
       "      <td>-3.253885</td>\n",
       "      <td>-1.344785</td>\n",
       "      <td>0.778770</td>\n",
       "      <td>-3.178421</td>\n",
       "      <td>-2.241247</td>\n",
       "      <td>0.076789</td>\n",
       "      <td>0.705845</td>\n",
       "      <td>-3.178349</td>\n",
       "      <td>-3.290212</td>\n",
       "      <td>-4.055884</td>\n",
       "      <td>...</td>\n",
       "      <td>2.506882</td>\n",
       "      <td>0.238419</td>\n",
       "      <td>0.795984</td>\n",
       "      <td>0.555804</td>\n",
       "      <td>3.693480</td>\n",
       "      <td>3.648031</td>\n",
       "      <td>-2.599992</td>\n",
       "      <td>-1.235266</td>\n",
       "      <td>-1.932245</td>\n",
       "      <td>1.393250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AJAP1</th>\n",
       "      <td>-0.671359</td>\n",
       "      <td>-1.453216</td>\n",
       "      <td>1.583014</td>\n",
       "      <td>2.223739</td>\n",
       "      <td>6.980131</td>\n",
       "      <td>-3.211525</td>\n",
       "      <td>-2.033678</td>\n",
       "      <td>-3.638395</td>\n",
       "      <td>1.529547</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.760967</td>\n",
       "      <td>0.402947</td>\n",
       "      <td>2.047986</td>\n",
       "      <td>9.711038</td>\n",
       "      <td>-5.583309</td>\n",
       "      <td>-1.635951</td>\n",
       "      <td>-2.851895</td>\n",
       "      <td>-4.855163</td>\n",
       "      <td>-1.584887</td>\n",
       "      <td>1.328536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAP10</th>\n",
       "      <td>2.111122</td>\n",
       "      <td>1.642002</td>\n",
       "      <td>1.268021</td>\n",
       "      <td>0.915710</td>\n",
       "      <td>-2.686503</td>\n",
       "      <td>0.648780</td>\n",
       "      <td>3.363863</td>\n",
       "      <td>-1.063846</td>\n",
       "      <td>-1.164144</td>\n",
       "      <td>2.371818</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.532954</td>\n",
       "      <td>0.104894</td>\n",
       "      <td>2.236829</td>\n",
       "      <td>1.582274</td>\n",
       "      <td>1.728103</td>\n",
       "      <td>2.456019</td>\n",
       "      <td>-2.895845</td>\n",
       "      <td>-3.776638</td>\n",
       "      <td>-2.602940</td>\n",
       "      <td>1.132555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKAP8L</th>\n",
       "      <td>-2.076503</td>\n",
       "      <td>5.121783</td>\n",
       "      <td>0.120849</td>\n",
       "      <td>4.835900</td>\n",
       "      <td>4.528920</td>\n",
       "      <td>-4.010499</td>\n",
       "      <td>-0.570129</td>\n",
       "      <td>-1.910726</td>\n",
       "      <td>-5.247164</td>\n",
       "      <td>-3.205733</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.448418</td>\n",
       "      <td>-0.832548</td>\n",
       "      <td>2.501409</td>\n",
       "      <td>4.901713</td>\n",
       "      <td>1.762507</td>\n",
       "      <td>-3.798529</td>\n",
       "      <td>-2.656816</td>\n",
       "      <td>1.570357</td>\n",
       "      <td>1.879632</td>\n",
       "      <td>4.546102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKIRIN2</th>\n",
       "      <td>0.182562</td>\n",
       "      <td>-2.814136</td>\n",
       "      <td>-1.704658</td>\n",
       "      <td>-2.635369</td>\n",
       "      <td>-2.622177</td>\n",
       "      <td>-2.169776</td>\n",
       "      <td>3.083701</td>\n",
       "      <td>-4.855234</td>\n",
       "      <td>-1.960743</td>\n",
       "      <td>-1.231475</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.941039</td>\n",
       "      <td>-4.338502</td>\n",
       "      <td>-6.590048</td>\n",
       "      <td>-6.723866</td>\n",
       "      <td>2.562907</td>\n",
       "      <td>-4.462658</td>\n",
       "      <td>-3.158130</td>\n",
       "      <td>-1.700410</td>\n",
       "      <td>-3.032855</td>\n",
       "      <td>-1.274919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AKT1</th>\n",
       "      <td>-0.600257</td>\n",
       "      <td>-1.156250</td>\n",
       "      <td>4.269575</td>\n",
       "      <td>-2.652594</td>\n",
       "      <td>1.637202</td>\n",
       "      <td>6.342937</td>\n",
       "      <td>2.674969</td>\n",
       "      <td>-0.104879</td>\n",
       "      <td>-0.478754</td>\n",
       "      <td>-0.124468</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399697</td>\n",
       "      <td>-2.622513</td>\n",
       "      <td>1.185376</td>\n",
       "      <td>4.711334</td>\n",
       "      <td>-1.332219</td>\n",
       "      <td>-2.756439</td>\n",
       "      <td>3.066091</td>\n",
       "      <td>3.494977</td>\n",
       "      <td>-2.954487</td>\n",
       "      <td>-1.335418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALG14</th>\n",
       "      <td>1.457071</td>\n",
       "      <td>-1.123955</td>\n",
       "      <td>1.168836</td>\n",
       "      <td>-4.140685</td>\n",
       "      <td>-0.767143</td>\n",
       "      <td>2.490318</td>\n",
       "      <td>3.910973</td>\n",
       "      <td>1.030674</td>\n",
       "      <td>3.085856</td>\n",
       "      <td>-1.179364</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.414906</td>\n",
       "      <td>-1.151661</td>\n",
       "      <td>3.431068</td>\n",
       "      <td>2.462015</td>\n",
       "      <td>-0.172023</td>\n",
       "      <td>4.241700</td>\n",
       "      <td>2.498942</td>\n",
       "      <td>1.538316</td>\n",
       "      <td>4.271883</td>\n",
       "      <td>9.790623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALPPL2</th>\n",
       "      <td>0.278612</td>\n",
       "      <td>-3.347032</td>\n",
       "      <td>-1.047794</td>\n",
       "      <td>-3.167413</td>\n",
       "      <td>-0.413878</td>\n",
       "      <td>-1.187347</td>\n",
       "      <td>-2.064943</td>\n",
       "      <td>1.779923</td>\n",
       "      <td>-4.103784</td>\n",
       "      <td>-3.772628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.636912</td>\n",
       "      <td>-0.258916</td>\n",
       "      <td>-4.925722</td>\n",
       "      <td>0.192451</td>\n",
       "      <td>3.438164</td>\n",
       "      <td>0.687503</td>\n",
       "      <td>0.757951</td>\n",
       "      <td>-1.844243</td>\n",
       "      <td>-4.572469</td>\n",
       "      <td>0.661904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMDHD1</th>\n",
       "      <td>1.442675</td>\n",
       "      <td>2.435977</td>\n",
       "      <td>-2.655512</td>\n",
       "      <td>-0.262620</td>\n",
       "      <td>-0.133281</td>\n",
       "      <td>0.071852</td>\n",
       "      <td>-0.937521</td>\n",
       "      <td>3.380939</td>\n",
       "      <td>1.799345</td>\n",
       "      <td>2.822316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.361554</td>\n",
       "      <td>-3.860945</td>\n",
       "      <td>2.688930</td>\n",
       "      <td>-1.500954</td>\n",
       "      <td>-2.484701</td>\n",
       "      <td>-4.613864</td>\n",
       "      <td>2.601704</td>\n",
       "      <td>1.524390</td>\n",
       "      <td>-0.309889</td>\n",
       "      <td>6.049197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMIGO1</th>\n",
       "      <td>-0.632069</td>\n",
       "      <td>-4.026140</td>\n",
       "      <td>-0.023985</td>\n",
       "      <td>1.703159</td>\n",
       "      <td>2.216783</td>\n",
       "      <td>-2.408620</td>\n",
       "      <td>2.931212</td>\n",
       "      <td>-3.947217</td>\n",
       "      <td>0.862303</td>\n",
       "      <td>-2.412431</td>\n",
       "      <td>...</td>\n",
       "      <td>1.330087</td>\n",
       "      <td>-0.709538</td>\n",
       "      <td>4.998298</td>\n",
       "      <td>5.434680</td>\n",
       "      <td>0.447682</td>\n",
       "      <td>0.431151</td>\n",
       "      <td>0.342613</td>\n",
       "      <td>0.362589</td>\n",
       "      <td>0.124003</td>\n",
       "      <td>-4.286540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YY1</th>\n",
       "      <td>1.955807</td>\n",
       "      <td>0.441289</td>\n",
       "      <td>-4.025722</td>\n",
       "      <td>3.241794</td>\n",
       "      <td>-1.471559</td>\n",
       "      <td>2.546710</td>\n",
       "      <td>-3.392705</td>\n",
       "      <td>-0.801326</td>\n",
       "      <td>8.747804</td>\n",
       "      <td>-0.891846</td>\n",
       "      <td>...</td>\n",
       "      <td>3.968216</td>\n",
       "      <td>-4.094314</td>\n",
       "      <td>-0.874615</td>\n",
       "      <td>1.884760</td>\n",
       "      <td>0.013866</td>\n",
       "      <td>0.810367</td>\n",
       "      <td>-1.970884</td>\n",
       "      <td>-2.359943</td>\n",
       "      <td>4.105038</td>\n",
       "      <td>-1.566941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBTB25</th>\n",
       "      <td>-0.465745</td>\n",
       "      <td>2.731525</td>\n",
       "      <td>0.583832</td>\n",
       "      <td>0.952136</td>\n",
       "      <td>3.981221</td>\n",
       "      <td>-6.242749</td>\n",
       "      <td>0.274338</td>\n",
       "      <td>-1.281054</td>\n",
       "      <td>3.280473</td>\n",
       "      <td>5.293598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832881</td>\n",
       "      <td>-0.647692</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>-1.540829</td>\n",
       "      <td>-0.289737</td>\n",
       "      <td>-4.011846</td>\n",
       "      <td>-1.242774</td>\n",
       "      <td>1.763224</td>\n",
       "      <td>-4.739805</td>\n",
       "      <td>4.830278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZCCHC6</th>\n",
       "      <td>2.616518</td>\n",
       "      <td>3.725200</td>\n",
       "      <td>-3.604447</td>\n",
       "      <td>0.953095</td>\n",
       "      <td>-1.414919</td>\n",
       "      <td>-1.686620</td>\n",
       "      <td>1.115142</td>\n",
       "      <td>-1.938877</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>-5.008178</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148678</td>\n",
       "      <td>-3.049896</td>\n",
       "      <td>-7.382941</td>\n",
       "      <td>-0.647739</td>\n",
       "      <td>-1.921551</td>\n",
       "      <td>-1.810419</td>\n",
       "      <td>-4.233128</td>\n",
       "      <td>3.262738</td>\n",
       "      <td>-0.812537</td>\n",
       "      <td>3.522548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC15</th>\n",
       "      <td>0.287416</td>\n",
       "      <td>4.228220</td>\n",
       "      <td>-1.932585</td>\n",
       "      <td>1.280172</td>\n",
       "      <td>-0.361667</td>\n",
       "      <td>-3.245687</td>\n",
       "      <td>-4.729219</td>\n",
       "      <td>-3.941767</td>\n",
       "      <td>3.047753</td>\n",
       "      <td>0.983202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.802575</td>\n",
       "      <td>0.580516</td>\n",
       "      <td>-2.626980</td>\n",
       "      <td>1.650896</td>\n",
       "      <td>-0.486386</td>\n",
       "      <td>-1.465460</td>\n",
       "      <td>2.756558</td>\n",
       "      <td>1.240380</td>\n",
       "      <td>1.369940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC16</th>\n",
       "      <td>3.983208</td>\n",
       "      <td>-4.736054</td>\n",
       "      <td>1.431894</td>\n",
       "      <td>-0.534885</td>\n",
       "      <td>1.189214</td>\n",
       "      <td>2.050506</td>\n",
       "      <td>1.918995</td>\n",
       "      <td>-1.047002</td>\n",
       "      <td>4.168625</td>\n",
       "      <td>-1.529144</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.787969</td>\n",
       "      <td>-0.655874</td>\n",
       "      <td>3.368040</td>\n",
       "      <td>0.137512</td>\n",
       "      <td>-2.539163</td>\n",
       "      <td>3.851830</td>\n",
       "      <td>-0.946804</td>\n",
       "      <td>1.647447</td>\n",
       "      <td>1.379092</td>\n",
       "      <td>-5.007336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC2</th>\n",
       "      <td>-5.452097</td>\n",
       "      <td>-2.864681</td>\n",
       "      <td>6.239840</td>\n",
       "      <td>4.199863</td>\n",
       "      <td>-1.725535</td>\n",
       "      <td>-3.715479</td>\n",
       "      <td>-3.816561</td>\n",
       "      <td>-2.090490</td>\n",
       "      <td>2.274487</td>\n",
       "      <td>-3.430965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157227</td>\n",
       "      <td>-2.579084</td>\n",
       "      <td>0.958787</td>\n",
       "      <td>3.746606</td>\n",
       "      <td>-0.782082</td>\n",
       "      <td>0.901215</td>\n",
       "      <td>-4.032160</td>\n",
       "      <td>3.597801</td>\n",
       "      <td>-0.142247</td>\n",
       "      <td>-2.502663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC7</th>\n",
       "      <td>-1.438812</td>\n",
       "      <td>0.525433</td>\n",
       "      <td>1.646436</td>\n",
       "      <td>1.928655</td>\n",
       "      <td>3.225473</td>\n",
       "      <td>-2.751004</td>\n",
       "      <td>-2.989698</td>\n",
       "      <td>-5.601409</td>\n",
       "      <td>2.200852</td>\n",
       "      <td>1.691665</td>\n",
       "      <td>...</td>\n",
       "      <td>3.800466</td>\n",
       "      <td>5.007216</td>\n",
       "      <td>-6.883397</td>\n",
       "      <td>0.784845</td>\n",
       "      <td>0.651235</td>\n",
       "      <td>-0.521085</td>\n",
       "      <td>-4.866340</td>\n",
       "      <td>-0.103401</td>\n",
       "      <td>0.323577</td>\n",
       "      <td>0.927459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZDHHC9</th>\n",
       "      <td>-5.000444</td>\n",
       "      <td>-0.419128</td>\n",
       "      <td>-1.826055</td>\n",
       "      <td>-7.374587</td>\n",
       "      <td>0.431975</td>\n",
       "      <td>0.447008</td>\n",
       "      <td>0.498743</td>\n",
       "      <td>-1.178437</td>\n",
       "      <td>0.252175</td>\n",
       "      <td>-0.106776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178797</td>\n",
       "      <td>-0.721575</td>\n",
       "      <td>10.173612</td>\n",
       "      <td>-0.566138</td>\n",
       "      <td>1.952077</td>\n",
       "      <td>0.953530</td>\n",
       "      <td>-4.176484</td>\n",
       "      <td>-2.543994</td>\n",
       "      <td>-4.591273</td>\n",
       "      <td>1.720633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFAT</th>\n",
       "      <td>-2.931567</td>\n",
       "      <td>-1.559858</td>\n",
       "      <td>-1.483557</td>\n",
       "      <td>-1.574698</td>\n",
       "      <td>-0.874071</td>\n",
       "      <td>-2.881306</td>\n",
       "      <td>4.222129</td>\n",
       "      <td>-1.200172</td>\n",
       "      <td>-0.990453</td>\n",
       "      <td>5.082496</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.772075</td>\n",
       "      <td>3.798225</td>\n",
       "      <td>0.403502</td>\n",
       "      <td>-3.078181</td>\n",
       "      <td>4.188723</td>\n",
       "      <td>5.179436</td>\n",
       "      <td>2.422908</td>\n",
       "      <td>2.133740</td>\n",
       "      <td>-0.258086</td>\n",
       "      <td>-5.022865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFP62</th>\n",
       "      <td>-1.271041</td>\n",
       "      <td>-2.920962</td>\n",
       "      <td>-3.853412</td>\n",
       "      <td>0.748130</td>\n",
       "      <td>4.703092</td>\n",
       "      <td>3.214546</td>\n",
       "      <td>0.476954</td>\n",
       "      <td>0.005453</td>\n",
       "      <td>0.526652</td>\n",
       "      <td>1.999028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.526324</td>\n",
       "      <td>-0.229234</td>\n",
       "      <td>0.321802</td>\n",
       "      <td>1.108952</td>\n",
       "      <td>3.446266</td>\n",
       "      <td>-5.234739</td>\n",
       "      <td>1.145793</td>\n",
       "      <td>1.785877</td>\n",
       "      <td>6.950120</td>\n",
       "      <td>1.517959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZFR</th>\n",
       "      <td>-4.744726</td>\n",
       "      <td>4.316600</td>\n",
       "      <td>-4.468643</td>\n",
       "      <td>5.237806</td>\n",
       "      <td>1.316022</td>\n",
       "      <td>-0.874962</td>\n",
       "      <td>5.295222</td>\n",
       "      <td>-3.100937</td>\n",
       "      <td>-0.505455</td>\n",
       "      <td>-4.697889</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.011458</td>\n",
       "      <td>-2.350506</td>\n",
       "      <td>-6.253545</td>\n",
       "      <td>0.666908</td>\n",
       "      <td>1.212308</td>\n",
       "      <td>-0.688220</td>\n",
       "      <td>0.660413</td>\n",
       "      <td>-2.524871</td>\n",
       "      <td>0.319327</td>\n",
       "      <td>2.381417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMYM3</th>\n",
       "      <td>1.471251</td>\n",
       "      <td>0.652375</td>\n",
       "      <td>-1.932988</td>\n",
       "      <td>2.670401</td>\n",
       "      <td>-2.737013</td>\n",
       "      <td>0.057174</td>\n",
       "      <td>2.756968</td>\n",
       "      <td>1.152510</td>\n",
       "      <td>-2.647043</td>\n",
       "      <td>7.612432</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.551895</td>\n",
       "      <td>1.795703</td>\n",
       "      <td>1.051413</td>\n",
       "      <td>-1.358606</td>\n",
       "      <td>1.383776</td>\n",
       "      <td>-2.503557</td>\n",
       "      <td>3.385820</td>\n",
       "      <td>-0.745977</td>\n",
       "      <td>-0.627234</td>\n",
       "      <td>-1.546457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMYND17</th>\n",
       "      <td>4.619858</td>\n",
       "      <td>-3.475264</td>\n",
       "      <td>0.038139</td>\n",
       "      <td>4.313893</td>\n",
       "      <td>-1.061013</td>\n",
       "      <td>-1.304709</td>\n",
       "      <td>0.256868</td>\n",
       "      <td>0.419320</td>\n",
       "      <td>3.540630</td>\n",
       "      <td>3.040002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813865</td>\n",
       "      <td>5.155772</td>\n",
       "      <td>-5.260261</td>\n",
       "      <td>1.344561</td>\n",
       "      <td>-2.431417</td>\n",
       "      <td>4.107977</td>\n",
       "      <td>0.237943</td>\n",
       "      <td>-1.939147</td>\n",
       "      <td>-0.133882</td>\n",
       "      <td>3.983232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF124</th>\n",
       "      <td>3.330737</td>\n",
       "      <td>5.589767</td>\n",
       "      <td>-4.290391</td>\n",
       "      <td>-0.717922</td>\n",
       "      <td>0.811929</td>\n",
       "      <td>4.553090</td>\n",
       "      <td>-2.496052</td>\n",
       "      <td>1.730051</td>\n",
       "      <td>0.636785</td>\n",
       "      <td>0.818016</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.356317</td>\n",
       "      <td>-1.932498</td>\n",
       "      <td>-3.455120</td>\n",
       "      <td>-0.084919</td>\n",
       "      <td>-1.237713</td>\n",
       "      <td>3.506137</td>\n",
       "      <td>-1.885701</td>\n",
       "      <td>0.273054</td>\n",
       "      <td>-2.894267</td>\n",
       "      <td>-0.983415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF132</th>\n",
       "      <td>-2.168550</td>\n",
       "      <td>-2.412464</td>\n",
       "      <td>-3.149500</td>\n",
       "      <td>-2.440988</td>\n",
       "      <td>10.078211</td>\n",
       "      <td>1.902683</td>\n",
       "      <td>3.813724</td>\n",
       "      <td>-3.439067</td>\n",
       "      <td>1.787848</td>\n",
       "      <td>2.228660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.986353</td>\n",
       "      <td>-1.858777</td>\n",
       "      <td>8.075192</td>\n",
       "      <td>7.790025</td>\n",
       "      <td>-2.206606</td>\n",
       "      <td>6.022623</td>\n",
       "      <td>-0.591418</td>\n",
       "      <td>-5.578319</td>\n",
       "      <td>-6.060207</td>\n",
       "      <td>1.476551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF138</th>\n",
       "      <td>5.501906</td>\n",
       "      <td>-0.524335</td>\n",
       "      <td>-4.959888</td>\n",
       "      <td>1.751520</td>\n",
       "      <td>-1.605983</td>\n",
       "      <td>-1.376737</td>\n",
       "      <td>4.639871</td>\n",
       "      <td>2.583844</td>\n",
       "      <td>1.688727</td>\n",
       "      <td>-0.609936</td>\n",
       "      <td>...</td>\n",
       "      <td>2.701609</td>\n",
       "      <td>-1.919130</td>\n",
       "      <td>-1.581302</td>\n",
       "      <td>-2.705314</td>\n",
       "      <td>-1.775207</td>\n",
       "      <td>7.263195</td>\n",
       "      <td>-5.359276</td>\n",
       "      <td>-1.377007</td>\n",
       "      <td>-2.831965</td>\n",
       "      <td>2.458768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF283</th>\n",
       "      <td>-3.034221</td>\n",
       "      <td>2.111506</td>\n",
       "      <td>-0.719169</td>\n",
       "      <td>0.225898</td>\n",
       "      <td>-2.721097</td>\n",
       "      <td>1.546914</td>\n",
       "      <td>-3.126284</td>\n",
       "      <td>0.706536</td>\n",
       "      <td>2.476561</td>\n",
       "      <td>-1.737517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.950084</td>\n",
       "      <td>0.811290</td>\n",
       "      <td>7.654198</td>\n",
       "      <td>-0.602563</td>\n",
       "      <td>-0.485668</td>\n",
       "      <td>0.706387</td>\n",
       "      <td>0.637638</td>\n",
       "      <td>1.044513</td>\n",
       "      <td>1.126691</td>\n",
       "      <td>1.881236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF322B</th>\n",
       "      <td>-5.860660</td>\n",
       "      <td>6.129064</td>\n",
       "      <td>1.159615</td>\n",
       "      <td>-2.537294</td>\n",
       "      <td>-3.155169</td>\n",
       "      <td>-1.897060</td>\n",
       "      <td>1.876461</td>\n",
       "      <td>-6.812772</td>\n",
       "      <td>-0.795365</td>\n",
       "      <td>7.940787</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.441415</td>\n",
       "      <td>3.249069</td>\n",
       "      <td>-4.036189</td>\n",
       "      <td>0.334181</td>\n",
       "      <td>-2.639774</td>\n",
       "      <td>7.711171</td>\n",
       "      <td>5.733905</td>\n",
       "      <td>0.518429</td>\n",
       "      <td>-2.275928</td>\n",
       "      <td>-1.721815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF426</th>\n",
       "      <td>6.177740</td>\n",
       "      <td>-2.424839</td>\n",
       "      <td>-0.226201</td>\n",
       "      <td>2.234599</td>\n",
       "      <td>-1.458643</td>\n",
       "      <td>-3.694419</td>\n",
       "      <td>5.435978</td>\n",
       "      <td>2.113951</td>\n",
       "      <td>-0.548585</td>\n",
       "      <td>-0.587499</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810145</td>\n",
       "      <td>-0.509916</td>\n",
       "      <td>-1.958090</td>\n",
       "      <td>-2.925023</td>\n",
       "      <td>-1.296322</td>\n",
       "      <td>3.874413</td>\n",
       "      <td>5.068583</td>\n",
       "      <td>-7.030104</td>\n",
       "      <td>-1.901056</td>\n",
       "      <td>-1.727810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF451</th>\n",
       "      <td>3.249119</td>\n",
       "      <td>1.893897</td>\n",
       "      <td>1.717736</td>\n",
       "      <td>-0.659733</td>\n",
       "      <td>1.719676</td>\n",
       "      <td>3.294666</td>\n",
       "      <td>-0.481807</td>\n",
       "      <td>0.529062</td>\n",
       "      <td>-6.321060</td>\n",
       "      <td>-1.217092</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084075</td>\n",
       "      <td>2.081276</td>\n",
       "      <td>-2.132384</td>\n",
       "      <td>0.780504</td>\n",
       "      <td>-2.191865</td>\n",
       "      <td>-3.959255</td>\n",
       "      <td>-5.588040</td>\n",
       "      <td>-1.734270</td>\n",
       "      <td>6.236782</td>\n",
       "      <td>-4.415510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF480</th>\n",
       "      <td>-0.140409</td>\n",
       "      <td>-1.341108</td>\n",
       "      <td>0.018698</td>\n",
       "      <td>-1.008055</td>\n",
       "      <td>0.655969</td>\n",
       "      <td>-0.532847</td>\n",
       "      <td>1.634830</td>\n",
       "      <td>1.264774</td>\n",
       "      <td>1.879042</td>\n",
       "      <td>1.682834</td>\n",
       "      <td>...</td>\n",
       "      <td>2.166815</td>\n",
       "      <td>-0.138187</td>\n",
       "      <td>0.269705</td>\n",
       "      <td>2.945537</td>\n",
       "      <td>-6.374132</td>\n",
       "      <td>-4.319404</td>\n",
       "      <td>-1.417171</td>\n",
       "      <td>-3.086070</td>\n",
       "      <td>-4.755754</td>\n",
       "      <td>2.109595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF660</th>\n",
       "      <td>-5.134059</td>\n",
       "      <td>-2.887868</td>\n",
       "      <td>-5.682801</td>\n",
       "      <td>-4.584404</td>\n",
       "      <td>0.923337</td>\n",
       "      <td>-3.747658</td>\n",
       "      <td>2.117484</td>\n",
       "      <td>-5.564895</td>\n",
       "      <td>-2.821189</td>\n",
       "      <td>-1.254334</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.868436</td>\n",
       "      <td>-0.401334</td>\n",
       "      <td>1.951397</td>\n",
       "      <td>4.396688</td>\n",
       "      <td>4.859973</td>\n",
       "      <td>4.140445</td>\n",
       "      <td>2.204386</td>\n",
       "      <td>1.565369</td>\n",
       "      <td>1.391193</td>\n",
       "      <td>0.320414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF696</th>\n",
       "      <td>1.536629</td>\n",
       "      <td>-1.218671</td>\n",
       "      <td>-1.244199</td>\n",
       "      <td>-4.751498</td>\n",
       "      <td>-2.160287</td>\n",
       "      <td>1.626458</td>\n",
       "      <td>-0.817378</td>\n",
       "      <td>0.777225</td>\n",
       "      <td>-0.594485</td>\n",
       "      <td>0.026376</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.468312</td>\n",
       "      <td>-0.279803</td>\n",
       "      <td>5.001230</td>\n",
       "      <td>2.464151</td>\n",
       "      <td>-0.827188</td>\n",
       "      <td>-2.598237</td>\n",
       "      <td>-1.420780</td>\n",
       "      <td>2.221469</td>\n",
       "      <td>0.488647</td>\n",
       "      <td>-7.675989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF772</th>\n",
       "      <td>1.805567</td>\n",
       "      <td>4.059267</td>\n",
       "      <td>-7.479062</td>\n",
       "      <td>-0.471547</td>\n",
       "      <td>-0.378794</td>\n",
       "      <td>-5.751023</td>\n",
       "      <td>6.168965</td>\n",
       "      <td>2.467858</td>\n",
       "      <td>-0.962984</td>\n",
       "      <td>-2.699509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.734462</td>\n",
       "      <td>-4.688808</td>\n",
       "      <td>-1.912480</td>\n",
       "      <td>-3.396070</td>\n",
       "      <td>1.147704</td>\n",
       "      <td>-3.591208</td>\n",
       "      <td>-3.216311</td>\n",
       "      <td>2.329710</td>\n",
       "      <td>2.226982</td>\n",
       "      <td>-0.744886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF776</th>\n",
       "      <td>-0.739313</td>\n",
       "      <td>4.252302</td>\n",
       "      <td>-1.231494</td>\n",
       "      <td>1.584117</td>\n",
       "      <td>5.788495</td>\n",
       "      <td>0.016007</td>\n",
       "      <td>2.675882</td>\n",
       "      <td>3.143431</td>\n",
       "      <td>-1.671526</td>\n",
       "      <td>-1.230410</td>\n",
       "      <td>...</td>\n",
       "      <td>1.841290</td>\n",
       "      <td>3.199943</td>\n",
       "      <td>5.509921</td>\n",
       "      <td>-0.675689</td>\n",
       "      <td>-0.410618</td>\n",
       "      <td>2.722958</td>\n",
       "      <td>-2.857927</td>\n",
       "      <td>0.232340</td>\n",
       "      <td>0.472510</td>\n",
       "      <td>-0.760252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF826</th>\n",
       "      <td>-4.294209</td>\n",
       "      <td>-4.498573</td>\n",
       "      <td>2.786462</td>\n",
       "      <td>-1.588052</td>\n",
       "      <td>1.542248</td>\n",
       "      <td>3.222761</td>\n",
       "      <td>-0.005525</td>\n",
       "      <td>-4.711981</td>\n",
       "      <td>-0.580551</td>\n",
       "      <td>2.152243</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.023105</td>\n",
       "      <td>-1.675647</td>\n",
       "      <td>3.320340</td>\n",
       "      <td>2.108714</td>\n",
       "      <td>-5.961589</td>\n",
       "      <td>1.679723</td>\n",
       "      <td>-1.473783</td>\n",
       "      <td>1.871397</td>\n",
       "      <td>1.968646</td>\n",
       "      <td>-1.017821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF845</th>\n",
       "      <td>-1.661144</td>\n",
       "      <td>-6.986089</td>\n",
       "      <td>2.273928</td>\n",
       "      <td>-2.426933</td>\n",
       "      <td>-4.627002</td>\n",
       "      <td>-4.044476</td>\n",
       "      <td>-3.991184</td>\n",
       "      <td>-0.903110</td>\n",
       "      <td>1.558416</td>\n",
       "      <td>0.672473</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.990304</td>\n",
       "      <td>-1.806129</td>\n",
       "      <td>1.643056</td>\n",
       "      <td>1.932765</td>\n",
       "      <td>1.084221</td>\n",
       "      <td>-1.214410</td>\n",
       "      <td>-2.985126</td>\n",
       "      <td>-10.986240</td>\n",
       "      <td>0.159237</td>\n",
       "      <td>0.906706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZNF878</th>\n",
       "      <td>3.395504</td>\n",
       "      <td>-6.274497</td>\n",
       "      <td>0.455548</td>\n",
       "      <td>0.592239</td>\n",
       "      <td>-0.852212</td>\n",
       "      <td>1.373684</td>\n",
       "      <td>-2.638325</td>\n",
       "      <td>4.028651</td>\n",
       "      <td>-1.117790</td>\n",
       "      <td>-2.844150</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.858309</td>\n",
       "      <td>1.752461</td>\n",
       "      <td>1.630060</td>\n",
       "      <td>-0.079563</td>\n",
       "      <td>-0.545380</td>\n",
       "      <td>-2.694063</td>\n",
       "      <td>-0.535988</td>\n",
       "      <td>-0.038242</td>\n",
       "      <td>-1.353040</td>\n",
       "      <td>-1.713555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZSWIM3</th>\n",
       "      <td>-0.494841</td>\n",
       "      <td>2.840674</td>\n",
       "      <td>-3.816640</td>\n",
       "      <td>3.052187</td>\n",
       "      <td>4.979421</td>\n",
       "      <td>-5.889279</td>\n",
       "      <td>-6.640408</td>\n",
       "      <td>3.711903</td>\n",
       "      <td>-3.156978</td>\n",
       "      <td>0.475878</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.065756</td>\n",
       "      <td>1.069487</td>\n",
       "      <td>-1.682956</td>\n",
       "      <td>4.824188</td>\n",
       "      <td>-0.930484</td>\n",
       "      <td>-3.155203</td>\n",
       "      <td>-4.252213</td>\n",
       "      <td>-6.578125</td>\n",
       "      <td>-6.558922</td>\n",
       "      <td>-0.104826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZWILCH</th>\n",
       "      <td>0.694298</td>\n",
       "      <td>-2.725693</td>\n",
       "      <td>-1.752258</td>\n",
       "      <td>-1.789789</td>\n",
       "      <td>2.228141</td>\n",
       "      <td>-4.494592</td>\n",
       "      <td>1.233303</td>\n",
       "      <td>-1.748305</td>\n",
       "      <td>2.534993</td>\n",
       "      <td>0.819987</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.033845</td>\n",
       "      <td>2.497995</td>\n",
       "      <td>-3.744730</td>\n",
       "      <td>-2.536536</td>\n",
       "      <td>-3.004383</td>\n",
       "      <td>3.336575</td>\n",
       "      <td>-1.095170</td>\n",
       "      <td>-3.466885</td>\n",
       "      <td>1.519252</td>\n",
       "      <td>-0.729152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 1         2         3         4          5         6   \\\n",
       "0                                                                        \n",
       "AACSL      0.141431 -4.153056  2.854971  0.413670   1.082093  1.882361   \n",
       "AAK1       3.528478 -0.949701  1.039986 -1.618816  -1.228012 -0.171763   \n",
       "ABCA17P   -2.597842  3.970710 -2.809212  0.474679  -2.714377 -0.474146   \n",
       "ABCB8      0.352052 -1.866545 -0.007765  3.289632   2.675149  3.819294   \n",
       "ABCC1     -4.638927  2.440799 -1.655580  0.506424   3.289914  2.460479   \n",
       "ABCF3      3.822458  0.241117 -0.629730 -6.448074  -2.221022  0.559189   \n",
       "ABCG1      0.562681  0.348409 -1.800319 -3.869393   6.949607 -0.700023   \n",
       "ACOT2      2.411224 -4.461962  2.932685 -3.482673   1.622149  1.970574   \n",
       "ACSF2      0.452929  2.932288  2.580745  0.576073   1.568796 -3.090774   \n",
       "ADAM2      0.355641 -0.011333  3.167627 -2.292000   0.309439  1.242176   \n",
       "ADAMTS10  -6.435335 -3.484197 -1.334098 -6.257098   1.414799  1.847578   \n",
       "ADAMTS8   -1.500518 -1.781252  0.390679 -4.498557  -2.950740  2.187399   \n",
       "ADCY6      0.239552  3.663878 -2.478119 -1.249731  -4.508629 -1.559120   \n",
       "ADCYAP1R1  0.458076  5.646495  1.054934  2.420345   1.272796  2.603719   \n",
       "ADH1B     -2.498004  4.363269  3.047640 -2.887588  -0.382138 -2.791821   \n",
       "ADH6      -0.224751 -4.080510  3.996458  2.259859  -0.291146  0.586628   \n",
       "AFF1       1.267301 -1.743443 -0.558082  0.711610  -1.599487 -4.036190   \n",
       "AGR3       2.689198  0.551704  4.595840  5.758756   2.143647  6.210038   \n",
       "AGRN      -3.803666  3.013388  0.094045 -3.100119   3.327980 -3.495524   \n",
       "AHR       -0.206353  5.031601 -6.772378  0.658805  -4.815799  2.131688   \n",
       "AIM1      -3.253885 -1.344785  0.778770 -3.178421  -2.241247  0.076789   \n",
       "AJAP1     -0.671359 -1.453216  1.583014  2.223739   6.980131 -3.211525   \n",
       "AKAP10     2.111122  1.642002  1.268021  0.915710  -2.686503  0.648780   \n",
       "AKAP8L    -2.076503  5.121783  0.120849  4.835900   4.528920 -4.010499   \n",
       "AKIRIN2    0.182562 -2.814136 -1.704658 -2.635369  -2.622177 -2.169776   \n",
       "AKT1      -0.600257 -1.156250  4.269575 -2.652594   1.637202  6.342937   \n",
       "ALG14      1.457071 -1.123955  1.168836 -4.140685  -0.767143  2.490318   \n",
       "ALPPL2     0.278612 -3.347032 -1.047794 -3.167413  -0.413878 -1.187347   \n",
       "AMDHD1     1.442675  2.435977 -2.655512 -0.262620  -0.133281  0.071852   \n",
       "AMIGO1    -0.632069 -4.026140 -0.023985  1.703159   2.216783 -2.408620   \n",
       "...             ...       ...       ...       ...        ...       ...   \n",
       "YY1        1.955807  0.441289 -4.025722  3.241794  -1.471559  2.546710   \n",
       "ZBTB25    -0.465745  2.731525  0.583832  0.952136   3.981221 -6.242749   \n",
       "ZCCHC6     2.616518  3.725200 -3.604447  0.953095  -1.414919 -1.686620   \n",
       "ZDHHC15    0.287416  4.228220 -1.932585  1.280172  -0.361667 -3.245687   \n",
       "ZDHHC16    3.983208 -4.736054  1.431894 -0.534885   1.189214  2.050506   \n",
       "ZDHHC2    -5.452097 -2.864681  6.239840  4.199863  -1.725535 -3.715479   \n",
       "ZDHHC7    -1.438812  0.525433  1.646436  1.928655   3.225473 -2.751004   \n",
       "ZDHHC9    -5.000444 -0.419128 -1.826055 -7.374587   0.431975  0.447008   \n",
       "ZFAT      -2.931567 -1.559858 -1.483557 -1.574698  -0.874071 -2.881306   \n",
       "ZFP62     -1.271041 -2.920962 -3.853412  0.748130   4.703092  3.214546   \n",
       "ZFR       -4.744726  4.316600 -4.468643  5.237806   1.316022 -0.874962   \n",
       "ZMYM3      1.471251  0.652375 -1.932988  2.670401  -2.737013  0.057174   \n",
       "ZMYND17    4.619858 -3.475264  0.038139  4.313893  -1.061013 -1.304709   \n",
       "ZNF124     3.330737  5.589767 -4.290391 -0.717922   0.811929  4.553090   \n",
       "ZNF132    -2.168550 -2.412464 -3.149500 -2.440988  10.078211  1.902683   \n",
       "ZNF138     5.501906 -0.524335 -4.959888  1.751520  -1.605983 -1.376737   \n",
       "ZNF283    -3.034221  2.111506 -0.719169  0.225898  -2.721097  1.546914   \n",
       "ZNF322B   -5.860660  6.129064  1.159615 -2.537294  -3.155169 -1.897060   \n",
       "ZNF426     6.177740 -2.424839 -0.226201  2.234599  -1.458643 -3.694419   \n",
       "ZNF451     3.249119  1.893897  1.717736 -0.659733   1.719676  3.294666   \n",
       "ZNF480    -0.140409 -1.341108  0.018698 -1.008055   0.655969 -0.532847   \n",
       "ZNF660    -5.134059 -2.887868 -5.682801 -4.584404   0.923337 -3.747658   \n",
       "ZNF696     1.536629 -1.218671 -1.244199 -4.751498  -2.160287  1.626458   \n",
       "ZNF772     1.805567  4.059267 -7.479062 -0.471547  -0.378794 -5.751023   \n",
       "ZNF776    -0.739313  4.252302 -1.231494  1.584117   5.788495  0.016007   \n",
       "ZNF826    -4.294209 -4.498573  2.786462 -1.588052   1.542248  3.222761   \n",
       "ZNF845    -1.661144 -6.986089  2.273928 -2.426933  -4.627002 -4.044476   \n",
       "ZNF878     3.395504 -6.274497  0.455548  0.592239  -0.852212  1.373684   \n",
       "ZSWIM3    -0.494841  2.840674 -3.816640  3.052187   4.979421 -5.889279   \n",
       "ZWILCH     0.694298 -2.725693 -1.752258 -1.789789   2.228141 -4.494592   \n",
       "\n",
       "                 7         8         9         10  ...        41        42  \\\n",
       "0                                                  ...                       \n",
       "AACSL      1.450223  2.130209  0.548923  0.583043  ... -4.551870 -6.645621   \n",
       "AAK1      -2.822020  0.047464 -0.456019  1.134087  ...  2.808942 -0.035529   \n",
       "ABCA17P   -6.738092 -2.811364 -1.017466 -1.646993  ... -2.324379 -0.999586   \n",
       "ABCB8      0.668285  2.608310  3.342104 -2.792534  ... -3.666569 -0.074404   \n",
       "ABCC1     -1.003678  1.537393 -1.342323 -1.003316  ... -0.375289 -3.214583   \n",
       "ABCF3     -0.817507  0.404023 -2.105254  6.397036  ...  0.416917 -1.061398   \n",
       "ABCG1      0.613762 -0.194774  0.477825  1.010311  ... -6.687904 -2.232982   \n",
       "ACOT2      3.783328  2.018564 -4.157278 -0.454352  ...  3.496161 -4.542483   \n",
       "ACSF2     -2.057883 -5.726421 -5.013427  1.913956  ... -3.264008  0.828671   \n",
       "ADAM2     -0.447120  0.028113 -3.598405  1.442208  ...  0.352784  2.091205   \n",
       "ADAMTS10  -0.854010 -1.351184 -5.122214  2.551296  ...  4.033988  3.940538   \n",
       "ADAMTS8   -4.911543 -2.987357  1.188654 -4.559955  ... -1.554750 -1.501601   \n",
       "ADCY6     -1.018709  0.820547 -0.839912 -2.340473  ...  4.726817 -2.105939   \n",
       "ADCYAP1R1  1.006353  5.376351 -0.302532  6.419028  ...  4.236462 -1.691084   \n",
       "ADH1B     -3.554774 -4.072156  0.171069 -4.810303  ... -3.070164 -2.778129   \n",
       "ADH6       3.458583  3.730949  5.144894 -0.546617  ...  3.345719  1.916228   \n",
       "AFF1       1.768361  3.132970  0.851968 -0.930023  ...  0.079065 -7.419378   \n",
       "AGR3       3.802123  4.112670  3.055105 -2.853347  ...  0.480619 -1.733525   \n",
       "AGRN       3.172164  0.686780  1.848163 -4.764568  ... -0.059656  1.874475   \n",
       "AHR       -0.616780  2.924277 -0.459491  1.096779  ... -2.079071  7.787621   \n",
       "AIM1       0.705845 -3.178349 -3.290212 -4.055884  ...  2.506882  0.238419   \n",
       "AJAP1     -2.033678 -3.638395  1.529547  0.024004  ... -1.760967  0.402947   \n",
       "AKAP10     3.363863 -1.063846 -1.164144  2.371818  ... -1.532954  0.104894   \n",
       "AKAP8L    -0.570129 -1.910726 -5.247164 -3.205733  ... -0.448418 -0.832548   \n",
       "AKIRIN2    3.083701 -4.855234 -1.960743 -1.231475  ... -1.941039 -4.338502   \n",
       "AKT1       2.674969 -0.104879 -0.478754 -0.124468  ... -0.399697 -2.622513   \n",
       "ALG14      3.910973  1.030674  3.085856 -1.179364  ... -1.414906 -1.151661   \n",
       "ALPPL2    -2.064943  1.779923 -4.103784 -3.772628  ... -0.636912 -0.258916   \n",
       "AMDHD1    -0.937521  3.380939  1.799345  2.822316  ...  2.361554 -3.860945   \n",
       "AMIGO1     2.931212 -3.947217  0.862303 -2.412431  ...  1.330087 -0.709538   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "YY1       -3.392705 -0.801326  8.747804 -0.891846  ...  3.968216 -4.094314   \n",
       "ZBTB25     0.274338 -1.281054  3.280473  5.293598  ...  0.832881 -0.647692   \n",
       "ZCCHC6     1.115142 -1.938877 -0.005253 -5.008178  ...  2.148678 -3.049896   \n",
       "ZDHHC15   -4.729219 -3.941767  3.047753  0.983202  ...  0.924528  0.802575   \n",
       "ZDHHC16    1.918995 -1.047002  4.168625 -1.529144  ... -1.787969 -0.655874   \n",
       "ZDHHC2    -3.816561 -2.090490  2.274487 -3.430965  ...  0.157227 -2.579084   \n",
       "ZDHHC7    -2.989698 -5.601409  2.200852  1.691665  ...  3.800466  5.007216   \n",
       "ZDHHC9     0.498743 -1.178437  0.252175 -0.106776  ... -0.178797 -0.721575   \n",
       "ZFAT       4.222129 -1.200172 -0.990453  5.082496  ... -1.772075  3.798225   \n",
       "ZFP62      0.476954  0.005453  0.526652  1.999028  ... -0.526324 -0.229234   \n",
       "ZFR        5.295222 -3.100937 -0.505455 -4.697889  ... -1.011458 -2.350506   \n",
       "ZMYM3      2.756968  1.152510 -2.647043  7.612432  ... -6.551895  1.795703   \n",
       "ZMYND17    0.256868  0.419320  3.540630  3.040002  ...  0.813865  5.155772   \n",
       "ZNF124    -2.496052  1.730051  0.636785  0.818016  ... -1.356317 -1.932498   \n",
       "ZNF132     3.813724 -3.439067  1.787848  2.228660  ...  0.986353 -1.858777   \n",
       "ZNF138     4.639871  2.583844  1.688727 -0.609936  ...  2.701609 -1.919130   \n",
       "ZNF283    -3.126284  0.706536  2.476561 -1.737517  ... -0.950084  0.811290   \n",
       "ZNF322B    1.876461 -6.812772 -0.795365  7.940787  ... -1.441415  3.249069   \n",
       "ZNF426     5.435978  2.113951 -0.548585 -0.587499  ...  1.810145 -0.509916   \n",
       "ZNF451    -0.481807  0.529062 -6.321060 -1.217092  ... -1.084075  2.081276   \n",
       "ZNF480     1.634830  1.264774  1.879042  1.682834  ...  2.166815 -0.138187   \n",
       "ZNF660     2.117484 -5.564895 -2.821189 -1.254334  ... -1.868436 -0.401334   \n",
       "ZNF696    -0.817378  0.777225 -0.594485  0.026376  ... -1.468312 -0.279803   \n",
       "ZNF772     6.168965  2.467858 -0.962984 -2.699509  ...  0.734462 -4.688808   \n",
       "ZNF776     2.675882  3.143431 -1.671526 -1.230410  ...  1.841290  3.199943   \n",
       "ZNF826    -0.005525 -4.711981 -0.580551  2.152243  ... -1.023105 -1.675647   \n",
       "ZNF845    -3.991184 -0.903110  1.558416  0.672473  ... -0.990304 -1.806129   \n",
       "ZNF878    -2.638325  4.028651 -1.117790 -2.844150  ... -4.858309  1.752461   \n",
       "ZSWIM3    -6.640408  3.711903 -3.156978  0.475878  ... -2.065756  1.069487   \n",
       "ZWILCH     1.233303 -1.748305  2.534993  0.819987  ... -4.033845  2.497995   \n",
       "\n",
       "                  43        44        45        46        47         48  \\\n",
       "0                                                                         \n",
       "AACSL      -3.970460 -2.041915  0.811989  0.979641  0.063161   3.652624   \n",
       "AAK1        0.473384 -1.971053  1.759803  3.151289 -5.189503  -0.233187   \n",
       "ABCA17P     0.987566  2.591347 -1.445705 -2.788339  2.295727   0.953828   \n",
       "ABCB8       4.630231  0.883074 -1.573444  4.909868  0.866853   2.374492   \n",
       "ABCC1       5.531917 -1.693335  1.506472  1.020980  4.933972   2.268159   \n",
       "ABCF3       4.559200 -1.014232  0.372768  1.563113  2.331843   7.556858   \n",
       "ABCG1      -3.412368 -2.943314 -2.985734  1.551215  1.133610   0.454167   \n",
       "ACOT2       1.584941 -0.167026 -1.091618  0.266757  0.242896  -1.986338   \n",
       "ACSF2       0.139710 -0.038387  1.705630  5.832881 -0.075640  -3.998545   \n",
       "ADAM2      -1.578376  5.417486  1.012646 -2.214839 -2.154928   2.582198   \n",
       "ADAMTS10    3.105636 -0.830551  0.912196 -0.126224 -2.345661   7.605495   \n",
       "ADAMTS8    -4.605737 -2.175497  3.599596  2.921266  3.366518  -3.623441   \n",
       "ADCY6       2.066263  1.598242 -4.352578  2.937616  0.838953   0.873717   \n",
       "ADCYAP1R1  -0.661785  0.803437 -0.274287 -0.872806  1.080127  -1.647711   \n",
       "ADH1B      -0.110080 -3.957719 -1.410711  0.695792  2.330447  -3.053955   \n",
       "ADH6        4.300311 -4.651193  0.775120 -3.972906 -0.698086  -0.430629   \n",
       "AFF1        3.747206 -2.912467 -3.456261  2.962799 -3.951777  -4.874321   \n",
       "AGR3       -0.008265  4.462258 -8.505583 -0.109861 -1.752762  -0.309343   \n",
       "AGRN        2.730239  2.368229  0.895757  1.090120  3.247198   3.194017   \n",
       "AHR         4.173739 -4.853682  1.384796 -2.567371 -4.304014  -2.440764   \n",
       "AIM1        0.795984  0.555804  3.693480  3.648031 -2.599992  -1.235266   \n",
       "AJAP1       2.047986  9.711038 -5.583309 -1.635951 -2.851895  -4.855163   \n",
       "AKAP10      2.236829  1.582274  1.728103  2.456019 -2.895845  -3.776638   \n",
       "AKAP8L      2.501409  4.901713  1.762507 -3.798529 -2.656816   1.570357   \n",
       "AKIRIN2    -6.590048 -6.723866  2.562907 -4.462658 -3.158130  -1.700410   \n",
       "AKT1        1.185376  4.711334 -1.332219 -2.756439  3.066091   3.494977   \n",
       "ALG14       3.431068  2.462015 -0.172023  4.241700  2.498942   1.538316   \n",
       "ALPPL2     -4.925722  0.192451  3.438164  0.687503  0.757951  -1.844243   \n",
       "AMDHD1      2.688930 -1.500954 -2.484701 -4.613864  2.601704   1.524390   \n",
       "AMIGO1      4.998298  5.434680  0.447682  0.431151  0.342613   0.362589   \n",
       "...              ...       ...       ...       ...       ...        ...   \n",
       "YY1        -0.874615  1.884760  0.013866  0.810367 -1.970884  -2.359943   \n",
       "ZBTB25      0.002772 -1.540829 -0.289737 -4.011846 -1.242774   1.763224   \n",
       "ZCCHC6     -7.382941 -0.647739 -1.921551 -1.810419 -4.233128   3.262738   \n",
       "ZDHHC15     0.580516 -2.626980  1.650896 -0.486386 -1.465460   2.756558   \n",
       "ZDHHC16     3.368040  0.137512 -2.539163  3.851830 -0.946804   1.647447   \n",
       "ZDHHC2      0.958787  3.746606 -0.782082  0.901215 -4.032160   3.597801   \n",
       "ZDHHC7     -6.883397  0.784845  0.651235 -0.521085 -4.866340  -0.103401   \n",
       "ZDHHC9     10.173612 -0.566138  1.952077  0.953530 -4.176484  -2.543994   \n",
       "ZFAT        0.403502 -3.078181  4.188723  5.179436  2.422908   2.133740   \n",
       "ZFP62       0.321802  1.108952  3.446266 -5.234739  1.145793   1.785877   \n",
       "ZFR        -6.253545  0.666908  1.212308 -0.688220  0.660413  -2.524871   \n",
       "ZMYM3       1.051413 -1.358606  1.383776 -2.503557  3.385820  -0.745977   \n",
       "ZMYND17    -5.260261  1.344561 -2.431417  4.107977  0.237943  -1.939147   \n",
       "ZNF124     -3.455120 -0.084919 -1.237713  3.506137 -1.885701   0.273054   \n",
       "ZNF132      8.075192  7.790025 -2.206606  6.022623 -0.591418  -5.578319   \n",
       "ZNF138     -1.581302 -2.705314 -1.775207  7.263195 -5.359276  -1.377007   \n",
       "ZNF283      7.654198 -0.602563 -0.485668  0.706387  0.637638   1.044513   \n",
       "ZNF322B    -4.036189  0.334181 -2.639774  7.711171  5.733905   0.518429   \n",
       "ZNF426     -1.958090 -2.925023 -1.296322  3.874413  5.068583  -7.030104   \n",
       "ZNF451     -2.132384  0.780504 -2.191865 -3.959255 -5.588040  -1.734270   \n",
       "ZNF480      0.269705  2.945537 -6.374132 -4.319404 -1.417171  -3.086070   \n",
       "ZNF660      1.951397  4.396688  4.859973  4.140445  2.204386   1.565369   \n",
       "ZNF696      5.001230  2.464151 -0.827188 -2.598237 -1.420780   2.221469   \n",
       "ZNF772     -1.912480 -3.396070  1.147704 -3.591208 -3.216311   2.329710   \n",
       "ZNF776      5.509921 -0.675689 -0.410618  2.722958 -2.857927   0.232340   \n",
       "ZNF826      3.320340  2.108714 -5.961589  1.679723 -1.473783   1.871397   \n",
       "ZNF845      1.643056  1.932765  1.084221 -1.214410 -2.985126 -10.986240   \n",
       "ZNF878      1.630060 -0.079563 -0.545380 -2.694063 -0.535988  -0.038242   \n",
       "ZSWIM3     -1.682956  4.824188 -0.930484 -3.155203 -4.252213  -6.578125   \n",
       "ZWILCH     -3.744730 -2.536536 -3.004383  3.336575 -1.095170  -3.466885   \n",
       "\n",
       "                 49        50  \n",
       "0                              \n",
       "AACSL     -2.387639  0.929521  \n",
       "AAK1       0.349614  0.704183  \n",
       "ABCA17P   -1.094031 -2.104951  \n",
       "ABCB8      1.410069 -3.828003  \n",
       "ABCC1     -0.734398 -2.618825  \n",
       "ABCF3      1.705383 -1.658708  \n",
       "ABCG1     -2.928591  2.399636  \n",
       "ACOT2      1.453353 -3.237589  \n",
       "ACSF2     -0.008504 -0.974825  \n",
       "ADAM2     -0.176870  1.551844  \n",
       "ADAMTS10   1.028245 -3.296487  \n",
       "ADAMTS8    6.498009  0.601931  \n",
       "ADCY6      2.164306 -7.228367  \n",
       "ADCYAP1R1 -7.314351 -0.454197  \n",
       "ADH1B     -4.306005  1.839237  \n",
       "ADH6      -0.816764  0.763499  \n",
       "AFF1      -0.339520  3.577241  \n",
       "AGR3      -0.280030 -3.342774  \n",
       "AGRN       1.101338  2.587900  \n",
       "AHR        2.933068  3.258371  \n",
       "AIM1      -1.932245  1.393250  \n",
       "AJAP1     -1.584887  1.328536  \n",
       "AKAP10    -2.602940  1.132555  \n",
       "AKAP8L     1.879632  4.546102  \n",
       "AKIRIN2   -3.032855 -1.274919  \n",
       "AKT1      -2.954487 -1.335418  \n",
       "ALG14      4.271883  9.790623  \n",
       "ALPPL2    -4.572469  0.661904  \n",
       "AMDHD1    -0.309889  6.049197  \n",
       "AMIGO1     0.124003 -4.286540  \n",
       "...             ...       ...  \n",
       "YY1        4.105038 -1.566941  \n",
       "ZBTB25    -4.739805  4.830278  \n",
       "ZCCHC6    -0.812537  3.522548  \n",
       "ZDHHC15    1.240380  1.369940  \n",
       "ZDHHC16    1.379092 -5.007336  \n",
       "ZDHHC2    -0.142247 -2.502663  \n",
       "ZDHHC7     0.323577  0.927459  \n",
       "ZDHHC9    -4.591273  1.720633  \n",
       "ZFAT      -0.258086 -5.022865  \n",
       "ZFP62      6.950120  1.517959  \n",
       "ZFR        0.319327  2.381417  \n",
       "ZMYM3     -0.627234 -1.546457  \n",
       "ZMYND17   -0.133882  3.983232  \n",
       "ZNF124    -2.894267 -0.983415  \n",
       "ZNF132    -6.060207  1.476551  \n",
       "ZNF138    -2.831965  2.458768  \n",
       "ZNF283     1.126691  1.881236  \n",
       "ZNF322B   -2.275928 -1.721815  \n",
       "ZNF426    -1.901056 -1.727810  \n",
       "ZNF451     6.236782 -4.415510  \n",
       "ZNF480    -4.755754  2.109595  \n",
       "ZNF660     1.391193  0.320414  \n",
       "ZNF696     0.488647 -7.675989  \n",
       "ZNF772     2.226982 -0.744886  \n",
       "ZNF776     0.472510 -0.760252  \n",
       "ZNF826     1.968646 -1.017821  \n",
       "ZNF845     0.159237  0.906706  \n",
       "ZNF878    -1.353040 -1.713555  \n",
       "ZSWIM3    -6.558922 -0.104826  \n",
       "ZWILCH     1.519252 -0.729152  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motif data should be formatted into a three-column list, where first column contains TF IDs and second column the target gene IDs and third column the interaction scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AAK1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCA17P</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCB8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCC1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCF3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ABCG1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAM2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAMTS10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADAMTS8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADCY6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ADCYAP1R1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AFF1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AJAP1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKAP10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKIRIN2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AKT1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ALG14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMDHD1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMIGO1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AMPH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANKRD39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ANP32E</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ARHGEF2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ASAP3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ASPSCR1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ATL1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14567</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WBSCR26</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14568</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14569</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR54</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14570</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WDR61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14571</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WFDC10B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14572</th>\n",
       "      <td>YY1</td>\n",
       "      <td>WIPF3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14573</th>\n",
       "      <td>YY1</td>\n",
       "      <td>YBX2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14574</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZBTB25</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14575</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14576</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14577</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC7</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14578</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZDHHC9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14579</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFAT</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14580</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFP62</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14581</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZFR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14582</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZMYM3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14583</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZMYND17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14584</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF124</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14585</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF132</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14586</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF138</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14587</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF283</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF322B</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14589</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF426</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14590</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF660</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14591</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF772</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF776</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14593</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF826</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14594</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZNF878</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14595</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZSWIM3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14596</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1    2\n",
       "0      AHR      41157  1.0\n",
       "1      AHR       AAK1  1.0\n",
       "2      AHR    ABCA17P  1.0\n",
       "3      AHR      ABCB8  1.0\n",
       "4      AHR      ABCC1  1.0\n",
       "5      AHR      ABCF3  1.0\n",
       "6      AHR      ABCG1  1.0\n",
       "7      AHR      ADAM2  1.0\n",
       "8      AHR   ADAMTS10  1.0\n",
       "9      AHR    ADAMTS8  1.0\n",
       "10     AHR      ADCY6  1.0\n",
       "11     AHR  ADCYAP1R1  1.0\n",
       "12     AHR       AFF1  1.0\n",
       "13     AHR       AGRN  1.0\n",
       "14     AHR      AJAP1  1.0\n",
       "15     AHR     AKAP10  1.0\n",
       "16     AHR    AKIRIN2  1.0\n",
       "17     AHR       AKT1  1.0\n",
       "18     AHR      ALG14  1.0\n",
       "19     AHR     AMDHD1  1.0\n",
       "20     AHR     AMIGO1  1.0\n",
       "21     AHR       AMPH  1.0\n",
       "22     AHR    ANKRD16  1.0\n",
       "23     AHR    ANKRD36  1.0\n",
       "24     AHR    ANKRD39  1.0\n",
       "25     AHR     ANP32E  1.0\n",
       "26     AHR    ARHGEF2  1.0\n",
       "27     AHR      ASAP3  1.0\n",
       "28     AHR    ASPSCR1  1.0\n",
       "29     AHR       ATL1  1.0\n",
       "...    ...        ...  ...\n",
       "14567  YY1    WBSCR26  1.0\n",
       "14568  YY1       WDR4  1.0\n",
       "14569  YY1      WDR54  1.0\n",
       "14570  YY1      WDR61  1.0\n",
       "14571  YY1    WFDC10B  1.0\n",
       "14572  YY1      WIPF3  1.0\n",
       "14573  YY1       YBX2  1.0\n",
       "14574  YY1     ZBTB25  1.0\n",
       "14575  YY1    ZDHHC15  1.0\n",
       "14576  YY1    ZDHHC16  1.0\n",
       "14577  YY1     ZDHHC7  1.0\n",
       "14578  YY1     ZDHHC9  1.0\n",
       "14579  YY1       ZFAT  1.0\n",
       "14580  YY1      ZFP62  1.0\n",
       "14581  YY1        ZFR  1.0\n",
       "14582  YY1      ZMYM3  1.0\n",
       "14583  YY1    ZMYND17  1.0\n",
       "14584  YY1     ZNF124  1.0\n",
       "14585  YY1     ZNF132  1.0\n",
       "14586  YY1     ZNF138  1.0\n",
       "14587  YY1     ZNF283  1.0\n",
       "14588  YY1    ZNF322B  1.0\n",
       "14589  YY1     ZNF426  1.0\n",
       "14590  YY1     ZNF660  1.0\n",
       "14591  YY1     ZNF772  1.0\n",
       "14592  YY1     ZNF776  1.0\n",
       "14593  YY1     ZNF826  1.0\n",
       "14594  YY1     ZNF878  1.0\n",
       "14595  YY1     ZSWIM3  1.0\n",
       "14596  YY1     ZWILCH  1.0\n",
       "\n",
       "[14597 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 87 unique TFs and 913 unique motifs in this motif dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data[0].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_data[1].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PPI (protein protein interaction) data should be formatted into a three-column list, where first two columns contain protein IDs and third column contains a score for each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHR</td>\n",
       "      <td>NR2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>SP1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUNX1</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RUNX1</td>\n",
       "      <td>PAX5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>MYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>SPI1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDIT3</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ARID3A</td>\n",
       "      <td>E2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ELK1</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ELK4</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EN1</td>\n",
       "      <td>PAX6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>ESR2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>NR2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GATA2</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GATA3</td>\n",
       "      <td>TAL1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>PBX1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HIF1A</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FOXA2</td>\n",
       "      <td>HOXA5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IRF1</td>\n",
       "      <td>STAT1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IRF2</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JUN</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JUN</td>\n",
       "      <td>NFE2L1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JUN</td>\n",
       "      <td>NFE2L2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>RXRA</td>\n",
       "      <td>TBP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>SP1</td>\n",
       "      <td>KLF4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>SPIB</td>\n",
       "      <td>TBP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>TFAP2A</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>AR</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>NFKB1</td>\n",
       "      <td>REL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>MYC</td>\n",
       "      <td>SP1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>AR</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>AR</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>AR</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>ELK1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>GATA3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>MAFG</td>\n",
       "      <td>PAX6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>NR4A2</td>\n",
       "      <td>RXRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>TP53</td>\n",
       "      <td>KLF4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>NFYA</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>REL</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>CREB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>MAX</td>\n",
       "      <td>MYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>TBP</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>NFKB1</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>DDIT3</td>\n",
       "      <td>HOXA5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>NR2E3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>HLF</td>\n",
       "      <td>MYB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>PPARG</td>\n",
       "      <td>NR2E3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1  2\n",
       "0       AHR    ESR1  1\n",
       "1       AHR    RELA  1\n",
       "2       AHR   NR2F1  1\n",
       "3        AR     SP1  1\n",
       "4     RUNX1     JUN  1\n",
       "5     RUNX1    PAX5  1\n",
       "6     CEBPA   NR3C1  1\n",
       "7     CEBPA     MYC  1\n",
       "8     CEBPA    SPI1  1\n",
       "9     DDIT3     JUN  1\n",
       "10   ARID3A    E2F1  1\n",
       "11     EGR1    RELA  1\n",
       "12     ELK1     SRF  1\n",
       "13     ELK4     SRF  1\n",
       "14      EN1    PAX6  1\n",
       "15     ESR1    ESR1  1\n",
       "16     ESR1    ESR2  1\n",
       "17     ESR1   NR2F1  1\n",
       "18     ETS1     JUN  1\n",
       "19    GATA2     JUN  1\n",
       "20    GATA3    TAL1  1\n",
       "21    NR3C1   NR3C1  1\n",
       "22    NR3C1    PBX1  1\n",
       "23    HIF1A     JUN  1\n",
       "24    FOXA2   HOXA5  1\n",
       "25     IRF1   STAT1  1\n",
       "26     IRF2   NFKB1  1\n",
       "27      JUN     JUN  1\n",
       "28      JUN  NFE2L1  1\n",
       "29      JUN  NFE2L2  1\n",
       "..      ...     ... ..\n",
       "208    RXRA     TBP  1\n",
       "209     SP1    KLF4  1\n",
       "210    SPIB     TBP  1\n",
       "211  TFAP2A    TP53  1\n",
       "212   BRCA1    ESR1  1\n",
       "213      AR     JUN  1\n",
       "214   BRCA1    TP53  1\n",
       "215   NFKB1     REL  1\n",
       "216     MYC     SP1  1\n",
       "217      AR   NR3C1  1\n",
       "218      AR   NFKB1  1\n",
       "219      AR    RELA  1\n",
       "220   BRCA1    ELK1  1\n",
       "221   BRCA1   NFKB1  1\n",
       "222    ETS1   GATA3  1\n",
       "223    ETS1   NR3C1  1\n",
       "224    MAFG    PAX6  1\n",
       "225   NR4A2    RXRA  1\n",
       "226    TP53    KLF4  1\n",
       "227    NFYA    TP53  1\n",
       "228     REL    RELA  1\n",
       "229    EGR1    TP53  1\n",
       "230   BRCA1   CREB1  1\n",
       "231     MAX     MYC  1\n",
       "232     TBP    TP53  1\n",
       "233   NFKB1    RELA  1\n",
       "234   DDIT3   HOXA5  1\n",
       "235   NR3C1   NR2E3  1\n",
       "236     HLF     MYB  1\n",
       "237   PPARG   NR2E3  1\n",
       "\n",
       "[238 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This PPI dataset has 238 interactions among 87 TFs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Panda\n",
    "Before running Lioness, we will first need to generate a `Panda` object. This will be used later to run `Lioness`. Note that the argument `keep_expression_matrix` should be specified as `True`. As Lioness iteractions need to call Panda function to build networks, which needs expression matrix as input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AHR</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AHR</td>\n",
       "      <td>NR2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>SP1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RUNX1</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RUNX1</td>\n",
       "      <td>PAX5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>MYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>SPI1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DDIT3</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ARID3A</td>\n",
       "      <td>E2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ELK1</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ELK4</td>\n",
       "      <td>SRF</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>EN1</td>\n",
       "      <td>PAX6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>ESR2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>NR2F1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GATA2</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GATA3</td>\n",
       "      <td>TAL1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>PBX1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>HIF1A</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>FOXA2</td>\n",
       "      <td>HOXA5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>IRF1</td>\n",
       "      <td>STAT1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>IRF2</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JUN</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JUN</td>\n",
       "      <td>NFE2L1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JUN</td>\n",
       "      <td>NFE2L2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>RXRA</td>\n",
       "      <td>TBP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>SP1</td>\n",
       "      <td>KLF4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>SPIB</td>\n",
       "      <td>TBP</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>TFAP2A</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>ESR1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>AR</td>\n",
       "      <td>JUN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>NFKB1</td>\n",
       "      <td>REL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>MYC</td>\n",
       "      <td>SP1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>AR</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>AR</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>AR</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>ELK1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>NFKB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>GATA3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>NR3C1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>MAFG</td>\n",
       "      <td>PAX6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>NR4A2</td>\n",
       "      <td>RXRA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>TP53</td>\n",
       "      <td>KLF4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>NFYA</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>REL</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>CREB1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>MAX</td>\n",
       "      <td>MYC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>TBP</td>\n",
       "      <td>TP53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>NFKB1</td>\n",
       "      <td>RELA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>DDIT3</td>\n",
       "      <td>HOXA5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NR3C1</td>\n",
       "      <td>NR2E3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>HLF</td>\n",
       "      <td>MYB</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>PPARG</td>\n",
       "      <td>NR2E3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>476 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1  2\n",
       "0       AHR    ESR1  1\n",
       "1       AHR    RELA  1\n",
       "2       AHR   NR2F1  1\n",
       "3        AR     SP1  1\n",
       "4     RUNX1     JUN  1\n",
       "5     RUNX1    PAX5  1\n",
       "6     CEBPA   NR3C1  1\n",
       "7     CEBPA     MYC  1\n",
       "8     CEBPA    SPI1  1\n",
       "9     DDIT3     JUN  1\n",
       "10   ARID3A    E2F1  1\n",
       "11     EGR1    RELA  1\n",
       "12     ELK1     SRF  1\n",
       "13     ELK4     SRF  1\n",
       "14      EN1    PAX6  1\n",
       "15     ESR1    ESR1  1\n",
       "16     ESR1    ESR2  1\n",
       "17     ESR1   NR2F1  1\n",
       "18     ETS1     JUN  1\n",
       "19    GATA2     JUN  1\n",
       "20    GATA3    TAL1  1\n",
       "21    NR3C1   NR3C1  1\n",
       "22    NR3C1    PBX1  1\n",
       "23    HIF1A     JUN  1\n",
       "24    FOXA2   HOXA5  1\n",
       "25     IRF1   STAT1  1\n",
       "26     IRF2   NFKB1  1\n",
       "27      JUN     JUN  1\n",
       "28      JUN  NFE2L1  1\n",
       "29      JUN  NFE2L2  1\n",
       "..      ...     ... ..\n",
       "208    RXRA     TBP  1\n",
       "209     SP1    KLF4  1\n",
       "210    SPIB     TBP  1\n",
       "211  TFAP2A    TP53  1\n",
       "212   BRCA1    ESR1  1\n",
       "213      AR     JUN  1\n",
       "214   BRCA1    TP53  1\n",
       "215   NFKB1     REL  1\n",
       "216     MYC     SP1  1\n",
       "217      AR   NR3C1  1\n",
       "218      AR   NFKB1  1\n",
       "219      AR    RELA  1\n",
       "220   BRCA1    ELK1  1\n",
       "221   BRCA1   NFKB1  1\n",
       "222    ETS1   GATA3  1\n",
       "223    ETS1   NR3C1  1\n",
       "224    MAFG    PAX6  1\n",
       "225   NR4A2    RXRA  1\n",
       "226    TP53    KLF4  1\n",
       "227    NFYA    TP53  1\n",
       "228     REL    RELA  1\n",
       "229    EGR1    TP53  1\n",
       "230   BRCA1   CREB1  1\n",
       "231     MAX     MYC  1\n",
       "232     TBP    TP53  1\n",
       "233   NFKB1    RELA  1\n",
       "234   DDIT3   HOXA5  1\n",
       "235   NR3C1   NR2E3  1\n",
       "236     HLF     MYB  1\n",
       "237   PPARG   NR2E3  1\n",
       "\n",
       "[476 rows x 3 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ppi_data[0] = pd.concat([ppi_data[0],ppi_data[1]])\n",
    "#ppi_data\n",
    "new_df=pd.DataFrame(data={0:ppi_data[0],1:ppi_data[1],2:ppi_data[2]})\n",
    "df=pd.concat([ppi_data,new_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading motif data ...\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading expression data ...\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading PPI data ...\n",
      "Number of PPIs: 238\n",
      "  Elapsed time: 0.00 sec.\n",
      "Calculating coexpression network ...\n",
      "  Elapsed time: 0.01 sec.\n",
      "Creating motif network ...\n",
      "  Elapsed time: 0.01 sec.\n",
      "Creating PPI network ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Normalizing networks ...\n",
      "  Elapsed time: 0.04 sec.\n",
      "Saving expression matrix and normalized networks ...\n",
      "  Elapsed time: 0.01 sec.\n",
      "Running PANDA algorithm ...\n",
      "step: 0, hamming: 0.7189662815459754\n",
      "step: 1, hamming: 0.3899291546314954\n",
      "step: 2, hamming: 0.4023668388969203\n",
      "step: 3, hamming: 0.40052096181128466\n",
      "step: 4, hamming: 0.38904060163854676\n",
      "step: 5, hamming: 0.37050927774796627\n",
      "step: 6, hamming: 0.346813714233211\n",
      "step: 7, hamming: 0.3197200219092709\n",
      "step: 8, hamming: 0.2908059296381211\n",
      "step: 9, hamming: 0.2614076747991081\n",
      "step: 10, hamming: 0.23256674933108332\n",
      "step: 11, hamming: 0.2050473463652485\n",
      "step: 12, hamming: 0.17936756642941443\n",
      "step: 13, hamming: 0.1558282020394879\n",
      "step: 14, hamming: 0.1345640692729987\n",
      "step: 15, hamming: 0.1155876981777767\n",
      "step: 16, hamming: 0.09882404071423918\n",
      "step: 17, hamming: 0.08414234823461533\n",
      "step: 18, hamming: 0.07137863350560043\n",
      "step: 19, hamming: 0.06035259742114878\n",
      "step: 20, hamming: 0.050879894600761214\n",
      "step: 21, hamming: 0.04278075541305479\n",
      "step: 22, hamming: 0.03588517917018383\n",
      "step: 23, hamming: 0.030036230563844166\n",
      "step: 24, hamming: 0.025091497107547298\n",
      "step: 25, hamming: 0.020923570455323982\n",
      "step: 26, hamming: 0.01741975365490937\n",
      "step: 27, hamming: 0.014481280791973363\n",
      "step: 28, hamming: 0.012022271977958703\n",
      "step: 29, hamming: 0.009968530625360779\n",
      "step: 30, hamming: 0.008256316822080972\n",
      "step: 31, hamming: 0.0068311523959430605\n",
      "step: 32, hamming: 0.005646666396278007\n",
      "step: 33, hamming: 0.0046635404280628776\n",
      "step: 34, hamming: 0.0038485478068236794\n",
      "step: 35, hamming: 0.0031736932436243594\n",
      "step: 36, hamming: 0.0026154528841061666\n",
      "step: 37, hamming: 0.0021541086160812906\n",
      "step: 38, hamming: 0.0017731679664810445\n",
      "step: 39, hamming: 0.0014588635383778888\n",
      "step: 40, hamming: 0.0011997237010732422\n",
      "step: 41, hamming: 0.0009862051158053523\n",
      "Running panda took: 1.09 seconds!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>gene</th>\n",
       "      <th>motif</th>\n",
       "      <th>force</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.964144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.884019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARID3A</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.557906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARNT</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.132133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.165714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CEBPA</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.312836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CREB1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.954271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DDIT3</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.747061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E2F1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.645380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EGR1</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.208149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ELK1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.684808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ELK4</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.761459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>EN1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.851133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ESR1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.647655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ESR2</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.692321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ETS1</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.725787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EWSR1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.698345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>FLI1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.701972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FOXA1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.905231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>FOXA2</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.897361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>FOXD3</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.845337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FOXF2</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.729675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GABPA</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.307186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GATA1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.819915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GATA2</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.734119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>GATA3</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.488669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>HAND1</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.866488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>HIF1A</td>\n",
       "      <td>41157</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.251958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>HLF</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.841171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>HNF4A</td>\n",
       "      <td>41157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.804764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86970</th>\n",
       "      <td>PAX6</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86971</th>\n",
       "      <td>PBX1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.065302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86972</th>\n",
       "      <td>POU5F1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.999732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86973</th>\n",
       "      <td>PPARG</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.922157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86974</th>\n",
       "      <td>PRRX2</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.977581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86975</th>\n",
       "      <td>REL</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.932124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86976</th>\n",
       "      <td>RELA</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.049103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86977</th>\n",
       "      <td>RORA</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.988828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86978</th>\n",
       "      <td>RUNX1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.045488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86979</th>\n",
       "      <td>RXRA</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.036852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86980</th>\n",
       "      <td>SOX10</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.859536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86981</th>\n",
       "      <td>SOX2</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.971293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86982</th>\n",
       "      <td>SOX5</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.083632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86983</th>\n",
       "      <td>SP1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.633314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86984</th>\n",
       "      <td>SPI1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.810397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86985</th>\n",
       "      <td>SPIB</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.685852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86986</th>\n",
       "      <td>SRF</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.047196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86987</th>\n",
       "      <td>SRY</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.956764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86988</th>\n",
       "      <td>STAT1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.044462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86989</th>\n",
       "      <td>STAT3</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86990</th>\n",
       "      <td>TAL1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.750527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86991</th>\n",
       "      <td>TBP</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.923475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86992</th>\n",
       "      <td>TCF3</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.031047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86993</th>\n",
       "      <td>TEAD1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.061318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86994</th>\n",
       "      <td>TFAP2A</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.963011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86995</th>\n",
       "      <td>TLX1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.796110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86996</th>\n",
       "      <td>TP53</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.784221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86997</th>\n",
       "      <td>USF1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.611736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86998</th>\n",
       "      <td>VDR</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.962252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86999</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.121426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf    gene  motif     force\n",
       "0         AHR   41157    1.0  2.964144\n",
       "1          AR   41157    0.0 -0.884019\n",
       "2      ARID3A   41157    0.0 -0.557906\n",
       "3        ARNT   41157    1.0  3.132133\n",
       "4       BRCA1   41157    0.0 -0.165714\n",
       "5       CEBPA   41157    0.0 -0.312836\n",
       "6       CREB1   41157    0.0 -0.954271\n",
       "7       DDIT3   41157    0.0 -0.747061\n",
       "8        E2F1   41157    0.0 -0.645380\n",
       "9        EGR1   41157    1.0  3.208149\n",
       "10       ELK1   41157    0.0 -0.684808\n",
       "11       ELK4   41157    0.0 -0.761459\n",
       "12        EN1   41157    0.0 -0.851133\n",
       "13       ESR1   41157    0.0 -0.647655\n",
       "14       ESR2   41157    0.0 -0.692321\n",
       "15       ETS1   41157    1.0  2.725787\n",
       "16      EWSR1   41157    0.0 -0.698345\n",
       "17       FLI1   41157    0.0 -0.701972\n",
       "18      FOXA1   41157    0.0 -0.905231\n",
       "19      FOXA2   41157    0.0 -0.897361\n",
       "20      FOXD3   41157    0.0 -0.845337\n",
       "21      FOXF2   41157    0.0 -0.729675\n",
       "22      GABPA   41157    0.0 -0.307186\n",
       "23      GATA1   41157    0.0 -0.819915\n",
       "24      GATA2   41157    1.0  2.734119\n",
       "25      GATA3   41157    0.0 -0.488669\n",
       "26      HAND1   41157    0.0 -0.866488\n",
       "27      HIF1A   41157    1.0  4.251958\n",
       "28        HLF   41157    0.0 -0.841171\n",
       "29      HNF4A   41157    0.0 -0.804764\n",
       "...       ...     ...    ...       ...\n",
       "86970    PAX6  ZWILCH    0.0 -0.842105\n",
       "86971    PBX1  ZWILCH    0.0 -1.065302\n",
       "86972  POU5F1  ZWILCH    0.0 -0.999732\n",
       "86973   PPARG  ZWILCH    0.0 -0.922157\n",
       "86974   PRRX2  ZWILCH    1.0  2.977581\n",
       "86975     REL  ZWILCH    0.0 -0.932124\n",
       "86976    RELA  ZWILCH    0.0 -1.049103\n",
       "86977    RORA  ZWILCH    0.0 -0.988828\n",
       "86978   RUNX1  ZWILCH    0.0 -1.045488\n",
       "86979    RXRA  ZWILCH    0.0 -1.036852\n",
       "86980   SOX10  ZWILCH    1.0  2.859536\n",
       "86981    SOX2  ZWILCH    0.0 -0.971293\n",
       "86982    SOX5  ZWILCH    0.0 -0.083632\n",
       "86983     SP1  ZWILCH    1.0  2.633314\n",
       "86984    SPI1  ZWILCH    1.0  2.810397\n",
       "86985    SPIB  ZWILCH    1.0  2.685852\n",
       "86986     SRF  ZWILCH    0.0 -1.047196\n",
       "86987     SRY  ZWILCH    0.0 -0.956764\n",
       "86988   STAT1  ZWILCH    0.0 -1.044462\n",
       "86989   STAT3  ZWILCH    0.0 -0.924700\n",
       "86990    TAL1  ZWILCH    0.0 -0.750527\n",
       "86991     TBP  ZWILCH    0.0 -0.923475\n",
       "86992    TCF3  ZWILCH    0.0 -1.031047\n",
       "86993   TEAD1  ZWILCH    0.0 -1.061318\n",
       "86994  TFAP2A  ZWILCH    0.0 -0.963011\n",
       "86995    TLX1  ZWILCH    0.0 -0.796110\n",
       "86996    TP53  ZWILCH    0.0 -0.784221\n",
       "86997    USF1  ZWILCH    0.0 -0.611736\n",
       "86998     VDR  ZWILCH    0.0 -0.962252\n",
       "86999     YY1  ZWILCH    1.0  3.121426\n",
       "\n",
       "[87000 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda_obj = Panda('~/netZooPy/tests/puma/ToyData/ToyExpressionData.txt',\n",
    "                  '~/netZooPy/tests/puma/ToyData/ToyMotifData.txt',\n",
    "                  '~/netZooPy/tests/puma/ToyData/ToyPPIData.txt',\n",
    "                  remove_missing=False, \n",
    "                  keep_expression_matrix=True, save_memory=False)\n",
    "panda_obj.export_panda_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Lioness to estimate sample-specific networks\n",
    "We will first use the `Panda` object as input for `Lioness` object. Then `Lioness` will run Panda algorithm in its iterations to estimate sample-specific network for each sample.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading input data ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 1:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451915130131665\n",
      "step: 1, hamming: 0.6067461274659866\n",
      "step: 2, hamming: 0.6105604110178086\n",
      "step: 3, hamming: 0.5875350566544837\n",
      "step: 4, hamming: 0.553598317095621\n",
      "step: 5, hamming: 0.5130977642028344\n",
      "step: 6, hamming: 0.46888897174383587\n",
      "step: 7, hamming: 0.42325335262309544\n",
      "step: 8, hamming: 0.3780071410446651\n",
      "step: 9, hamming: 0.3345126063650471\n",
      "step: 10, hamming: 0.29371889150860553\n",
      "step: 11, hamming: 0.256215635375215\n",
      "step: 12, hamming: 0.2223029668802453\n",
      "step: 13, hamming: 0.19205902295385965\n",
      "step: 14, hamming: 0.16540299653390683\n",
      "step: 15, hamming: 0.14214587924488872\n",
      "step: 16, hamming: 0.12203018089850888\n",
      "step: 17, hamming: 0.10475927721891459\n",
      "step: 18, hamming: 0.09001790128025045\n",
      "step: 19, hamming: 0.07748617353400274\n",
      "step: 20, hamming: 0.06684920778474639\n",
      "step: 21, hamming: 0.05780549039322097\n",
      "step: 22, hamming: 0.050075273181656076\n",
      "step: 23, hamming: 0.04341014640011884\n",
      "step: 24, hamming: 0.037602349632078355\n",
      "step: 25, hamming: 0.03249107812351406\n",
      "step: 26, hamming: 0.027962472467870057\n",
      "step: 27, hamming: 0.023942393402995726\n",
      "step: 28, hamming: 0.020384212163623416\n",
      "step: 29, hamming: 0.017255759085262883\n",
      "step: 30, hamming: 0.014529132197793934\n",
      "step: 31, hamming: 0.012174934686279325\n",
      "step: 32, hamming: 0.010160473548057764\n",
      "step: 33, hamming: 0.008450451023408795\n",
      "step: 34, hamming: 0.007008679501096658\n",
      "step: 35, hamming: 0.005799847383305951\n",
      "step: 36, hamming: 0.0047908698579349875\n",
      "step: 37, hamming: 0.003951715021127739\n",
      "step: 38, hamming: 0.0032557729560454525\n",
      "step: 39, hamming: 0.0026798912102928003\n",
      "step: 40, hamming: 0.0022041994917917375\n",
      "step: 41, hamming: 0.0018118190733138475\n",
      "step: 42, hamming: 0.001488523179360306\n",
      "step: 43, hamming: 0.0012223905275455405\n",
      "step: 44, hamming: 0.0010034762989084775\n",
      "step: 45, hamming: 0.0008235128661467359\n",
      "Running panda took: 1.57 seconds!\n",
      "  Elapsed time: 1.57 sec.\n",
      "Saving LIONESS network 1 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 2:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6445476038755726\n",
      "step: 1, hamming: 0.6066183636796103\n",
      "step: 2, hamming: 0.6105445645592841\n",
      "step: 3, hamming: 0.5875332090279166\n",
      "step: 4, hamming: 0.5536082244075643\n",
      "step: 5, hamming: 0.5131126090450783\n",
      "step: 6, hamming: 0.4689036142976634\n",
      "step: 7, hamming: 0.42326732437298714\n",
      "step: 8, hamming: 0.37801889244651393\n",
      "step: 9, hamming: 0.3345221998366117\n",
      "step: 10, hamming: 0.2937266882099521\n",
      "step: 11, hamming: 0.25622198762640763\n",
      "step: 12, hamming: 0.2223082180455078\n",
      "step: 13, hamming: 0.19206345829316707\n",
      "step: 14, hamming: 0.16540684598000693\n",
      "step: 15, hamming: 0.14214929350383065\n",
      "step: 16, hamming: 0.12203326585701965\n",
      "step: 17, hamming: 0.10476213861746786\n",
      "step: 18, hamming: 0.09002061402300268\n",
      "step: 19, hamming: 0.07748873372939052\n",
      "step: 20, hamming: 0.06685160145106347\n",
      "step: 21, hamming: 0.05780769330088383\n",
      "step: 22, hamming: 0.05007728699579842\n",
      "step: 23, hamming: 0.043411967564035835\n",
      "step: 24, hamming: 0.03760397086506173\n",
      "step: 25, hamming: 0.03249250247772415\n",
      "step: 26, hamming: 0.02796370867990171\n",
      "step: 27, hamming: 0.023943455630073416\n",
      "step: 28, hamming: 0.020385117331234274\n",
      "step: 29, hamming: 0.017256526537376273\n",
      "step: 30, hamming: 0.014529780088585704\n",
      "step: 31, hamming: 0.01217547887586061\n",
      "step: 32, hamming: 0.01016092964904858\n",
      "step: 33, hamming: 0.00845083256181436\n",
      "step: 34, hamming: 0.007008998275263656\n",
      "step: 35, hamming: 0.00580011293166026\n",
      "step: 36, hamming: 0.004791090473343346\n",
      "step: 37, hamming: 0.003951898060626234\n",
      "step: 38, hamming: 0.0032559245644580514\n",
      "step: 39, hamming: 0.002680016580549968\n",
      "step: 40, hamming: 0.002204303033904436\n",
      "step: 41, hamming: 0.0018119045001386948\n",
      "step: 42, hamming: 0.0014885935906914863\n",
      "step: 43, hamming: 0.0012224485182821007\n",
      "step: 44, hamming: 0.001003524027229864\n",
      "step: 45, hamming: 0.0008235521248684664\n",
      "Running panda took: 1.44 seconds!\n",
      "  Elapsed time: 1.44 sec.\n",
      "Saving LIONESS network 2 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 3:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6452294464816625\n",
      "step: 1, hamming: 0.6066718767537945\n",
      "step: 2, hamming: 0.6105013812906783\n",
      "step: 3, hamming: 0.5874769465007303\n",
      "step: 4, hamming: 0.5535467800773778\n",
      "step: 5, hamming: 0.5130509623219737\n",
      "step: 6, hamming: 0.46884619807330846\n",
      "step: 7, hamming: 0.42321422343773074\n",
      "step: 8, hamming: 0.37797019212390504\n",
      "step: 9, hamming: 0.3344779785095642\n",
      "step: 10, hamming: 0.29368704884702534\n",
      "step: 11, hamming: 0.2561872040679576\n",
      "step: 12, hamming: 0.22227787688473338\n",
      "step: 13, hamming: 0.19203719585754386\n",
      "step: 14, hamming: 0.1653841888050198\n",
      "step: 15, hamming: 0.14212975837567024\n",
      "step: 16, hamming: 0.12201639360763507\n",
      "step: 17, hamming: 0.10474755327204792\n",
      "step: 18, hamming: 0.09000797360987653\n",
      "step: 19, hamming: 0.07747770691527577\n",
      "step: 20, hamming: 0.06684195286417426\n",
      "step: 21, hamming: 0.05779924885457799\n",
      "step: 22, hamming: 0.0500699126573237\n",
      "step: 23, hamming: 0.04340556363927608\n",
      "step: 24, hamming: 0.03759846201370095\n",
      "step: 25, hamming: 0.032487809256736604\n",
      "step: 26, hamming: 0.027959741303826367\n",
      "step: 27, hamming: 0.023940126981286655\n",
      "step: 28, hamming: 0.020382340568109492\n",
      "step: 29, hamming: 0.017254219929107445\n",
      "step: 30, hamming: 0.014527868465885224\n",
      "step: 31, hamming: 0.01217389759028471\n",
      "step: 32, hamming: 0.010159622712944908\n",
      "step: 33, hamming: 0.008449753233040156\n",
      "step: 34, hamming: 0.00700810750381105\n",
      "step: 35, hamming: 0.005799378246977346\n",
      "step: 36, hamming: 0.004790484906859\n",
      "step: 37, hamming: 0.003951399167495922\n",
      "step: 38, hamming: 0.0032555137716483097\n",
      "step: 39, hamming: 0.0026796785137800714\n",
      "step: 40, hamming: 0.0022040249594432525\n",
      "step: 41, hamming: 0.0018116758740187896\n",
      "step: 42, hamming: 0.00148840570066346\n",
      "step: 43, hamming: 0.0012222941612371428\n",
      "step: 44, hamming: 0.001003397260817946\n",
      "step: 45, hamming: 0.0008234480506902851\n",
      "Running panda took: 1.45 seconds!\n",
      "  Elapsed time: 1.45 sec.\n",
      "Saving LIONESS network 3 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 4:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451852429809524\n",
      "step: 1, hamming: 0.6065219652377924\n",
      "step: 2, hamming: 0.6104344926479942\n",
      "step: 3, hamming: 0.5874558460006224\n",
      "step: 4, hamming: 0.5535449632471361\n",
      "step: 5, hamming: 0.5130643774408143\n",
      "step: 6, hamming: 0.4688705437378891\n",
      "step: 7, hamming: 0.4232496104307795\n",
      "step: 8, hamming: 0.37801701958351697\n",
      "step: 9, hamming: 0.3345340739226694\n",
      "step: 10, hamming: 0.2937480738815744\n",
      "step: 11, hamming: 0.2562488692873377\n",
      "step: 12, hamming: 0.22233727067256506\n",
      "step: 13, hamming: 0.19209248683867375\n",
      "step: 14, hamming: 0.16543446318988872\n",
      "step: 15, hamming: 0.1421747497816679\n",
      "step: 16, hamming: 0.12205609283083738\n",
      "step: 17, hamming: 0.10478214956850972\n",
      "step: 18, hamming: 0.09003785868900757\n",
      "step: 19, hamming: 0.07750337290976046\n",
      "step: 20, hamming: 0.06686386614000324\n",
      "step: 21, hamming: 0.05781785478415813\n",
      "step: 22, hamming: 0.05008562777061508\n",
      "step: 23, hamming: 0.04341877501262821\n",
      "step: 24, hamming: 0.03760951592560502\n",
      "step: 25, hamming: 0.032497011652074345\n",
      "step: 26, hamming: 0.02796736911372712\n",
      "step: 27, hamming: 0.02394642097778111\n",
      "step: 28, hamming: 0.020387520088663666\n",
      "step: 29, hamming: 0.017258473103985816\n",
      "step: 30, hamming: 0.01453135749256901\n",
      "step: 31, hamming: 0.012176759183597896\n",
      "step: 32, hamming: 0.010161970824797953\n",
      "step: 33, hamming: 0.008451681020184565\n",
      "step: 34, hamming: 0.007009691223106571\n",
      "step: 35, hamming: 0.005800680312786872\n",
      "step: 36, hamming: 0.004791555961913859\n",
      "step: 37, hamming: 0.003952280550194627\n",
      "step: 38, hamming: 0.003256239262093696\n",
      "step: 39, hamming: 0.002680275699607181\n",
      "step: 40, hamming: 0.0022045164857199522\n",
      "step: 41, hamming: 0.0018120803581844042\n",
      "step: 42, hamming: 0.001488738520630177\n",
      "step: 43, hamming: 0.0012225679351576834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 44, hamming: 0.0010036223951606135\n",
      "step: 45, hamming: 0.0008236331272743101\n",
      "Running panda took: 1.37 seconds!\n",
      "  Elapsed time: 1.37 sec.\n",
      "Saving LIONESS network 4 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 5:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446007808140573\n",
      "step: 1, hamming: 0.6066561612626608\n",
      "step: 2, hamming: 0.6105143267305174\n",
      "step: 3, hamming: 0.5875153809819785\n",
      "step: 4, hamming: 0.5536025207133696\n",
      "step: 5, hamming: 0.5131152115952171\n",
      "step: 6, hamming: 0.46890851450749593\n",
      "step: 7, hamming: 0.42326886776988776\n",
      "step: 8, hamming: 0.3780158211356529\n",
      "step: 9, hamming: 0.3345152633298887\n",
      "step: 10, hamming: 0.2937178419931415\n",
      "step: 11, hamming: 0.2562128416941286\n",
      "step: 12, hamming: 0.22229937209191528\n",
      "step: 13, hamming: 0.19205541566268614\n",
      "step: 14, hamming: 0.16539967610526804\n",
      "step: 15, hamming: 0.14214304912137624\n",
      "step: 16, hamming: 0.12202778290912641\n",
      "step: 17, hamming: 0.10475728352365837\n",
      "step: 18, hamming: 0.090016151593947\n",
      "step: 19, hamming: 0.07748448858975361\n",
      "step: 20, hamming: 0.06684754079961647\n",
      "step: 21, hamming: 0.05780382637771552\n",
      "step: 22, hamming: 0.050073651393493325\n",
      "step: 23, hamming: 0.04340859802170938\n",
      "step: 24, hamming: 0.0376009243886287\n",
      "step: 25, hamming: 0.032489808432629164\n",
      "step: 26, hamming: 0.027961372564714912\n",
      "step: 27, hamming: 0.023941467122490446\n",
      "step: 28, hamming: 0.020383446949155247\n",
      "step: 29, hamming: 0.01725513600007104\n",
      "step: 30, hamming: 0.014528627076186464\n",
      "step: 31, hamming: 0.012174525611025123\n",
      "step: 32, hamming: 0.010160143004486255\n",
      "step: 33, hamming: 0.008450183672932479\n",
      "step: 34, hamming: 0.0070084629144963285\n",
      "step: 35, hamming: 0.005799671567508494\n",
      "step: 36, hamming: 0.00479072675513629\n",
      "step: 37, hamming: 0.00395159839993978\n",
      "step: 38, hamming: 0.0032556778303245086\n",
      "step: 39, hamming: 0.002679813604130495\n",
      "step: 40, hamming: 0.0022041361378598878\n",
      "step: 41, hamming: 0.0018117673263748355\n",
      "step: 42, hamming: 0.0014884808938152954\n",
      "step: 43, hamming: 0.001222355966581887\n",
      "step: 44, hamming: 0.001003448045910429\n",
      "step: 45, hamming: 0.0008234897655709236\n",
      "Running panda took: 1.46 seconds!\n",
      "  Elapsed time: 1.46 sec.\n",
      "Saving LIONESS network 5 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 6:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449147271756762\n",
      "step: 1, hamming: 0.6066459340545902\n",
      "step: 2, hamming: 0.6105137892381169\n",
      "step: 3, hamming: 0.5875076422353582\n",
      "step: 4, hamming: 0.553580794787596\n",
      "step: 5, hamming: 0.5130868604034443\n",
      "step: 6, hamming: 0.4688806568825481\n",
      "step: 7, hamming: 0.42324491174509904\n",
      "step: 8, hamming: 0.37799750607572563\n",
      "step: 9, hamming: 0.3345019592116213\n",
      "step: 10, hamming: 0.29370783535904643\n",
      "step: 11, hamming: 0.25620490280375174\n",
      "step: 12, hamming: 0.2222927405692275\n",
      "step: 13, hamming: 0.19204960851436953\n",
      "step: 14, hamming: 0.16539452124403728\n",
      "step: 15, hamming: 0.14213836522146303\n",
      "step: 16, hamming: 0.12202359171841876\n",
      "step: 17, hamming: 0.1047535348014802\n",
      "step: 18, hamming: 0.09001300549732213\n",
      "step: 19, hamming: 0.07748198595168344\n",
      "step: 20, hamming: 0.06684561523161113\n",
      "step: 21, hamming: 0.057802393319260764\n",
      "step: 22, hamming: 0.05007261907425589\n",
      "step: 23, hamming: 0.043407887819397944\n",
      "step: 24, hamming: 0.03760046113425871\n",
      "step: 25, hamming: 0.032489527040096326\n",
      "step: 26, hamming: 0.027961217331469623\n",
      "step: 27, hamming: 0.023941395280352783\n",
      "step: 28, hamming: 0.020383428084858147\n",
      "step: 29, hamming: 0.01725514805667612\n",
      "step: 30, hamming: 0.01452865748618028\n",
      "step: 31, hamming: 0.012174566104760925\n",
      "step: 32, hamming: 0.010160186691797834\n",
      "step: 33, hamming: 0.008450226110780602\n",
      "step: 34, hamming: 0.007008501791631318\n",
      "step: 35, hamming: 0.005799706008307911\n",
      "step: 36, hamming: 0.004790756420696588\n",
      "step: 37, hamming: 0.003951623456964681\n",
      "step: 38, hamming: 0.0032556987003403777\n",
      "step: 39, hamming: 0.00267983077356759\n",
      "step: 40, hamming: 0.002204150177080668\n",
      "step: 41, hamming: 0.0018117787582195696\n",
      "step: 42, hamming: 0.0014884901773124216\n",
      "step: 43, hamming: 0.0012223635096057386\n",
      "step: 44, hamming: 0.0010034541834039374\n",
      "step: 45, hamming: 0.0008234947565908472\n",
      "Running panda took: 1.28 seconds!\n",
      "  Elapsed time: 1.28 sec.\n",
      "Saving LIONESS network 6 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 7:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446977027334075\n",
      "step: 1, hamming: 0.6064843916807414\n",
      "step: 2, hamming: 0.6103824327902414\n",
      "step: 3, hamming: 0.5873860105971278\n",
      "step: 4, hamming: 0.553462428350063\n",
      "step: 5, hamming: 0.5129759831080029\n",
      "step: 6, hamming: 0.46877915323218355\n",
      "step: 7, hamming: 0.42315760514309164\n",
      "step: 8, hamming: 0.37792629014527557\n",
      "step: 9, hamming: 0.3344456468150189\n",
      "step: 10, hamming: 0.29366421996136893\n",
      "step: 11, hamming: 0.25617114055220225\n",
      "step: 12, hamming: 0.22226630385566215\n",
      "step: 13, hamming: 0.19202874698683792\n",
      "step: 14, hamming: 0.1653778736143681\n",
      "step: 15, hamming: 0.14212484935803069\n",
      "step: 16, hamming: 0.12201243940238896\n",
      "step: 17, hamming: 0.10474418411446189\n",
      "step: 18, hamming: 0.09000497478051857\n",
      "step: 19, hamming: 0.07747497601974164\n",
      "step: 20, hamming: 0.06683946462194991\n",
      "step: 21, hamming: 0.05779698175620105\n",
      "step: 22, hamming: 0.05006786610992063\n",
      "step: 23, hamming: 0.04340371997910614\n",
      "step: 24, hamming: 0.03759681143075524\n",
      "step: 25, hamming: 0.03248634755930704\n",
      "step: 26, hamming: 0.02795846902706734\n",
      "step: 27, hamming: 0.02393903917646325\n",
      "step: 28, hamming: 0.02038142735532686\n",
      "step: 29, hamming: 0.017253462963678155\n",
      "step: 30, hamming: 0.014527247466191108\n",
      "step: 31, hamming: 0.012173391298399968\n",
      "step: 32, hamming: 0.01015921171833301\n",
      "step: 33, hamming: 0.008449420102928758\n",
      "step: 34, hamming: 0.007007837027271363\n",
      "step: 35, hamming: 0.005799158448934519\n",
      "step: 36, hamming: 0.0047903059897394145\n",
      "step: 37, hamming: 0.00395125337010865\n",
      "step: 38, hamming: 0.0032553948541319587\n",
      "step: 39, hamming: 0.0026795814132303665\n",
      "step: 40, hamming: 0.0022039455997269423\n",
      "step: 41, hamming: 0.0018116109753956073\n",
      "step: 42, hamming: 0.001488352605143334\n",
      "step: 43, hamming: 0.0012222507105209746\n",
      "step: 44, hamming: 0.0010033616948213154\n",
      "step: 45, hamming: 0.0008234189320553584\n",
      "Running panda took: 1.28 seconds!\n",
      "  Elapsed time: 1.28 sec.\n",
      "Saving LIONESS network 7 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 8:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6459338403224688\n",
      "step: 1, hamming: 0.6066595645190107\n",
      "step: 2, hamming: 0.6104610627797932\n",
      "step: 3, hamming: 0.5874377958138726\n",
      "step: 4, hamming: 0.5535034427944524\n",
      "step: 5, hamming: 0.5130076219885701\n",
      "step: 6, hamming: 0.4688044641107459\n",
      "step: 7, hamming: 0.423176090435273\n",
      "step: 8, hamming: 0.37793707228730244\n",
      "step: 9, hamming: 0.3344497542143437\n",
      "step: 10, hamming: 0.29366290145773\n",
      "step: 11, hamming: 0.2561661870411493\n",
      "step: 12, hamming: 0.22225964953219257\n",
      "step: 13, hamming: 0.19202134254653253\n",
      "step: 14, hamming: 0.16537035801892333\n",
      "step: 15, hamming: 0.14211767839413378\n",
      "step: 16, hamming: 0.12200586248374187\n",
      "step: 17, hamming: 0.10473836447075487\n",
      "step: 18, hamming: 0.08999993448755682\n",
      "step: 19, hamming: 0.07747066783111718\n",
      "step: 20, hamming: 0.06683582626078943\n",
      "step: 21, hamming: 0.05779392295165678\n",
      "step: 22, hamming: 0.05006528552822069\n",
      "step: 23, hamming: 0.04340155522633314\n",
      "step: 24, hamming: 0.037595006417562825\n",
      "step: 25, hamming: 0.03248484646178138\n",
      "step: 26, hamming: 0.02795721995514243\n",
      "step: 27, hamming: 0.023938001058869156\n",
      "step: 28, hamming: 0.020380561586156625\n",
      "step: 29, hamming: 0.017252738442971302\n",
      "step: 30, hamming: 0.014526640538941682\n",
      "step: 31, hamming: 0.01217288260449653\n",
      "step: 32, hamming: 0.010158785500792763\n",
      "step: 33, hamming: 0.008449063051904343\n",
      "step: 34, hamming: 0.007007538802914241\n",
      "step: 35, hamming: 0.005798910108587993\n",
      "step: 36, hamming: 0.004790099693035461\n",
      "step: 37, hamming: 0.003951082234771278\n",
      "step: 38, hamming: 0.003255253101132006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 39, hamming: 0.0026794641739463394\n",
      "step: 40, hamming: 0.0022038487570621673\n",
      "step: 41, hamming: 0.0018115310562277815\n",
      "step: 42, hamming: 0.0014882867079583828\n",
      "step: 43, hamming: 0.0012221964189122285\n",
      "step: 44, hamming: 0.0010033169964392779\n",
      "step: 45, hamming: 0.0008233821537042111\n",
      "Running panda took: 1.37 seconds!\n",
      "  Elapsed time: 1.37 sec.\n",
      "Saving LIONESS network 8 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 9:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6448351993831178\n",
      "step: 1, hamming: 0.6064879489440816\n",
      "step: 2, hamming: 0.6103640777269351\n",
      "step: 3, hamming: 0.5873512050264047\n",
      "step: 4, hamming: 0.5534219407870923\n",
      "step: 5, hamming: 0.5129356817661761\n",
      "step: 6, hamming: 0.46874212604373694\n",
      "step: 7, hamming: 0.42312367964198355\n",
      "step: 8, hamming: 0.37789569142567153\n",
      "step: 9, hamming: 0.33441876359267736\n",
      "step: 10, hamming: 0.2936404320381599\n",
      "step: 11, hamming: 0.25615039464431727\n",
      "step: 12, hamming: 0.2222483777256884\n",
      "step: 13, hamming: 0.1920133441212464\n",
      "step: 14, hamming: 0.16536460861948546\n",
      "step: 15, hamming: 0.14211341395771593\n",
      "step: 16, hamming: 0.12200253916357386\n",
      "step: 17, hamming: 0.10473558500674482\n",
      "step: 18, hamming: 0.08999750917615142\n",
      "step: 19, hamming: 0.07746846011445688\n",
      "step: 20, hamming: 0.0668337240386083\n",
      "step: 21, hamming: 0.05779188696902117\n",
      "step: 22, hamming: 0.05006331165089908\n",
      "step: 23, hamming: 0.043399650735111435\n",
      "step: 24, hamming: 0.03759319857032193\n",
      "step: 25, hamming: 0.03248315937467175\n",
      "step: 26, hamming: 0.02795567387570829\n",
      "step: 27, hamming: 0.023936604678199185\n",
      "step: 28, hamming: 0.02037932226923581\n",
      "step: 29, hamming: 0.01725165670698856\n",
      "step: 30, hamming: 0.014525707823730277\n",
      "step: 31, hamming: 0.012172086330504326\n",
      "step: 32, hamming: 0.010158111149147997\n",
      "step: 33, hamming: 0.008448495729481618\n",
      "step: 34, hamming: 0.007007063860617916\n",
      "step: 35, hamming: 0.00579851383581758\n",
      "step: 36, hamming: 0.004789769960958664\n",
      "step: 37, hamming: 0.003950808640232261\n",
      "step: 38, hamming: 0.0032550265635088458\n",
      "step: 39, hamming: 0.002679276999410677\n",
      "step: 40, hamming: 0.0022036943194786497\n",
      "step: 41, hamming: 0.001811403763151287\n",
      "step: 42, hamming: 0.0014881818789435535\n",
      "step: 43, hamming: 0.0012221101496973927\n",
      "step: 44, hamming: 0.0010032460442440812\n",
      "step: 45, hamming: 0.0008233238300691795\n",
      "Running panda took: 1.40 seconds!\n",
      "  Elapsed time: 1.40 sec.\n",
      "Saving LIONESS network 9 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 10:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446385831077224\n",
      "step: 1, hamming: 0.6067294163043444\n",
      "step: 2, hamming: 0.6105288665126055\n",
      "step: 3, hamming: 0.5875025168461764\n",
      "step: 4, hamming: 0.5535668756916815\n",
      "step: 5, hamming: 0.5130674611873675\n",
      "step: 6, hamming: 0.46885934016141356\n",
      "step: 7, hamming: 0.42322390362455425\n",
      "step: 8, hamming: 0.3779784610738321\n",
      "step: 9, hamming: 0.3344846104039679\n",
      "step: 10, hamming: 0.2936914468258334\n",
      "step: 11, hamming: 0.25618942581891635\n",
      "step: 12, hamming: 0.2222783089112929\n",
      "step: 13, hamming: 0.19203629520565335\n",
      "step: 14, hamming: 0.1653823459684807\n",
      "step: 15, hamming: 0.14212736448871935\n",
      "step: 16, hamming: 0.12201379782743489\n",
      "step: 17, hamming: 0.10474497350107494\n",
      "step: 18, hamming: 0.09000555414562793\n",
      "step: 19, hamming: 0.07747554704258397\n",
      "step: 20, hamming: 0.06684008398127479\n",
      "step: 21, hamming: 0.05779766899483368\n",
      "step: 22, hamming: 0.05006859159088394\n",
      "step: 23, hamming: 0.04340445929636734\n",
      "step: 24, hamming: 0.03759753082194868\n",
      "step: 25, hamming: 0.03248701536330191\n",
      "step: 26, hamming: 0.027959057287518876\n",
      "step: 27, hamming: 0.02393953329078775\n",
      "step: 28, hamming: 0.02038182485122052\n",
      "step: 29, hamming: 0.01725377205537819\n",
      "step: 30, hamming: 0.01452748155142716\n",
      "step: 31, hamming: 0.012173565486877314\n",
      "step: 32, hamming: 0.010159339518948408\n",
      "step: 33, hamming: 0.008449513032409811\n",
      "step: 34, hamming: 0.0070079049394139465\n",
      "step: 35, hamming: 0.005799208359310959\n",
      "step: 36, hamming: 0.004790342845117792\n",
      "step: 37, hamming: 0.003951280675493335\n",
      "step: 38, hamming: 0.0032554152008438713\n",
      "step: 39, hamming: 0.0026795966888212373\n",
      "step: 40, hamming: 0.002203957150287452\n",
      "step: 41, hamming: 0.0018116197541239662\n",
      "step: 42, hamming: 0.0014883593143951975\n",
      "step: 43, hamming: 0.0012222558625903603\n",
      "step: 44, hamming: 0.0010033656704848055\n",
      "step: 45, hamming: 0.0008234220134066718\n",
      "Running panda took: 1.48 seconds!\n",
      "  Elapsed time: 1.48 sec.\n",
      "Saving LIONESS network 10 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 11:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6445970723339552\n",
      "step: 1, hamming: 0.6063968880265759\n",
      "step: 2, hamming: 0.610362895089462\n",
      "step: 3, hamming: 0.5873891776463085\n",
      "step: 4, hamming: 0.5534802081887703\n",
      "step: 5, hamming: 0.5130020812531749\n",
      "step: 6, hamming: 0.4688120305062983\n",
      "step: 7, hamming: 0.42319558082755515\n",
      "step: 8, hamming: 0.377968666349301\n",
      "step: 9, hamming: 0.33449062529413504\n",
      "step: 10, hamming: 0.29370943003793337\n",
      "step: 11, hamming: 0.25621479946024567\n",
      "step: 12, hamming: 0.22230732905919456\n",
      "step: 13, hamming: 0.1920663351741951\n",
      "step: 14, hamming: 0.16541153798715838\n",
      "step: 15, hamming: 0.1421545477565385\n",
      "step: 16, hamming: 0.12203842025372977\n",
      "step: 17, hamming: 0.10476672314281757\n",
      "step: 18, hamming: 0.0900244111748685\n",
      "step: 19, hamming: 0.07749164789418152\n",
      "step: 20, hamming: 0.06685368125563591\n",
      "step: 21, hamming: 0.05780905110180941\n",
      "step: 22, hamming: 0.05007803092604655\n",
      "step: 23, hamming: 0.04341221333469125\n",
      "step: 24, hamming: 0.03760386116781324\n",
      "step: 25, hamming: 0.03249215842908325\n",
      "step: 26, hamming: 0.027963237690867417\n",
      "step: 27, hamming: 0.023942944227938814\n",
      "step: 28, hamming: 0.02038462017381501\n",
      "step: 29, hamming: 0.017256076965424815\n",
      "step: 30, hamming: 0.014529391608107678\n",
      "step: 31, hamming: 0.01217515416538417\n",
      "step: 32, hamming: 0.01016066495007358\n",
      "step: 33, hamming: 0.008450619213181505\n",
      "step: 34, hamming: 0.007008827032421282\n",
      "step: 35, hamming: 0.0057999760866447245\n",
      "step: 36, hamming: 0.004790981350177024\n",
      "step: 37, hamming: 0.003951810943599431\n",
      "step: 38, hamming: 0.003255854969433177\n",
      "step: 39, hamming: 0.0026799609209546132\n",
      "step: 40, hamming: 0.0022042584544433395\n",
      "step: 41, hamming: 0.0018118687383047569\n",
      "step: 42, hamming: 0.001488564864698104\n",
      "step: 43, hamming: 0.0012224254093928496\n",
      "step: 44, hamming: 0.001003505414610442\n",
      "step: 45, hamming: 0.0008235371154278103\n",
      "Running panda took: 1.46 seconds!\n",
      "  Elapsed time: 1.46 sec.\n",
      "Saving LIONESS network 11 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 12:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449227035246179\n",
      "step: 1, hamming: 0.6067351153769638\n",
      "step: 2, hamming: 0.6106375012468723\n",
      "step: 3, hamming: 0.5876437952563218\n",
      "step: 4, hamming: 0.5537178078810691\n",
      "step: 5, hamming: 0.5132213547717154\n",
      "step: 6, hamming: 0.4690101547789507\n",
      "step: 7, hamming: 0.42336856484025914\n",
      "step: 8, hamming: 0.37811364961234867\n",
      "step: 9, hamming: 0.3346090555291644\n",
      "step: 10, hamming: 0.29380462591393197\n",
      "step: 11, hamming: 0.25629077493594926\n",
      "step: 12, hamming: 0.2223680768718527\n",
      "step: 13, hamming: 0.1921150818070645\n",
      "step: 14, hamming: 0.1654509766214052\n",
      "step: 15, hamming: 0.14218673742040683\n",
      "step: 16, hamming: 0.12206484516686276\n",
      "step: 17, hamming: 0.10478861898320703\n",
      "step: 18, hamming: 0.09004269287907357\n",
      "step: 19, hamming: 0.0775070250720587\n",
      "step: 20, hamming: 0.06686675609772613\n",
      "step: 21, hamming: 0.05782028426552099\n",
      "step: 22, hamming: 0.05008776953393789\n",
      "step: 23, hamming: 0.043420696376732626\n",
      "step: 24, hamming: 0.037611262734358726\n",
      "step: 25, hamming: 0.03249860924051235\n",
      "step: 26, hamming: 0.027968830176749445\n",
      "step: 27, hamming: 0.023947752343236338\n",
      "step: 28, hamming: 0.020388722079863027\n",
      "step: 29, hamming: 0.017259546093626295\n",
      "step: 30, hamming: 0.014532302553875647\n",
      "step: 31, hamming: 0.01217758101100222\n",
      "step: 32, hamming: 0.010162676990644747\n",
      "step: 33, hamming: 0.00845228151463363\n",
      "step: 34, hamming: 0.0070101971767046085\n",
      "step: 35, hamming: 0.005801103434029777\n",
      "step: 36, hamming: 0.0047919078009273945\n",
      "step: 37, hamming: 0.003952571735671334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 38, hamming: 0.0032564793806740514\n",
      "step: 39, hamming: 0.0026804732113290546\n",
      "step: 40, hamming: 0.00220467863067728\n",
      "step: 41, hamming: 0.0018122132860573486\n",
      "step: 42, hamming: 0.0014888473474811738\n",
      "step: 43, hamming: 0.0012226569714781802\n",
      "step: 44, hamming: 0.0010036952106255875\n",
      "step: 45, hamming: 0.0008236926599718468\n",
      "Running panda took: 1.44 seconds!\n",
      "  Elapsed time: 1.44 sec.\n",
      "Saving LIONESS network 12 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 13:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6454815045482025\n",
      "step: 1, hamming: 0.6064888334615245\n",
      "step: 2, hamming: 0.610353785046755\n",
      "step: 3, hamming: 0.5873467987983988\n",
      "step: 4, hamming: 0.5534173891580667\n",
      "step: 5, hamming: 0.5129251262883208\n",
      "step: 6, hamming: 0.4687264900712108\n",
      "step: 7, hamming: 0.4231036921678549\n",
      "step: 8, hamming: 0.37787210728432713\n",
      "step: 9, hamming: 0.3343925087997385\n",
      "step: 10, hamming: 0.2936124890554121\n",
      "step: 11, hamming: 0.2561217809997432\n",
      "step: 12, hamming: 0.22222032595119312\n",
      "step: 13, hamming: 0.19198634671852602\n",
      "step: 14, hamming: 0.16533917584848354\n",
      "step: 15, hamming: 0.14208990495011609\n",
      "step: 16, hamming: 0.12198119524882962\n",
      "step: 17, hamming: 0.10471648707194936\n",
      "step: 18, hamming: 0.08998053903852753\n",
      "step: 19, hamming: 0.07745350260977645\n",
      "step: 20, hamming: 0.06682062957867617\n",
      "step: 21, hamming: 0.05778050118366357\n",
      "step: 22, hamming: 0.05005347176858947\n",
      "step: 23, hamming: 0.04339119603182728\n",
      "step: 24, hamming: 0.03758595561496705\n",
      "step: 25, hamming: 0.03247697829457956\n",
      "step: 26, hamming: 0.0279504168476757\n",
      "step: 27, hamming: 0.023932149875743877\n",
      "step: 28, hamming: 0.020375560744930254\n",
      "step: 29, hamming: 0.01724848885084861\n",
      "step: 30, hamming: 0.014523048953008225\n",
      "step: 31, hamming: 0.012169862212797655\n",
      "step: 32, hamming: 0.010156256059334054\n",
      "step: 33, hamming: 0.008446951777683006\n",
      "step: 34, hamming: 0.007005781213394501\n",
      "step: 35, hamming: 0.0057974502702537525\n",
      "step: 36, hamming: 0.004788889434590357\n",
      "step: 37, hamming: 0.00395008054412097\n",
      "step: 38, hamming: 0.003254425165595956\n",
      "step: 39, hamming: 0.0026787806466122715\n",
      "step: 40, hamming: 0.0022032850078452723\n",
      "step: 41, hamming: 0.0018110664850589518\n",
      "step: 42, hamming: 0.0014879041422859187\n",
      "step: 43, hamming: 0.0012218815774630238\n",
      "step: 44, hamming: 0.0010030580312243072\n",
      "step: 45, hamming: 0.0008231692507589999\n",
      "Running panda took: 1.63 seconds!\n",
      "  Elapsed time: 1.63 sec.\n",
      "Saving LIONESS network 13 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 14:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6443394902350985\n",
      "step: 1, hamming: 0.6066808238313083\n",
      "step: 2, hamming: 0.610500062853496\n",
      "step: 3, hamming: 0.5874753808094995\n",
      "step: 4, hamming: 0.5535342710047162\n",
      "step: 5, hamming: 0.5130322757657333\n",
      "step: 6, hamming: 0.46882485488303\n",
      "step: 7, hamming: 0.42319334974995554\n",
      "step: 8, hamming: 0.37795210281175695\n",
      "step: 9, hamming: 0.3344633679474344\n",
      "step: 10, hamming: 0.2936753764324572\n",
      "step: 11, hamming: 0.2561774279980078\n",
      "step: 12, hamming: 0.2222694459009427\n",
      "step: 13, hamming: 0.19203006056522645\n",
      "step: 14, hamming: 0.16537809623297536\n",
      "step: 15, hamming: 0.14212455805772894\n",
      "step: 16, hamming: 0.1220119905197326\n",
      "step: 17, hamming: 0.10474380875818415\n",
      "step: 18, hamming: 0.0900048204529394\n",
      "step: 19, hamming: 0.07747506491160117\n",
      "step: 20, hamming: 0.06683975370409437\n",
      "step: 21, hamming: 0.05779742984599143\n",
      "step: 22, hamming: 0.050068421477429015\n",
      "step: 23, hamming: 0.04340433264758675\n",
      "step: 24, hamming: 0.03759742735740401\n",
      "step: 25, hamming: 0.03248692677678964\n",
      "step: 26, hamming: 0.0279589832412535\n",
      "step: 27, hamming: 0.023939475652045558\n",
      "step: 28, hamming: 0.020381784048555823\n",
      "step: 29, hamming: 0.017253746643295972\n",
      "step: 30, hamming: 0.01452746881029944\n",
      "step: 31, hamming: 0.012173562253857687\n",
      "step: 32, hamming: 0.010159342835363386\n",
      "step: 33, hamming: 0.008449520673285471\n",
      "step: 34, hamming: 0.0070079141991438425\n",
      "step: 35, hamming: 0.00579921781960979\n",
      "step: 36, hamming: 0.004790352300931194\n",
      "step: 37, hamming: 0.003951289630160982\n",
      "step: 38, hamming: 0.003255423348636021\n",
      "step: 39, hamming: 0.0026796039355505563\n",
      "step: 40, hamming: 0.002203963484083054\n",
      "step: 41, hamming: 0.0018116252199462351\n",
      "step: 42, hamming: 0.0014883639807463862\n",
      "step: 43, hamming: 0.0012222598171972375\n",
      "step: 44, hamming: 0.0010033690024990156\n",
      "step: 45, hamming: 0.0008234248099945843\n",
      "Running panda took: 1.51 seconds!\n",
      "  Elapsed time: 1.51 sec.\n",
      "Saving LIONESS network 14 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 15:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6442838140322693\n",
      "step: 1, hamming: 0.6065432591877655\n",
      "step: 2, hamming: 0.6104256202756483\n",
      "step: 3, hamming: 0.587423936441978\n",
      "step: 4, hamming: 0.553504385366511\n",
      "step: 5, hamming: 0.513017729482651\n",
      "step: 6, hamming: 0.46881952598171256\n",
      "step: 7, hamming: 0.42319353876507343\n",
      "step: 8, hamming: 0.3779564465155286\n",
      "step: 9, hamming: 0.3344697612732674\n",
      "step: 10, hamming: 0.29368304260522193\n",
      "step: 11, hamming: 0.256185917293156\n",
      "step: 12, hamming: 0.2222781858231914\n",
      "step: 13, hamming: 0.19203847272328894\n",
      "step: 14, hamming: 0.16538591145842108\n",
      "step: 15, hamming: 0.14213154151205495\n",
      "step: 16, hamming: 0.12201804684367129\n",
      "step: 17, hamming: 0.1047489180035845\n",
      "step: 18, hamming: 0.09000896576805427\n",
      "step: 19, hamming: 0.07747832725281115\n",
      "step: 20, hamming: 0.06684227158737589\n",
      "step: 21, hamming: 0.05779932753307891\n",
      "step: 22, hamming: 0.05006981276657544\n",
      "step: 23, hamming: 0.0434053519354664\n",
      "step: 24, hamming: 0.037598189054180546\n",
      "step: 25, hamming: 0.032487520650437404\n",
      "step: 26, hamming: 0.027959467696440886\n",
      "step: 27, hamming: 0.02393988620692473\n",
      "step: 28, hamming: 0.020382138917549576\n",
      "step: 29, hamming: 0.017254056868073374\n",
      "step: 30, hamming: 0.014527740108929025\n",
      "step: 31, hamming: 0.012173798233668112\n",
      "step: 32, hamming: 0.010159546268615193\n",
      "step: 33, hamming: 0.008449694062619568\n",
      "step: 34, hamming: 0.007008060926552585\n",
      "step: 35, hamming: 0.005799341301245978\n",
      "step: 36, hamming: 0.004790455394888105\n",
      "step: 37, hamming: 0.00395137536393676\n",
      "step: 38, hamming: 0.0032554944712860395\n",
      "step: 39, hamming: 0.0026796628068911574\n",
      "step: 40, hamming: 0.002204012129634986\n",
      "step: 41, hamming: 0.0018116653686568124\n",
      "step: 42, hamming: 0.0014883970876660615\n",
      "step: 43, hamming: 0.0012222870952029131\n",
      "step: 44, hamming: 0.0010033914641085341\n",
      "step: 45, hamming: 0.0008234432936021232\n",
      "Running panda took: 1.87 seconds!\n",
      "  Elapsed time: 1.87 sec.\n",
      "Saving LIONESS network 15 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 16:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451357933509941\n",
      "step: 1, hamming: 0.6066460429428087\n",
      "step: 2, hamming: 0.6104672809769208\n",
      "step: 3, hamming: 0.5874429022191956\n",
      "step: 4, hamming: 0.5535053796996751\n",
      "step: 5, hamming: 0.5130102568327171\n",
      "step: 6, hamming: 0.4688078805102339\n",
      "step: 7, hamming: 0.4231820754210904\n",
      "step: 8, hamming: 0.37794630825110404\n",
      "step: 9, hamming: 0.3344617217511006\n",
      "step: 10, hamming: 0.29367695762019436\n",
      "step: 11, hamming: 0.2561810037908881\n",
      "step: 12, hamming: 0.22227406388621243\n",
      "step: 13, hamming: 0.19203490092286687\n",
      "step: 14, hamming: 0.16538281310135527\n",
      "step: 15, hamming: 0.14212890528332245\n",
      "step: 16, hamming: 0.12201583114277065\n",
      "step: 17, hamming: 0.10474707223510565\n",
      "step: 18, hamming: 0.09000753562825538\n",
      "step: 19, hamming: 0.07747728987948438\n",
      "step: 20, hamming: 0.0668415436219817\n",
      "step: 21, hamming: 0.05779884394184241\n",
      "step: 22, hamming: 0.05006952159265663\n",
      "step: 23, hamming: 0.04340519640748409\n",
      "step: 24, hamming: 0.03759811012297122\n",
      "step: 25, hamming: 0.03248747270773837\n",
      "step: 26, hamming: 0.027959425880675678\n",
      "step: 27, hamming: 0.023939839110948585\n",
      "step: 28, hamming: 0.020382085146816525\n",
      "step: 29, hamming: 0.01725399643445798\n",
      "step: 30, hamming: 0.014527676899527598\n",
      "step: 31, hamming: 0.01217373502349932\n",
      "step: 32, hamming: 0.010159486018898028\n",
      "step: 33, hamming: 0.00844963874092018\n",
      "step: 34, hamming: 0.007008011615450409\n",
      "step: 35, hamming: 0.0057992982643482755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 36, hamming: 0.004790418528106762\n",
      "step: 37, hamming: 0.003951344147935315\n",
      "step: 38, hamming: 0.0032554682222274782\n",
      "step: 39, hamming: 0.0026796408398129684\n",
      "step: 40, hamming: 0.002203993820812893\n",
      "step: 41, hamming: 0.0018116502262686585\n",
      "step: 42, hamming: 0.001488384584632186\n",
      "step: 43, hamming: 0.0012222767839663953\n",
      "step: 44, hamming: 0.0010033829666874518\n",
      "step: 45, hamming: 0.000823436296690418\n",
      "Running panda took: 1.47 seconds!\n",
      "  Elapsed time: 1.47 sec.\n",
      "Saving LIONESS network 16 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 17:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453372188039735\n",
      "step: 1, hamming: 0.6066676287287168\n",
      "step: 2, hamming: 0.6104644764183814\n",
      "step: 3, hamming: 0.5874393760760523\n",
      "step: 4, hamming: 0.553506577030067\n",
      "step: 5, hamming: 0.513009831711223\n",
      "step: 6, hamming: 0.46880493446531923\n",
      "step: 7, hamming: 0.42317541168503875\n",
      "step: 8, hamming: 0.3779353604570779\n",
      "step: 9, hamming: 0.3344472571824323\n",
      "step: 10, hamming: 0.2936601064819789\n",
      "step: 11, hamming: 0.2561633601925414\n",
      "step: 12, hamming: 0.2222568377191461\n",
      "step: 13, hamming: 0.19201879569174002\n",
      "step: 14, hamming: 0.16536830973066033\n",
      "step: 15, hamming: 0.14211622482773686\n",
      "step: 16, hamming: 0.1220048801756969\n",
      "step: 17, hamming: 0.10473769975276605\n",
      "step: 18, hamming: 0.08999949865507745\n",
      "step: 19, hamming: 0.07747042421728799\n",
      "step: 20, hamming: 0.06683571528299426\n",
      "step: 21, hamming: 0.057793931389394995\n",
      "step: 22, hamming: 0.0500654130944078\n",
      "step: 23, hamming: 0.043401792301824775\n",
      "step: 24, hamming: 0.037595325807174514\n",
      "step: 25, hamming: 0.032485225085799395\n",
      "step: 26, hamming: 0.027957631020622514\n",
      "step: 27, hamming: 0.023938413555977034\n",
      "step: 28, hamming: 0.020380954334854536\n",
      "step: 29, hamming: 0.017253098946343814\n",
      "step: 30, hamming: 0.01452696113853556\n",
      "step: 31, hamming: 0.0121731618457686\n",
      "step: 32, hamming: 0.010159024956166727\n",
      "step: 33, hamming: 0.008449266035746398\n",
      "step: 34, hamming: 0.00700770946888819\n",
      "step: 35, hamming: 0.005799052725515067\n",
      "step: 36, hamming: 0.004790218419084312\n",
      "step: 37, hamming: 0.0039511807920843516\n",
      "step: 38, hamming: 0.0032553347191006422\n",
      "step: 39, hamming: 0.0026795316594392425\n",
      "step: 40, hamming: 0.002203904483139546\n",
      "step: 41, hamming: 0.0018115770171323303\n",
      "step: 42, hamming: 0.0014883245770206238\n",
      "step: 43, hamming: 0.0012222275926846026\n",
      "step: 44, hamming: 0.0010033426427683537\n",
      "step: 45, hamming: 0.0008234032422371877\n",
      "Running panda took: 1.44 seconds!\n",
      "  Elapsed time: 1.44 sec.\n",
      "Saving LIONESS network 17 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 18:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449357768289141\n",
      "step: 1, hamming: 0.6067478759109104\n",
      "step: 2, hamming: 0.6105187387290015\n",
      "step: 3, hamming: 0.5874835466275927\n",
      "step: 4, hamming: 0.5535417550864121\n",
      "step: 5, hamming: 0.5130402196441705\n",
      "step: 6, hamming: 0.4688318533055642\n",
      "step: 7, hamming: 0.4231991954317866\n",
      "step: 8, hamming: 0.3779561923867192\n",
      "step: 9, hamming: 0.3344661242851883\n",
      "step: 10, hamming: 0.2936772734231136\n",
      "step: 11, hamming: 0.2561788722687788\n",
      "step: 12, hamming: 0.22227085717962167\n",
      "step: 13, hamming: 0.19203122760644709\n",
      "step: 14, hamming: 0.1653790756966992\n",
      "step: 15, hamming: 0.1421253806373162\n",
      "step: 16, hamming: 0.12201265787241682\n",
      "step: 17, hamming: 0.10474431714590451\n",
      "step: 18, hamming: 0.09000515595301478\n",
      "step: 19, hamming: 0.07747531799818602\n",
      "step: 20, hamming: 0.06683996904018245\n",
      "step: 21, hamming: 0.05779759760298822\n",
      "step: 22, hamming: 0.05006852906896944\n",
      "step: 23, hamming: 0.04340439208110958\n",
      "step: 24, hamming: 0.03759745306277047\n",
      "step: 25, hamming: 0.032486925619651025\n",
      "step: 26, hamming: 0.02795896080983829\n",
      "step: 27, hamming: 0.023939439452491516\n",
      "step: 28, hamming: 0.020381738530574928\n",
      "step: 29, hamming: 0.017253695360279313\n",
      "step: 30, hamming: 0.014527415793591236\n",
      "step: 31, hamming: 0.012173510704249178\n",
      "step: 32, hamming: 0.010159294487304502\n",
      "step: 33, hamming: 0.008449476113258234\n",
      "step: 34, hamming: 0.007007874287828733\n",
      "step: 35, hamming: 0.005799182820196558\n",
      "step: 36, hamming: 0.004790321840049439\n",
      "step: 37, hamming: 0.003951263491362449\n",
      "step: 38, hamming: 0.0032554011485745617\n",
      "step: 39, hamming: 0.0026795851877981086\n",
      "step: 40, hamming: 0.0022039477307740163\n",
      "step: 41, hamming: 0.0018116120424978204\n",
      "step: 42, hamming: 0.0014883529993387446\n",
      "step: 43, hamming: 0.001222250689760564\n",
      "step: 44, hamming: 0.00100336143314437\n",
      "step: 45, hamming: 0.0008234185581224598\n",
      "Running panda took: 1.39 seconds!\n",
      "  Elapsed time: 1.39 sec.\n",
      "Saving LIONESS network 18 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 19:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455081786434624\n",
      "step: 1, hamming: 0.6064194187335534\n",
      "step: 2, hamming: 0.6103607543150151\n",
      "step: 3, hamming: 0.5873862777167242\n",
      "step: 4, hamming: 0.553480413292402\n",
      "step: 5, hamming: 0.5130026093014715\n",
      "step: 6, hamming: 0.4688113588544863\n",
      "step: 7, hamming: 0.4231895358092202\n",
      "step: 8, hamming: 0.3779544175944803\n",
      "step: 9, hamming: 0.334468551809112\n",
      "step: 10, hamming: 0.29368214409431076\n",
      "step: 11, hamming: 0.2561851448411255\n",
      "step: 12, hamming: 0.22227745758160627\n",
      "step: 13, hamming: 0.19203763284596798\n",
      "step: 14, hamming: 0.16538496739496927\n",
      "step: 15, hamming: 0.14213058508151208\n",
      "step: 16, hamming: 0.12201710874059979\n",
      "step: 17, hamming: 0.10474810732061204\n",
      "step: 18, hamming: 0.0900083402499391\n",
      "step: 19, hamming: 0.0774778918911271\n",
      "step: 20, hamming: 0.06684199669576119\n",
      "step: 21, hamming: 0.05779919229936601\n",
      "step: 22, hamming: 0.05006980214737102\n",
      "step: 23, hamming: 0.043405433551497834\n",
      "step: 24, hamming: 0.03759834459496707\n",
      "step: 25, hamming: 0.03248771827770938\n",
      "step: 26, hamming: 0.027959694807564463\n",
      "step: 27, hamming: 0.02394012862051738\n",
      "step: 28, hamming: 0.02038238580092493\n",
      "step: 29, hamming: 0.017254298712404578\n",
      "step: 30, hamming: 0.014527969806710518\n",
      "step: 31, hamming: 0.012174010998701936\n",
      "step: 32, hamming: 0.010159739559041087\n",
      "step: 33, hamming: 0.008449866915345706\n",
      "step: 34, hamming: 0.007008213042126161\n",
      "step: 35, hamming: 0.005799473355581618\n",
      "step: 36, hamming: 0.004790568744366452\n",
      "step: 37, hamming: 0.003951471883857407\n",
      "step: 38, hamming: 0.0032555761831291967\n",
      "step: 39, hamming: 0.0026797315837817073\n",
      "step: 40, hamming: 0.0022040697609956353\n",
      "step: 41, hamming: 0.0018117134754357936\n",
      "step: 42, hamming: 0.001488437118285034\n",
      "step: 43, hamming: 0.001222320323319217\n",
      "step: 44, hamming: 0.001003418986989929\n",
      "step: 45, hamming: 0.0008234660519401803\n",
      "Running panda took: 1.68 seconds!\n",
      "  Elapsed time: 1.68 sec.\n",
      "Saving LIONESS network 19 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 20:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6439395298412646\n",
      "step: 1, hamming: 0.6064446364151149\n",
      "step: 2, hamming: 0.6103874530759167\n",
      "step: 3, hamming: 0.587378224328637\n",
      "step: 4, hamming: 0.5534408069230905\n",
      "step: 5, hamming: 0.5129461968109986\n",
      "step: 6, hamming: 0.4687480609928045\n",
      "step: 7, hamming: 0.42312972606616733\n",
      "step: 8, hamming: 0.37790354425769523\n",
      "step: 9, hamming: 0.3344283359725711\n",
      "step: 10, hamming: 0.29365126515501877\n",
      "step: 11, hamming: 0.2561617693952472\n",
      "step: 12, hamming: 0.2222599888792307\n",
      "step: 13, hamming: 0.19202452498598022\n",
      "step: 14, hamming: 0.16537501759571194\n",
      "step: 15, hamming: 0.14212288137746554\n",
      "step: 16, hamming: 0.1220110190534544\n",
      "step: 17, hamming: 0.10474308059192004\n",
      "step: 18, hamming: 0.09000401407761763\n",
      "step: 19, hamming: 0.0774741244284439\n",
      "step: 20, hamming: 0.06683863595804257\n",
      "step: 21, hamming: 0.057796138871261137\n",
      "step: 22, hamming: 0.05006700914121158\n",
      "step: 23, hamming: 0.04340287535360048\n",
      "step: 24, hamming: 0.03759601882243467\n",
      "step: 25, hamming: 0.032485634181046795\n",
      "step: 26, hamming: 0.027957849133881683\n",
      "step: 27, hamming: 0.023938508535798857\n",
      "step: 28, hamming: 0.020380978291102247\n",
      "step: 29, hamming: 0.01725308408299542\n",
      "step: 30, hamming: 0.014526928573715066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 31, hamming: 0.01217312410619027\n",
      "step: 32, hamming: 0.01015898896686592\n",
      "step: 33, hamming: 0.008449235814467613\n",
      "step: 34, hamming: 0.007007685785454703\n",
      "step: 35, hamming: 0.00579903522329828\n",
      "step: 36, hamming: 0.004790206180267455\n",
      "step: 37, hamming: 0.003951172904489711\n",
      "step: 38, hamming: 0.0032553302873377606\n",
      "step: 39, hamming: 0.0026795297588183783\n",
      "step: 40, hamming: 0.002203904360801872\n",
      "step: 41, hamming: 0.0018115780790296146\n",
      "step: 42, hamming: 0.0014883263714357698\n",
      "step: 43, hamming: 0.0012222297878657692\n",
      "step: 44, hamming: 0.0010033450031440966\n",
      "step: 45, hamming: 0.0008234056073751168\n",
      "Running panda took: 1.44 seconds!\n",
      "  Elapsed time: 1.44 sec.\n",
      "Saving LIONESS network 20 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 21:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450848367279335\n",
      "step: 1, hamming: 0.6066233811194319\n",
      "step: 2, hamming: 0.6104738378033745\n",
      "step: 3, hamming: 0.5874590505004481\n",
      "step: 4, hamming: 0.5535293117119651\n",
      "step: 5, hamming: 0.5130347272475934\n",
      "step: 6, hamming: 0.4688311364101987\n",
      "step: 7, hamming: 0.423201202950783\n",
      "step: 8, hamming: 0.3779600554810355\n",
      "step: 9, hamming: 0.33447046880848674\n",
      "step: 10, hamming: 0.29368153860449103\n",
      "step: 11, hamming: 0.256182992647495\n",
      "step: 12, hamming: 0.2222745986871024\n",
      "step: 13, hamming: 0.19203454863632288\n",
      "step: 14, hamming: 0.16538197139376912\n",
      "step: 15, hamming: 0.14212785490634228\n",
      "step: 16, hamming: 0.12201474746203655\n",
      "step: 17, hamming: 0.10474604616613903\n",
      "step: 18, hamming: 0.09000658579037368\n",
      "step: 19, hamming: 0.07747648716459518\n",
      "step: 20, hamming: 0.06684087416347566\n",
      "step: 21, hamming: 0.057798285097580865\n",
      "step: 22, hamming: 0.05006904051012411\n",
      "step: 23, hamming: 0.04340476637970672\n",
      "step: 24, hamming: 0.03759772603328491\n",
      "step: 25, hamming: 0.03248712996303483\n",
      "step: 26, hamming: 0.027959118987273036\n",
      "step: 27, hamming: 0.02393956316965707\n",
      "step: 28, hamming: 0.020381838360275333\n",
      "step: 29, hamming: 0.01725377713847119\n",
      "step: 30, hamming: 0.014527483895349053\n",
      "step: 31, hamming: 0.012173567305814021\n",
      "step: 32, hamming: 0.010159341931297581\n",
      "step: 33, hamming: 0.008449516089574678\n",
      "step: 34, hamming: 0.007007908113106247\n",
      "step: 35, hamming: 0.005799211545052685\n",
      "step: 36, hamming: 0.004790346079477176\n",
      "step: 37, hamming: 0.0039512838325598924\n",
      "step: 38, hamming: 0.0032554181896431323\n",
      "step: 39, hamming: 0.0026795994570336656\n",
      "step: 40, hamming: 0.002203959671213237\n",
      "step: 41, hamming: 0.0018116220164900933\n",
      "step: 42, hamming: 0.0014883613416886985\n",
      "step: 43, hamming: 0.0012222576717636868\n",
      "step: 44, hamming: 0.0010033672638236294\n",
      "step: 45, hamming: 0.0008234234034135413\n",
      "Running panda took: 1.49 seconds!\n",
      "  Elapsed time: 1.50 sec.\n",
      "Saving LIONESS network 21 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 22:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453859638093248\n",
      "step: 1, hamming: 0.6066965790988161\n",
      "step: 2, hamming: 0.610491835290849\n",
      "step: 3, hamming: 0.5874671302827961\n",
      "step: 4, hamming: 0.5535319083441806\n",
      "step: 5, hamming: 0.5130343438245889\n",
      "step: 6, hamming: 0.46883010704011946\n",
      "step: 7, hamming: 0.42319927931184576\n",
      "step: 8, hamming: 0.377958105463949\n",
      "step: 9, hamming: 0.3344683776483375\n",
      "step: 10, hamming: 0.2936795511371286\n",
      "step: 11, hamming: 0.2561812888003239\n",
      "step: 12, hamming: 0.22227300354666066\n",
      "step: 13, hamming: 0.19203305839021254\n",
      "step: 14, hamming: 0.1653805885957311\n",
      "step: 15, hamming: 0.14212657310449728\n",
      "step: 16, hamming: 0.12201356805469017\n",
      "step: 17, hamming: 0.10474498314173354\n",
      "step: 18, hamming: 0.09000563040511554\n",
      "step: 19, hamming: 0.07747556450073831\n",
      "step: 20, hamming: 0.06684002119191149\n",
      "step: 21, hamming: 0.05779753362320707\n",
      "step: 22, hamming: 0.05006839068356394\n",
      "step: 23, hamming: 0.0434042189139166\n",
      "step: 24, hamming: 0.037597278056102544\n",
      "step: 25, hamming: 0.032486767572642544\n",
      "step: 26, hamming: 0.02795883192482231\n",
      "step: 27, hamming: 0.023939341310999906\n",
      "step: 28, hamming: 0.0203816693839802\n",
      "step: 29, hamming: 0.017253650955294648\n",
      "step: 30, hamming: 0.014527390237321747\n",
      "step: 31, hamming: 0.01217349800996446\n",
      "step: 32, hamming: 0.01015929016531572\n",
      "step: 33, hamming: 0.008449476812725697\n",
      "step: 34, hamming: 0.007007877819352467\n",
      "step: 35, hamming: 0.005799187804456239\n",
      "step: 36, hamming: 0.004790327214202527\n",
      "step: 37, hamming: 0.0039512686521922325\n",
      "step: 38, hamming: 0.003255405875696191\n",
      "step: 39, hamming: 0.002679589392770105\n",
      "step: 40, hamming: 0.002203951397817051\n",
      "step: 41, hamming: 0.0018116151924508805\n",
      "step: 42, hamming: 0.001488355679500168\n",
      "step: 43, hamming: 0.001222252954038418\n",
      "step: 44, hamming: 0.0010033633357891538\n",
      "step: 45, hamming: 0.0008234201342693884\n",
      "Running panda took: 1.60 seconds!\n",
      "  Elapsed time: 1.60 sec.\n",
      "Saving LIONESS network 22 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 23:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6435616555805806\n",
      "step: 1, hamming: 0.6065774661127371\n",
      "step: 2, hamming: 0.610468595413635\n",
      "step: 3, hamming: 0.5874707578579492\n",
      "step: 4, hamming: 0.5535538083324445\n",
      "step: 5, hamming: 0.5130651148399519\n",
      "step: 6, hamming: 0.4688591784891974\n",
      "step: 7, hamming: 0.4232231791435239\n",
      "step: 8, hamming: 0.37797491630976726\n",
      "step: 9, hamming: 0.33447914071665064\n",
      "step: 10, hamming: 0.29368549481398054\n",
      "step: 11, hamming: 0.2561836385957451\n",
      "step: 12, hamming: 0.22227301663758198\n",
      "step: 13, hamming: 0.19203158876314616\n",
      "step: 14, hamming: 0.16537816530669008\n",
      "step: 15, hamming: 0.14212379235777955\n",
      "step: 16, hamming: 0.12201066274790016\n",
      "step: 17, hamming: 0.10474203578042608\n",
      "step: 18, hamming: 0.09000269743578625\n",
      "step: 19, hamming: 0.07747275376080467\n",
      "step: 20, hamming: 0.06683734743392257\n",
      "step: 21, hamming: 0.0577950210516337\n",
      "step: 22, hamming: 0.05006608553256088\n",
      "step: 23, hamming: 0.04340213107403853\n",
      "step: 24, hamming: 0.03759541702968891\n",
      "step: 25, hamming: 0.03248513867836511\n",
      "step: 26, hamming: 0.02795742639550285\n",
      "step: 27, hamming: 0.023938141602490862\n",
      "step: 28, hamming: 0.020380652363289783\n",
      "step: 29, hamming: 0.017252792040855003\n",
      "step: 30, hamming: 0.014526666851716616\n",
      "step: 31, hamming: 0.01217289023901222\n",
      "step: 32, hamming: 0.010158780660191595\n",
      "step: 33, hamming: 0.00844905057468564\n",
      "step: 34, hamming: 0.007007521946824564\n",
      "step: 35, hamming: 0.0057988910697919045\n",
      "step: 36, hamming: 0.0047900800900266445\n",
      "step: 37, hamming: 0.00395106318079814\n",
      "step: 38, hamming: 0.003255235281823731\n",
      "step: 39, hamming: 0.002679448012328946\n",
      "step: 40, hamming: 0.0022038343696844176\n",
      "step: 41, hamming: 0.0018115184280166996\n",
      "step: 42, hamming: 0.001488275744870974\n",
      "step: 43, hamming: 0.0012221869810817728\n",
      "step: 44, hamming: 0.0010033089289654207\n",
      "step: 45, hamming: 0.0008233752989717561\n",
      "Running panda took: 1.69 seconds!\n",
      "  Elapsed time: 1.69 sec.\n",
      "Saving LIONESS network 23 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 24:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451374874258013\n",
      "step: 1, hamming: 0.6065597248100142\n",
      "step: 2, hamming: 0.6104785268315843\n",
      "step: 3, hamming: 0.5874892432769232\n",
      "step: 4, hamming: 0.553567791572826\n",
      "step: 5, hamming: 0.5130739316093188\n",
      "step: 6, hamming: 0.46886944915852663\n",
      "step: 7, hamming: 0.42323727854509435\n",
      "step: 8, hamming: 0.3779939472775404\n",
      "step: 9, hamming: 0.3345015214724543\n",
      "step: 10, hamming: 0.2937094193739473\n",
      "step: 11, hamming: 0.2562077730351674\n",
      "step: 12, hamming: 0.22229629530279874\n",
      "step: 13, hamming: 0.1920534367947331\n",
      "step: 14, hamming: 0.1653983848286492\n",
      "step: 15, hamming: 0.14214207545812796\n",
      "step: 16, hamming: 0.1220271673144806\n",
      "step: 17, hamming: 0.10475689437070312\n",
      "step: 18, hamming: 0.09001608575237811\n",
      "step: 19, hamming: 0.0774847471094978\n",
      "step: 20, hamming: 0.06684806346197888\n",
      "step: 21, hamming: 0.05780455531349575\n",
      "step: 22, hamming: 0.05007450535329472\n",
      "step: 23, hamming: 0.043409517421342025\n",
      "step: 24, hamming: 0.03760185201290928\n",
      "step: 25, hamming: 0.032490701336341195\n",
      "step: 26, hamming: 0.027962198197821787\n",
      "step: 27, hamming: 0.023942208045912385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 28, hamming: 0.020384097306212396\n",
      "step: 29, hamming: 0.017255699648224446\n",
      "step: 30, hamming: 0.014529113182227082\n",
      "step: 31, hamming: 0.01217494163127554\n",
      "step: 32, hamming: 0.010160495993560507\n",
      "step: 33, hamming: 0.00845048121903037\n",
      "step: 34, hamming: 0.007008712784988987\n",
      "step: 35, hamming: 0.005799880479159791\n",
      "step: 36, hamming: 0.0047909007626825515\n",
      "step: 37, hamming: 0.003951742881773004\n",
      "step: 38, hamming: 0.003255797494304968\n",
      "step: 39, hamming: 0.002679912466988639\n",
      "step: 40, hamming: 0.0022042177025821627\n",
      "step: 41, hamming: 0.0018118345457225505\n",
      "step: 42, hamming: 0.001488536241043407\n",
      "step: 43, hamming: 0.0012224015057432389\n",
      "step: 44, hamming: 0.0010034854919391913\n",
      "step: 45, hamming: 0.0008235205432486311\n",
      "Running panda took: 1.63 seconds!\n",
      "  Elapsed time: 1.63 sec.\n",
      "Saving LIONESS network 24 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 25:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6457395627686134\n",
      "step: 1, hamming: 0.6064608877070141\n",
      "step: 2, hamming: 0.6103727825941946\n",
      "step: 3, hamming: 0.5873823343108445\n",
      "step: 4, hamming: 0.5534684140947953\n",
      "step: 5, hamming: 0.5129868713457605\n",
      "step: 6, hamming: 0.4687928920384118\n",
      "step: 7, hamming: 0.42317106468860927\n",
      "step: 8, hamming: 0.37793701543032326\n",
      "step: 9, hamming: 0.33445323750400024\n",
      "step: 10, hamming: 0.2936686849697661\n",
      "step: 11, hamming: 0.2561730726924933\n",
      "step: 12, hamming: 0.22226674885043102\n",
      "step: 13, hamming: 0.19202822641393982\n",
      "step: 14, hamming: 0.16537671830260126\n",
      "step: 15, hamming: 0.14212334810171473\n",
      "step: 16, hamming: 0.1220107685282419\n",
      "step: 17, hamming: 0.10474248793357539\n",
      "step: 18, hamming: 0.0900033014058808\n",
      "step: 19, hamming: 0.07747333480760729\n",
      "step: 20, hamming: 0.06683787049330008\n",
      "step: 21, hamming: 0.057795449353665554\n",
      "step: 22, hamming: 0.05006637373531308\n",
      "step: 23, hamming: 0.04340227376591354\n",
      "step: 24, hamming: 0.037595421359931186\n",
      "step: 25, hamming: 0.03248502656477799\n",
      "step: 26, hamming: 0.027957229203086794\n",
      "step: 27, hamming: 0.023937892823722058\n",
      "step: 28, hamming: 0.02038038336070406\n",
      "step: 29, hamming: 0.017252529167439003\n",
      "step: 30, hamming: 0.014526424760499897\n",
      "step: 31, hamming: 0.012172676375081907\n",
      "step: 32, hamming: 0.010158597046433316\n",
      "step: 33, hamming: 0.00844889606081156\n",
      "step: 34, hamming: 0.007007393568975931\n",
      "step: 35, hamming: 0.005798785453088683\n",
      "step: 36, hamming: 0.00478999387390706\n",
      "step: 37, hamming: 0.003950993159323441\n",
      "step: 38, hamming: 0.0032551785601338114\n",
      "step: 39, hamming: 0.0026794020858112943\n",
      "step: 40, hamming: 0.0022037972259520604\n",
      "step: 41, hamming: 0.001811488397401411\n",
      "step: 42, hamming: 0.0014882514628416364\n",
      "step: 43, hamming: 0.0012221673419373596\n",
      "step: 44, hamming: 0.0010032930382546355\n",
      "step: 45, hamming: 0.0008233624337668082\n",
      "Running panda took: 1.46 seconds!\n",
      "  Elapsed time: 1.46 sec.\n",
      "Saving LIONESS network 25 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 26:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6454967053255309\n",
      "step: 1, hamming: 0.6066220697004651\n",
      "step: 2, hamming: 0.6104569962998728\n",
      "step: 3, hamming: 0.5874394872323973\n",
      "step: 4, hamming: 0.5535107400339879\n",
      "step: 5, hamming: 0.5130174988541942\n",
      "step: 6, hamming: 0.46881371156127255\n",
      "step: 7, hamming: 0.423183189727278\n",
      "step: 8, hamming: 0.3779417340382216\n",
      "step: 9, hamming: 0.33445240537930465\n",
      "step: 10, hamming: 0.29366433099533695\n",
      "step: 11, hamming: 0.25616701474636144\n",
      "step: 12, hamming: 0.22226007042599116\n",
      "step: 13, hamming: 0.19202153205178443\n",
      "step: 14, hamming: 0.16537042896642054\n",
      "step: 15, hamming: 0.14211769909143143\n",
      "step: 16, hamming: 0.12200588715609277\n",
      "step: 17, hamming: 0.10473835923993632\n",
      "step: 18, hamming: 0.08999989292935066\n",
      "step: 19, hamming: 0.07747064160809622\n",
      "step: 20, hamming: 0.0668358086697313\n",
      "step: 21, hamming: 0.05779392045356974\n",
      "step: 22, hamming: 0.0500653033441485\n",
      "step: 23, hamming: 0.04340157748923955\n",
      "step: 24, hamming: 0.03759501485574575\n",
      "step: 25, hamming: 0.032484830736557\n",
      "step: 26, hamming: 0.02795717839135787\n",
      "step: 27, hamming: 0.023937933402786545\n",
      "step: 28, hamming: 0.020380473632123763\n",
      "step: 29, hamming: 0.017252638944378788\n",
      "step: 30, hamming: 0.014526536911014595\n",
      "step: 31, hamming: 0.012172781802429688\n",
      "step: 32, hamming: 0.010158691621547707\n",
      "step: 33, hamming: 0.008448978359003058\n",
      "step: 34, hamming: 0.007007463803651777\n",
      "step: 35, hamming: 0.005798844650285859\n",
      "step: 36, hamming: 0.004790043434414072\n",
      "step: 37, hamming: 0.003951034364586719\n",
      "step: 38, hamming: 0.0032552126647618614\n",
      "step: 39, hamming: 0.0026794302159438544\n",
      "step: 40, hamming: 0.002203820368039428\n",
      "step: 41, hamming: 0.0018115074140994402\n",
      "step: 42, hamming: 0.0014882670710365021\n",
      "step: 43, hamming: 0.001222180142905843\n",
      "step: 44, hamming: 0.0010033035277769954\n",
      "step: 45, hamming: 0.0008233710253226156\n",
      "Running panda took: 1.31 seconds!\n",
      "  Elapsed time: 1.31 sec.\n",
      "Saving LIONESS network 26 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 27:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453631316746891\n",
      "step: 1, hamming: 0.6065851763245883\n",
      "step: 2, hamming: 0.6104687108084997\n",
      "step: 3, hamming: 0.5874708414567839\n",
      "step: 4, hamming: 0.553550407750318\n",
      "step: 5, hamming: 0.5130590911886108\n",
      "step: 6, hamming: 0.46885507018779043\n",
      "step: 7, hamming: 0.42322248970490667\n",
      "step: 8, hamming: 0.3779776691218549\n",
      "step: 9, hamming: 0.3344846767623718\n",
      "step: 10, hamming: 0.2936929239245133\n",
      "step: 11, hamming: 0.25619229570609636\n",
      "step: 12, hamming: 0.22228219573712402\n",
      "step: 13, hamming: 0.19204076380694882\n",
      "step: 14, hamming: 0.16538705195619063\n",
      "step: 15, hamming: 0.1421320864636762\n",
      "step: 16, hamming: 0.12201829280274731\n",
      "step: 17, hamming: 0.10474902300437654\n",
      "step: 18, hamming: 0.09000903326934918\n",
      "step: 19, hamming: 0.07747844392157899\n",
      "step: 20, hamming: 0.06684245770007373\n",
      "step: 21, hamming: 0.05779957445832755\n",
      "step: 22, hamming: 0.05007009118596108\n",
      "step: 23, hamming: 0.04340563042618257\n",
      "step: 24, hamming: 0.03759844243481214\n",
      "step: 25, hamming: 0.032487722556562154\n",
      "step: 26, hamming: 0.027959603794912816\n",
      "step: 27, hamming: 0.023939953980151817\n",
      "step: 28, hamming: 0.020382151479531888\n",
      "step: 29, hamming: 0.017254024423504023\n",
      "step: 30, hamming: 0.01452767604514493\n",
      "step: 31, hamming: 0.012173715629341518\n",
      "step: 32, hamming: 0.010159456372537531\n",
      "step: 33, hamming: 0.008449604489718605\n",
      "step: 34, hamming: 0.007007976542379953\n",
      "step: 35, hamming: 0.005799264728544192\n",
      "step: 36, hamming: 0.004790387758916941\n",
      "step: 37, hamming: 0.003951316728991019\n",
      "step: 38, hamming: 0.003255444306355403\n",
      "step: 39, hamming: 0.0026796202983168293\n",
      "step: 40, hamming: 0.002203976375195701\n",
      "step: 41, hamming: 0.0018116354595427964\n",
      "step: 42, hamming: 0.0014883721681424754\n",
      "step: 43, hamming: 0.0012222663986386547\n",
      "step: 44, hamming: 0.0010033743151023403\n",
      "step: 45, hamming: 0.0008234291107443008\n",
      "Running panda took: 1.40 seconds!\n",
      "  Elapsed time: 1.40 sec.\n",
      "Saving LIONESS network 27 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 28:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451358310824151\n",
      "step: 1, hamming: 0.6067202812268652\n",
      "step: 2, hamming: 0.6105084509106121\n",
      "step: 3, hamming: 0.5874770766110782\n",
      "step: 4, hamming: 0.5535401297746914\n",
      "step: 5, hamming: 0.5130406798275531\n",
      "step: 6, hamming: 0.468834505899187\n",
      "step: 7, hamming: 0.42320295085821286\n",
      "step: 8, hamming: 0.37796147500536265\n",
      "step: 9, hamming: 0.33447178250965853\n",
      "step: 10, hamming: 0.29368269158902255\n",
      "step: 11, hamming: 0.2561841423380478\n",
      "step: 12, hamming: 0.22227561563567477\n",
      "step: 13, hamming: 0.19203539669004427\n",
      "step: 14, hamming: 0.16538265059792898\n",
      "step: 15, hamming: 0.14212839414046055\n",
      "step: 16, hamming: 0.12201518520640055\n",
      "step: 17, hamming: 0.10474641185408667\n",
      "step: 18, hamming: 0.0900068778671942\n",
      "step: 19, hamming: 0.07747669705672466\n",
      "step: 20, hamming: 0.06684104136237137\n",
      "step: 21, hamming: 0.05779843506391113\n",
      "step: 22, hamming: 0.0500691794290296\n",
      "step: 23, hamming: 0.04340488739282701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 24, hamming: 0.03759781991517671\n",
      "step: 25, hamming: 0.032487194607542826\n",
      "step: 26, hamming: 0.027959154377771425\n",
      "step: 27, hamming: 0.02393957466123415\n",
      "step: 28, hamming: 0.02038183196273885\n",
      "step: 29, hamming: 0.0172537603782067\n",
      "step: 30, hamming: 0.014527461133636087\n",
      "step: 31, hamming: 0.012173542661750416\n",
      "step: 32, hamming: 0.010159317402521097\n",
      "step: 33, hamming: 0.008449492965825199\n",
      "step: 34, hamming: 0.0070078870284569885\n",
      "step: 35, hamming: 0.005799192688495383\n",
      "step: 36, hamming: 0.004790329522725734\n",
      "step: 37, hamming: 0.003951269494190278\n",
      "step: 38, hamming: 0.0032554058740101068\n",
      "step: 39, hamming: 0.002679588945293777\n",
      "step: 40, hamming: 0.002203950746453343\n",
      "step: 41, hamming: 0.0018116144780692205\n",
      "step: 42, hamming: 0.0014883549740263417\n",
      "step: 43, hamming: 0.0012222522977348154\n",
      "step: 44, hamming: 0.0010033627438220327\n",
      "step: 45, hamming: 0.0008234196121337722\n",
      "Running panda took: 1.47 seconds!\n",
      "  Elapsed time: 1.47 sec.\n",
      "Saving LIONESS network 28 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 29:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455351008769453\n",
      "step: 1, hamming: 0.606672749921852\n",
      "step: 2, hamming: 0.6104835041920981\n",
      "step: 3, hamming: 0.5874613800094756\n",
      "step: 4, hamming: 0.5535261027676674\n",
      "step: 5, hamming: 0.5130277394158711\n",
      "step: 6, hamming: 0.4688217647975806\n",
      "step: 7, hamming: 0.42319024909743624\n",
      "step: 8, hamming: 0.3779481583579115\n",
      "step: 9, hamming: 0.3344579933187368\n",
      "step: 10, hamming: 0.29366902135579265\n",
      "step: 11, hamming: 0.25617054999127764\n",
      "step: 12, hamming: 0.22226262032373947\n",
      "step: 13, hamming: 0.19202326345396323\n",
      "step: 14, hamming: 0.16537148632869755\n",
      "step: 15, hamming: 0.142118264201008\n",
      "step: 16, hamming: 0.12200609786553838\n",
      "step: 17, hamming: 0.10473832871874272\n",
      "step: 18, hamming: 0.08999971271249907\n",
      "step: 19, hamming: 0.07747039064766185\n",
      "step: 20, hamming: 0.06683555388538215\n",
      "step: 21, hamming: 0.057793686110969834\n",
      "step: 22, hamming: 0.05006509427037561\n",
      "step: 23, hamming: 0.0434014084405363\n",
      "step: 24, hamming: 0.03759489383654498\n",
      "step: 25, hamming: 0.03248476025786066\n",
      "step: 26, hamming: 0.027957153083011842\n",
      "step: 27, hamming: 0.02393794536063863\n",
      "step: 28, hamming: 0.020380515556205\n",
      "step: 29, hamming: 0.017252700679045323\n",
      "step: 30, hamming: 0.014526608601153243\n",
      "step: 31, hamming: 0.012172855437592786\n",
      "step: 32, hamming: 0.010158762007530757\n",
      "step: 33, hamming: 0.008449042800011774\n",
      "step: 34, hamming: 0.007007520983952693\n",
      "step: 35, hamming: 0.0057988942286212805\n",
      "step: 36, hamming: 0.004790085658922592\n",
      "step: 37, hamming: 0.003951069946375388\n",
      "step: 38, hamming: 0.003255242407783724\n",
      "step: 39, hamming: 0.0026794549366581924\n",
      "step: 40, hamming: 0.0022038408329425082\n",
      "step: 41, hamming: 0.0018115242912096789\n",
      "step: 42, hamming: 0.0014882809567832705\n",
      "step: 43, hamming: 0.0012221915474429238\n",
      "step: 44, hamming: 0.0010033128856263504\n",
      "step: 45, hamming: 0.0008233786956426912\n",
      "Running panda took: 1.48 seconds!\n",
      "  Elapsed time: 1.48 sec.\n",
      "Saving LIONESS network 29 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 30:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6441925069818978\n",
      "step: 1, hamming: 0.6063136112432318\n",
      "step: 2, hamming: 0.6103932208346182\n",
      "step: 3, hamming: 0.5874474722143483\n",
      "step: 4, hamming: 0.5535649464335758\n",
      "step: 5, hamming: 0.5130928337695615\n",
      "step: 6, hamming: 0.4688939095266525\n",
      "step: 7, hamming: 0.42325885276505987\n",
      "step: 8, hamming: 0.3780087198271526\n",
      "step: 9, hamming: 0.3345100558700422\n",
      "step: 10, hamming: 0.29371427477324347\n",
      "step: 11, hamming: 0.2562107598757153\n",
      "step: 12, hamming: 0.22229857181764945\n",
      "step: 13, hamming: 0.19205531145425267\n",
      "step: 14, hamming: 0.16539978011088832\n",
      "step: 15, hamming: 0.14214287312471557\n",
      "step: 16, hamming: 0.12202708424928997\n",
      "step: 17, hamming: 0.10475587118837182\n",
      "step: 18, hamming: 0.09001400820385419\n",
      "step: 19, hamming: 0.07748166638509849\n",
      "step: 20, hamming: 0.06684413555730546\n",
      "step: 21, hamming: 0.05779998950120317\n",
      "step: 22, hamming: 0.050069556133776956\n",
      "step: 23, hamming: 0.04340444199041316\n",
      "step: 24, hamming: 0.03759688450174655\n",
      "step: 25, hamming: 0.03248603443830073\n",
      "step: 26, hamming: 0.027957966759979882\n",
      "step: 27, hamming: 0.023938483652704984\n",
      "step: 28, hamming: 0.020380896229155798\n",
      "step: 29, hamming: 0.017252993901731097\n",
      "step: 30, hamming: 0.014526852479589947\n",
      "step: 31, hamming: 0.012173067882295998\n",
      "step: 32, hamming: 0.010158950869745383\n",
      "step: 33, hamming: 0.008449210882153614\n",
      "step: 34, hamming: 0.007007669688350223\n",
      "step: 35, hamming: 0.0057990246081408205\n",
      "step: 36, hamming: 0.0047901987375082965\n",
      "step: 37, hamming: 0.003951167141327133\n",
      "step: 38, hamming: 0.003255325350745026\n",
      "step: 39, hamming: 0.0026795253550045642\n",
      "step: 40, hamming: 0.00220390032018264\n",
      "step: 41, hamming: 0.00181157433539964\n",
      "step: 42, hamming: 0.0014883229143649352\n",
      "step: 43, hamming: 0.0012222266229141624\n",
      "step: 44, hamming: 0.0010033421337824068\n",
      "step: 45, hamming: 0.0008234030345286952\n",
      "Running panda took: 1.34 seconds!\n",
      "  Elapsed time: 1.34 sec.\n",
      "Saving LIONESS network 30 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 31:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6453374894828801\n",
      "step: 1, hamming: 0.6066967069145408\n",
      "step: 2, hamming: 0.6104840389078136\n",
      "step: 3, hamming: 0.5874565733434927\n",
      "step: 4, hamming: 0.5535188219795216\n",
      "step: 5, hamming: 0.513020487378786\n",
      "step: 6, hamming: 0.46881505964233633\n",
      "step: 7, hamming: 0.4231847518707811\n",
      "step: 8, hamming: 0.37794456875278254\n",
      "step: 9, hamming: 0.33445666843682814\n",
      "step: 10, hamming: 0.29366950408238246\n",
      "step: 11, hamming: 0.25617272617649023\n",
      "step: 12, hamming: 0.2222658147874626\n",
      "step: 13, hamming: 0.1920270800407892\n",
      "step: 14, hamming: 0.1653756413311506\n",
      "step: 15, hamming: 0.14212251274972568\n",
      "step: 16, hamming: 0.12201028566869024\n",
      "step: 17, hamming: 0.10474235378727428\n",
      "step: 18, hamming: 0.09000359838145934\n",
      "step: 19, hamming: 0.07747406299496645\n",
      "step: 20, hamming: 0.06683892024429142\n",
      "step: 21, hamming: 0.05779672357526197\n",
      "step: 22, hamming: 0.05006780392032587\n",
      "step: 23, hamming: 0.043403791059584765\n",
      "step: 24, hamming: 0.03759695373852516\n",
      "step: 25, hamming: 0.032486517555771505\n",
      "step: 26, hamming: 0.027958627760509547\n",
      "step: 27, hamming: 0.023939165045318303\n",
      "step: 28, hamming: 0.020381512374332632\n",
      "step: 29, hamming: 0.017253509321226976\n",
      "step: 30, hamming: 0.01452726248992399\n",
      "step: 31, hamming: 0.01217338409078206\n",
      "step: 32, hamming: 0.01015918998360408\n",
      "step: 33, hamming: 0.008449389939631924\n",
      "step: 34, hamming: 0.007007803396138958\n",
      "step: 35, hamming: 0.00579912466367174\n",
      "step: 36, hamming: 0.004790274070575606\n",
      "step: 37, hamming: 0.0039512242592668854\n",
      "step: 38, hamming: 0.0032553690960775276\n",
      "step: 39, hamming: 0.0026795590154728125\n",
      "step: 40, hamming: 0.002203926367689019\n",
      "step: 41, hamming: 0.0018115946043863235\n",
      "step: 42, hamming: 0.0014883387657010927\n",
      "step: 43, hamming: 0.0012222390734737921\n",
      "step: 44, hamming: 0.0010033519517067167\n",
      "step: 45, hamming: 0.000823410802845809\n",
      "Running panda took: 1.45 seconds!\n",
      "  Elapsed time: 1.45 sec.\n",
      "Saving LIONESS network 31 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 32:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449746310280959\n",
      "step: 1, hamming: 0.6067058332082043\n",
      "step: 2, hamming: 0.6105227585080634\n",
      "step: 3, hamming: 0.5874998685226731\n",
      "step: 4, hamming: 0.5535644118765825\n",
      "step: 5, hamming: 0.5130637409325514\n",
      "step: 6, hamming: 0.46885520719749507\n",
      "step: 7, hamming: 0.4232205988814208\n",
      "step: 8, hamming: 0.37797509767336696\n",
      "step: 9, hamming: 0.33448191453778936\n",
      "step: 10, hamming: 0.2936901232265988\n",
      "step: 11, hamming: 0.25618939533535684\n",
      "step: 12, hamming: 0.22227946486100847\n",
      "step: 13, hamming: 0.19203833582770793\n",
      "step: 14, hamming: 0.16538499865483383\n",
      "step: 15, hamming: 0.14213033937025302\n",
      "step: 16, hamming: 0.12201684471858767\n",
      "step: 17, hamming: 0.10474787876788032\n",
      "step: 18, hamming: 0.09000825020978294\n",
      "step: 19, hamming: 0.07747800883938391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 20, hamming: 0.06684228198432347\n",
      "step: 21, hamming: 0.057799594299135344\n",
      "step: 22, hamming: 0.050070253711365226\n",
      "step: 23, hamming: 0.043405873825744117\n",
      "step: 24, hamming: 0.03759871933174081\n",
      "step: 25, hamming: 0.03248800162100365\n",
      "step: 26, hamming: 0.027959871764145786\n",
      "step: 27, hamming: 0.023940202435957004\n",
      "step: 28, hamming: 0.0203823727736058\n",
      "step: 29, hamming: 0.01725422092678858\n",
      "step: 30, hamming: 0.014527849564486101\n",
      "step: 31, hamming: 0.01217386793079806\n",
      "step: 32, hamming: 0.010159588465741652\n",
      "step: 33, hamming: 0.008449718062226332\n",
      "step: 34, hamming: 0.007008073488535551\n",
      "step: 35, hamming: 0.005799346994199016\n",
      "step: 36, hamming: 0.004790456976200907\n",
      "step: 37, hamming: 0.003951374637817454\n",
      "step: 38, hamming: 0.003255492594470654\n",
      "step: 39, hamming: 0.0026796605246515556\n",
      "step: 40, hamming: 0.002204009784151208\n",
      "step: 41, hamming: 0.0018116631348835933\n",
      "step: 42, hamming: 0.0014883950460219354\n",
      "step: 43, hamming: 0.001222285278787714\n",
      "step: 44, hamming: 0.0010033898748033553\n",
      "step: 45, hamming: 0.0008234419197268525\n",
      "Running panda took: 1.49 seconds!\n",
      "  Elapsed time: 1.49 sec.\n",
      "Saving LIONESS network 32 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 33:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.645656780846221\n",
      "step: 1, hamming: 0.6066002678232713\n",
      "step: 2, hamming: 0.6105813100451256\n",
      "step: 3, hamming: 0.5876366440945432\n",
      "step: 4, hamming: 0.5537384939042065\n",
      "step: 5, hamming: 0.5132510551234207\n",
      "step: 6, hamming: 0.46903516653762006\n",
      "step: 7, hamming: 0.42338117383780194\n",
      "step: 8, hamming: 0.3781135868865883\n",
      "step: 9, hamming: 0.33459944752452325\n",
      "step: 10, hamming: 0.2937892636785368\n",
      "step: 11, hamming: 0.25627321887464255\n",
      "step: 12, hamming: 0.22235047832920657\n",
      "step: 13, hamming: 0.1920987613785881\n",
      "step: 14, hamming: 0.16543669817665527\n",
      "step: 15, hamming: 0.14217460617494093\n",
      "step: 16, hamming: 0.12205478208053704\n",
      "step: 17, hamming: 0.1047804213053023\n",
      "step: 18, hamming: 0.09003612775603831\n",
      "step: 19, hamming: 0.07750184049531987\n",
      "step: 20, hamming: 0.06686269734372635\n",
      "step: 21, hamming: 0.057817121269325866\n",
      "step: 22, hamming: 0.050085333681801514\n",
      "step: 23, hamming: 0.043418854271905304\n",
      "step: 24, hamming: 0.037609897086251905\n",
      "step: 25, hamming: 0.03249761930169969\n",
      "step: 26, hamming: 0.02796812163890966\n",
      "step: 27, hamming: 0.02394725318637872\n",
      "step: 28, hamming: 0.020388370931300064\n",
      "step: 29, hamming: 0.01725930031609945\n",
      "step: 30, hamming: 0.014532130574808764\n",
      "step: 31, hamming: 0.012177459602951909\n",
      "step: 32, hamming: 0.010162590210066961\n",
      "step: 33, hamming: 0.008452218615141232\n",
      "step: 34, hamming: 0.007010150880511294\n",
      "step: 35, hamming: 0.005801068940490524\n",
      "step: 36, hamming: 0.004791881667170446\n",
      "step: 37, hamming: 0.003952551627023813\n",
      "step: 38, hamming: 0.0032564637102903927\n",
      "step: 39, hamming: 0.0026804608719384142\n",
      "step: 40, hamming: 0.002204668841868316\n",
      "step: 41, hamming: 0.0018122054772229607\n",
      "step: 42, hamming: 0.0014888410848218324\n",
      "step: 43, hamming: 0.0012226519307712387\n",
      "step: 44, hamming: 0.0010036911397340626\n",
      "step: 45, hamming: 0.0008236893647987569\n",
      "Running panda took: 1.57 seconds!\n",
      "  Elapsed time: 1.57 sec.\n",
      "Saving LIONESS network 33 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 34:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6449981971449824\n",
      "step: 1, hamming: 0.6065931831332113\n",
      "step: 2, hamming: 0.6105293103087643\n",
      "step: 3, hamming: 0.5875457591281373\n",
      "step: 4, hamming: 0.5536260899397788\n",
      "step: 5, hamming: 0.5131280856850265\n",
      "step: 6, hamming: 0.4689154027720692\n",
      "step: 7, hamming: 0.42327401236915707\n",
      "step: 8, hamming: 0.37802087584233957\n",
      "step: 9, hamming: 0.3345210485552016\n",
      "step: 10, hamming: 0.2937231499498878\n",
      "step: 11, hamming: 0.25621717754897094\n",
      "step: 12, hamming: 0.2223028366559903\n",
      "step: 13, hamming: 0.19205800982166413\n",
      "step: 14, hamming: 0.16540159888503908\n",
      "step: 15, hamming: 0.14214436276067294\n",
      "step: 16, hamming: 0.12202874055933112\n",
      "step: 17, hamming: 0.10475804762546265\n",
      "step: 18, hamming: 0.09001694647451311\n",
      "step: 19, hamming: 0.07748544549209825\n",
      "step: 20, hamming: 0.06684869516951136\n",
      "step: 21, hamming: 0.05780513650530213\n",
      "step: 22, hamming: 0.0500750561258025\n",
      "step: 23, hamming: 0.04341002702171404\n",
      "step: 24, hamming: 0.03760230583906534\n",
      "step: 25, hamming: 0.032491088953373076\n",
      "step: 26, hamming: 0.027962521329952538\n",
      "step: 27, hamming: 0.02394247114838837\n",
      "step: 28, hamming: 0.020384308356420206\n",
      "step: 29, hamming: 0.017255866049331867\n",
      "step: 30, hamming: 0.0145292430032475\n",
      "step: 31, hamming: 0.012175043490551072\n",
      "step: 32, hamming: 0.010160576904046068\n",
      "step: 33, hamming: 0.008450545793374795\n",
      "step: 34, hamming: 0.00700876423318378\n",
      "step: 35, hamming: 0.005799922014886081\n",
      "step: 36, hamming: 0.0047909345756680976\n",
      "step: 37, hamming: 0.003951770460150725\n",
      "step: 38, hamming: 0.003255820005222335\n",
      "step: 39, hamming: 0.0026799308615177105\n",
      "step: 40, hamming: 0.0022042327414596497\n",
      "step: 41, hamming: 0.0018118468450875748\n",
      "step: 42, hamming: 0.0014885463019372101\n",
      "step: 43, hamming: 0.0012224097309378745\n",
      "step: 44, hamming: 0.0010034922163639059\n",
      "step: 45, hamming: 0.0008235260380245241\n",
      "Running panda took: 1.42 seconds!\n",
      "  Elapsed time: 1.42 sec.\n",
      "Saving LIONESS network 34 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 35:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6426000250494666\n",
      "step: 1, hamming: 0.6063071921865066\n",
      "step: 2, hamming: 0.6104809784525866\n",
      "step: 3, hamming: 0.5875787789957998\n",
      "step: 4, hamming: 0.5536924206915095\n",
      "step: 5, hamming: 0.5132239248844174\n",
      "step: 6, hamming: 0.4690279451515534\n",
      "step: 7, hamming: 0.4234005441297798\n",
      "step: 8, hamming: 0.37815866164971773\n",
      "step: 9, hamming: 0.3346641106844513\n",
      "step: 10, hamming: 0.29386522327545833\n",
      "step: 11, hamming: 0.25635291398589544\n",
      "step: 12, hamming: 0.22242780353078664\n",
      "step: 13, hamming: 0.1921699165297888\n",
      "step: 14, hamming: 0.16549975125668107\n",
      "step: 15, hamming: 0.14222892834479822\n",
      "step: 16, hamming: 0.12210051057291259\n",
      "step: 17, hamming: 0.10481816001497271\n",
      "step: 18, hamming: 0.0900667032672284\n",
      "step: 19, hamming: 0.07752617005100722\n",
      "step: 20, hamming: 0.0668817243872407\n",
      "step: 21, hamming: 0.0578317689475502\n",
      "step: 22, hamming: 0.05009647661533095\n",
      "step: 23, hamming: 0.04342729230869476\n",
      "step: 24, hamming: 0.037616314867588554\n",
      "step: 25, hamming: 0.03250255684979377\n",
      "step: 26, hamming: 0.027971999052752584\n",
      "step: 27, hamming: 0.023950359393898557\n",
      "step: 28, hamming: 0.020390900537206765\n",
      "step: 29, hamming: 0.017261384985951662\n",
      "step: 30, hamming: 0.014533860839728177\n",
      "step: 31, hamming: 0.01217890040608688\n",
      "step: 32, hamming: 0.010163791137853356\n",
      "step: 33, hamming: 0.008453219201499561\n",
      "step: 34, hamming: 0.007010983652762791\n",
      "step: 35, hamming: 0.0058017611584794\n",
      "step: 36, hamming: 0.004792456415076777\n",
      "step: 37, hamming: 0.003953028346447055\n",
      "step: 38, hamming: 0.0032568587382547807\n",
      "step: 39, hamming: 0.00268078789777572\n",
      "step: 40, hamming: 0.002204939377727329\n",
      "step: 41, hamming: 0.0018124290872041863\n",
      "step: 42, hamming: 0.0014890257649965678\n",
      "step: 43, hamming: 0.0012228043489239575\n",
      "step: 44, hamming: 0.001003816846444069\n",
      "step: 45, hamming: 0.000823792977083675\n",
      "Running panda took: 1.43 seconds!\n",
      "  Elapsed time: 1.43 sec.\n",
      "Saving LIONESS network 35 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 36:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.64538220832322\n",
      "step: 1, hamming: 0.6066969273795093\n",
      "step: 2, hamming: 0.6105383132265271\n",
      "step: 3, hamming: 0.5875246652799395\n",
      "step: 4, hamming: 0.5535923400428431\n",
      "step: 5, hamming: 0.5130969212335403\n",
      "step: 6, hamming: 0.4688916302793686\n",
      "step: 7, hamming: 0.42325914937362835\n",
      "step: 8, hamming: 0.378015222114137\n",
      "step: 9, hamming: 0.33452209514331444\n",
      "step: 10, hamming: 0.2937286374246192\n",
      "step: 11, hamming: 0.25622520736811455\n",
      "step: 12, hamming: 0.22231178357995593\n",
      "step: 13, hamming: 0.1920670653122524\n",
      "step: 14, hamming: 0.16541028792801002\n",
      "step: 15, hamming: 0.14215237669006922\n",
      "step: 16, hamming: 0.12203588566680589\n",
      "step: 17, hamming: 0.10476421069641416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 18, hamming: 0.09002215316977837\n",
      "step: 19, hamming: 0.07748976024988688\n",
      "step: 20, hamming: 0.06685218168838661\n",
      "step: 21, hamming: 0.05780791039379919\n",
      "step: 22, hamming: 0.050077221528608516\n",
      "step: 23, hamming: 0.0434117205050181\n",
      "step: 24, hamming: 0.037603623119995024\n",
      "step: 25, hamming: 0.03249211431074741\n",
      "step: 26, hamming: 0.027963316385545108\n",
      "step: 27, hamming: 0.023943089250558148\n",
      "step: 28, hamming: 0.020384791637139868\n",
      "step: 29, hamming: 0.017256245588859704\n",
      "step: 30, hamming: 0.014529542455540409\n",
      "step: 31, hamming: 0.012175281230148284\n",
      "step: 32, hamming: 0.010160766715614722\n",
      "step: 33, hamming: 0.008450698522426137\n",
      "step: 34, hamming: 0.007008887974781527\n",
      "step: 35, hamming: 0.005800022398900317\n",
      "step: 36, hamming: 0.004791016237650568\n",
      "step: 37, hamming: 0.003951837291752234\n",
      "step: 38, hamming: 0.00325587489135729\n",
      "step: 39, hamming: 0.0026799760260441635\n",
      "step: 40, hamming: 0.0022042699447564137\n",
      "step: 41, hamming: 0.0018118775067475124\n",
      "step: 42, hamming: 0.0014885715795620964\n",
      "step: 43, hamming: 0.0012224305718871041\n",
      "step: 44, hamming: 0.0010035093959526774\n",
      "step: 45, hamming: 0.0008235401960124153\n",
      "Running panda took: 1.34 seconds!\n",
      "  Elapsed time: 1.34 sec.\n",
      "Saving LIONESS network 36 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 37:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450937603753659\n",
      "step: 1, hamming: 0.6064436660837454\n",
      "step: 2, hamming: 0.6105412861118334\n",
      "step: 3, hamming: 0.5876066009972608\n",
      "step: 4, hamming: 0.5537181506446464\n",
      "step: 5, hamming: 0.513240162239385\n",
      "step: 6, hamming: 0.46903005243019497\n",
      "step: 7, hamming: 0.4233816338164551\n",
      "step: 8, hamming: 0.37811730216131945\n",
      "step: 9, hamming: 0.33460427640855356\n",
      "step: 10, hamming: 0.2937937095904566\n",
      "step: 11, hamming: 0.2562765055577094\n",
      "step: 12, hamming: 0.2223522829719493\n",
      "step: 13, hamming: 0.19209908884148155\n",
      "step: 14, hamming: 0.16543565555984818\n",
      "step: 15, hamming: 0.14217249050227085\n",
      "step: 16, hamming: 0.12205187338509321\n",
      "step: 17, hamming: 0.10477697957160514\n",
      "step: 18, hamming: 0.09003236276240753\n",
      "step: 19, hamming: 0.07749793593084385\n",
      "step: 20, hamming: 0.06685876647177448\n",
      "step: 21, hamming: 0.057813243887915165\n",
      "step: 22, hamming: 0.0500815676624227\n",
      "step: 23, hamming: 0.04341524699674483\n",
      "step: 24, hamming: 0.03760647449990497\n",
      "step: 25, hamming: 0.032494402723185246\n",
      "step: 26, hamming: 0.027965137026658638\n",
      "step: 27, hamming: 0.023944516079917964\n",
      "step: 28, hamming: 0.020385898620171303\n",
      "step: 29, hamming: 0.0172570960909915\n",
      "step: 30, hamming: 0.014530191634674654\n",
      "step: 31, hamming: 0.012175775564193877\n",
      "step: 32, hamming: 0.010161143736087765\n",
      "step: 33, hamming: 0.008450987745447076\n",
      "step: 34, hamming: 0.007009111445933513\n",
      "step: 35, hamming: 0.0058001966698539\n",
      "step: 36, hamming: 0.004791153274172695\n",
      "step: 37, hamming: 0.003951945757724806\n",
      "step: 38, hamming: 0.003255961297104378\n",
      "step: 39, hamming: 0.0026800452215014355\n",
      "step: 40, hamming: 0.0022043256087412007\n",
      "step: 41, hamming: 0.001811922446478194\n",
      "step: 42, hamming: 0.0014886079619687155\n",
      "step: 43, hamming: 0.0012224600919155365\n",
      "step: 44, hamming: 0.0010035333901201558\n",
      "step: 45, hamming: 0.0008235597257708738\n",
      "Running panda took: 1.47 seconds!\n",
      "  Elapsed time: 1.47 sec.\n",
      "Saving LIONESS network 37 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 38:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6440630972479127\n",
      "step: 1, hamming: 0.6065362164769551\n",
      "step: 2, hamming: 0.6104532026394006\n",
      "step: 3, hamming: 0.5874604045897802\n",
      "step: 4, hamming: 0.5535439576972261\n",
      "step: 5, hamming: 0.5130532048778697\n",
      "step: 6, hamming: 0.46884617663248196\n",
      "step: 7, hamming: 0.4232106291765286\n",
      "step: 8, hamming: 0.37796351385560756\n",
      "step: 9, hamming: 0.3344694203478319\n",
      "step: 10, hamming: 0.29367784651746603\n",
      "step: 11, hamming: 0.25617791005946733\n",
      "step: 12, hamming: 0.2222690261377353\n",
      "step: 13, hamming: 0.19202898759027917\n",
      "step: 14, hamming: 0.16537677619988342\n",
      "step: 15, hamming: 0.1421232526413549\n",
      "step: 16, hamming: 0.12201074582554954\n",
      "step: 17, hamming: 0.10474263477818664\n",
      "step: 18, hamming: 0.09000370065023104\n",
      "step: 19, hamming: 0.07747402152715009\n",
      "step: 20, hamming: 0.06683879577708175\n",
      "step: 21, hamming: 0.0577965551811048\n",
      "step: 22, hamming: 0.05006760738559485\n",
      "step: 23, hamming: 0.04340357349694229\n",
      "step: 24, hamming: 0.03759672349751944\n",
      "step: 25, hamming: 0.032486280205226196\n",
      "step: 26, hamming: 0.027958392107693967\n",
      "step: 27, hamming: 0.023938938938551046\n",
      "step: 28, hamming: 0.02038130226939476\n",
      "step: 29, hamming: 0.017253319330902392\n",
      "step: 30, hamming: 0.014527095084336914\n",
      "step: 31, hamming: 0.012173239529471589\n",
      "step: 32, hamming: 0.010159067354651505\n",
      "step: 33, hamming: 0.008449287001086303\n",
      "step: 34, hamming: 0.007007717820632078\n",
      "step: 35, hamming: 0.005799053624028304\n",
      "step: 36, hamming: 0.004790215055454622\n",
      "step: 37, hamming: 0.003951175336782719\n",
      "step: 38, hamming: 0.0032553284588682867\n",
      "step: 39, hamming: 0.002679525308582267\n",
      "step: 40, hamming: 0.0022038984233926826\n",
      "step: 41, hamming: 0.0018115714471615197\n",
      "step: 42, hamming: 0.0014883195860679434\n",
      "step: 43, hamming: 0.0012222231985379237\n",
      "step: 44, hamming: 0.0010033388191364957\n",
      "step: 45, hamming: 0.0008233999448704641\n",
      "Running panda took: 1.71 seconds!\n",
      "  Elapsed time: 1.71 sec.\n",
      "Saving LIONESS network 38 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 39:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450086728222191\n",
      "step: 1, hamming: 0.6066279590964608\n",
      "step: 2, hamming: 0.6105189710157038\n",
      "step: 3, hamming: 0.5875129702246185\n",
      "step: 4, hamming: 0.5535881728916275\n",
      "step: 5, hamming: 0.5130942680692989\n",
      "step: 6, hamming: 0.46888370874519314\n",
      "step: 7, hamming: 0.4232446919415762\n",
      "step: 8, hamming: 0.3779945678456299\n",
      "step: 9, hamming: 0.3344972254062106\n",
      "step: 10, hamming: 0.2937027228697728\n",
      "step: 11, hamming: 0.2561999698599468\n",
      "step: 12, hamming: 0.22228841230441748\n",
      "step: 13, hamming: 0.19204594680519657\n",
      "step: 14, hamming: 0.1653914528419214\n",
      "step: 15, hamming: 0.1421357904234999\n",
      "step: 16, hamming: 0.12202150658243802\n",
      "step: 17, hamming: 0.10475182786542589\n",
      "step: 18, hamming: 0.0900115144619496\n",
      "step: 19, hamming: 0.077480629396115\n",
      "step: 20, hamming: 0.06684439090269251\n",
      "step: 21, hamming: 0.057801297739797815\n",
      "step: 22, hamming: 0.050071632819650706\n",
      "step: 23, hamming: 0.043407005970971885\n",
      "step: 24, hamming: 0.03759966095033141\n",
      "step: 25, hamming: 0.032488803075486614\n",
      "step: 26, hamming: 0.027960560323392912\n",
      "step: 27, hamming: 0.02394080074258424\n",
      "step: 28, hamming: 0.020382896359423756\n",
      "step: 29, hamming: 0.017254678449675067\n",
      "step: 30, hamming: 0.014528247886828674\n",
      "step: 31, hamming: 0.012174211893688459\n",
      "step: 32, hamming: 0.010159883384549461\n",
      "step: 33, hamming: 0.008449969131479785\n",
      "step: 34, hamming: 0.007008285718950414\n",
      "step: 35, hamming: 0.005799525161101376\n",
      "step: 36, hamming: 0.004790605850968227\n",
      "step: 37, hamming: 0.003951498594437272\n",
      "step: 38, hamming: 0.00325559543899562\n",
      "step: 39, hamming: 0.002679745529938822\n",
      "step: 40, hamming: 0.002204079908626591\n",
      "step: 41, hamming: 0.0018117209077095206\n",
      "step: 42, hamming: 0.0014884425909900134\n",
      "step: 43, hamming: 0.0012223243766523934\n",
      "step: 44, hamming: 0.001003422008988331\n",
      "step: 45, hamming: 0.0008234683171750291\n",
      "Running panda took: 1.63 seconds!\n",
      "  Elapsed time: 1.63 sec.\n",
      "Saving LIONESS network 39 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 40:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6446899829950575\n",
      "step: 1, hamming: 0.6065720657385845\n",
      "step: 2, hamming: 0.6104769060458073\n",
      "step: 3, hamming: 0.587468913447344\n",
      "step: 4, hamming: 0.553541697529604\n",
      "step: 5, hamming: 0.5130476369210645\n",
      "step: 6, hamming: 0.4688442859318447\n",
      "step: 7, hamming: 0.42321479157426\n",
      "step: 8, hamming: 0.3779745725802862\n",
      "step: 9, hamming: 0.33448614616016825\n",
      "step: 10, hamming: 0.2936979376976792\n",
      "step: 11, hamming: 0.2561994062741515\n",
      "step: 12, hamming: 0.22229019835157474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 13, hamming: 0.19204895567253832\n",
      "step: 14, hamming: 0.16539501582991112\n",
      "step: 15, hamming: 0.14213947488622694\n",
      "step: 16, hamming: 0.12202499625028415\n",
      "step: 17, hamming: 0.10475511557113841\n",
      "step: 18, hamming: 0.09001460630282851\n",
      "step: 19, hamming: 0.07748348660857823\n",
      "step: 20, hamming: 0.06684699483703208\n",
      "step: 21, hamming: 0.05780364640854137\n",
      "step: 22, hamming: 0.050073752658040344\n",
      "step: 23, hamming: 0.04340891190950424\n",
      "step: 24, hamming: 0.03760138090088408\n",
      "step: 25, hamming: 0.03249034534683271\n",
      "step: 26, hamming: 0.027961940282617167\n",
      "step: 27, hamming: 0.02394202780317782\n",
      "step: 28, hamming: 0.020383977524069206\n",
      "step: 29, hamming: 0.017255623043299223\n",
      "step: 30, hamming: 0.014529064889671081\n",
      "step: 31, hamming: 0.012174912034662342\n",
      "step: 32, hamming: 0.010160478451575828\n",
      "step: 33, hamming: 0.008450471314942093\n",
      "step: 34, hamming: 0.007008707096476387\n",
      "step: 35, hamming: 0.005799877499710264\n",
      "step: 36, hamming: 0.0047908995699176915\n",
      "step: 37, hamming: 0.00395174269330924\n",
      "step: 38, hamming: 0.0032557978512664297\n",
      "step: 39, hamming: 0.002679913100611091\n",
      "step: 40, hamming: 0.0022042184426658354\n",
      "step: 41, hamming: 0.0018118352905121414\n",
      "step: 42, hamming: 0.0014885369392666512\n",
      "step: 43, hamming: 0.0012224021313626207\n",
      "step: 44, hamming: 0.0010034860389152175\n",
      "step: 45, hamming: 0.0008235210127976292\n",
      "Running panda took: 1.75 seconds!\n",
      "  Elapsed time: 1.75 sec.\n",
      "Saving LIONESS network 40 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 41:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6439588770722169\n",
      "step: 1, hamming: 0.6066729641072987\n",
      "step: 2, hamming: 0.610570644142953\n",
      "step: 3, hamming: 0.5875671840941767\n",
      "step: 4, hamming: 0.5536412525576695\n",
      "step: 5, hamming: 0.5131442161431155\n",
      "step: 6, hamming: 0.46893158026561005\n",
      "step: 7, hamming: 0.42328943886764275\n",
      "step: 8, hamming: 0.3780348238806754\n",
      "step: 9, hamming: 0.33453258916311723\n",
      "step: 10, hamming: 0.2937325823349309\n",
      "step: 11, hamming: 0.2562249355349886\n",
      "step: 12, hamming: 0.2223091005920621\n",
      "step: 13, hamming: 0.1920630661505349\n",
      "step: 14, hamming: 0.16540567166944584\n",
      "step: 15, hamming: 0.142147668817602\n",
      "step: 16, hamming: 0.12203143924750813\n",
      "step: 17, hamming: 0.10476020433355153\n",
      "step: 18, hamming: 0.09001866866683568\n",
      "step: 19, hamming: 0.07748681890195007\n",
      "step: 20, hamming: 0.06684977392071134\n",
      "step: 21, hamming: 0.0578060040584336\n",
      "step: 22, hamming: 0.050075759807870907\n",
      "step: 23, hamming: 0.04341058715028377\n",
      "step: 24, hamming: 0.03760274175604868\n",
      "step: 25, hamming: 0.03249142680963005\n",
      "step: 26, hamming: 0.02796277245679931\n",
      "step: 27, hamming: 0.023942649977909784\n",
      "step: 28, hamming: 0.020384429573280502\n",
      "step: 29, hamming: 0.017255942798246847\n",
      "step: 30, hamming: 0.0145292867393714\n",
      "step: 31, hamming: 0.012175064147083636\n",
      "step: 32, hamming: 0.010160581814379425\n",
      "step: 33, hamming: 0.00845054093972525\n",
      "step: 34, hamming: 0.007008754041534421\n",
      "step: 35, hamming: 0.005799908869690069\n",
      "step: 36, hamming: 0.004790920217625292\n",
      "step: 37, hamming: 0.003951756159188346\n",
      "step: 38, hamming: 0.0032558065114434266\n",
      "step: 39, hamming: 0.0026799185423229273\n",
      "step: 40, hamming: 0.0022042217409228154\n",
      "step: 41, hamming: 0.0018118371780350322\n",
      "step: 42, hamming: 0.0014885379101339346\n",
      "step: 43, hamming: 0.0012224025182755501\n",
      "step: 44, hamming: 0.0010034860661721843\n",
      "step: 45, hamming: 0.0008235208251140585\n",
      "Running panda took: 1.82 seconds!\n",
      "  Elapsed time: 1.82 sec.\n",
      "Saving LIONESS network 41 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 42:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450900888790349\n",
      "step: 1, hamming: 0.6066516738621498\n",
      "step: 2, hamming: 0.6104985666021202\n",
      "step: 3, hamming: 0.5874788063748753\n",
      "step: 4, hamming: 0.553540130637487\n",
      "step: 5, hamming: 0.5130397114128462\n",
      "step: 6, hamming: 0.46883365127766485\n",
      "step: 7, hamming: 0.42320294317071977\n",
      "step: 8, hamming: 0.3779626905033644\n",
      "step: 9, hamming: 0.3344747386057253\n",
      "step: 10, hamming: 0.2936868597522509\n",
      "step: 11, hamming: 0.2561887985822545\n",
      "step: 12, hamming: 0.2222804132814435\n",
      "step: 13, hamming: 0.1920401220158585\n",
      "step: 14, hamming: 0.16538712296890065\n",
      "step: 15, hamming: 0.14213253273283047\n",
      "step: 16, hamming: 0.12201908520533228\n",
      "step: 17, hamming: 0.10475002765824205\n",
      "step: 18, hamming: 0.09001017467263965\n",
      "step: 19, hamming: 0.077479699555011\n",
      "step: 20, hamming: 0.0668437502548238\n",
      "step: 21, hamming: 0.057800867645357974\n",
      "step: 22, hamming: 0.05007135445772276\n",
      "step: 23, hamming: 0.04340683431558541\n",
      "step: 24, hamming: 0.03759956900963259\n",
      "step: 25, hamming: 0.03248876286486136\n",
      "step: 26, hamming: 0.02796055963694356\n",
      "step: 27, hamming: 0.02394082711040951\n",
      "step: 28, hamming: 0.020382937686533594\n",
      "step: 29, hamming: 0.01725472869077617\n",
      "step: 30, hamming: 0.014528302543333647\n",
      "step: 31, hamming: 0.01217426646707725\n",
      "step: 32, hamming: 0.010159934842999263\n",
      "step: 33, hamming: 0.008450015826353949\n",
      "step: 34, hamming: 0.007008327003644892\n",
      "step: 35, hamming: 0.005799561059886949\n",
      "step: 36, hamming: 0.004790636812690914\n",
      "step: 37, hamming: 0.003951524989153562\n",
      "step: 38, hamming: 0.0032556177779712583\n",
      "step: 39, hamming: 0.0026797643542427973\n",
      "step: 40, hamming: 0.0022040957228296318\n",
      "step: 41, hamming: 0.0018117341418863388\n",
      "step: 42, hamming: 0.0014884536410248769\n",
      "step: 43, hamming: 0.0012223335797172294\n",
      "step: 44, hamming: 0.0010034296580480636\n",
      "step: 45, hamming: 0.0008234746634137485\n",
      "Running panda took: 1.63 seconds!\n",
      "  Elapsed time: 1.63 sec.\n",
      "Saving LIONESS network 42 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 43:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.644052761896358\n",
      "step: 1, hamming: 0.6068467935854113\n",
      "step: 2, hamming: 0.6107288577190169\n",
      "step: 3, hamming: 0.5877376684115262\n",
      "step: 4, hamming: 0.5538142766265394\n",
      "step: 5, hamming: 0.5133115341098062\n",
      "step: 6, hamming: 0.46908987086502824\n",
      "step: 7, hamming: 0.4234332320104306\n",
      "step: 8, hamming: 0.3781623554018538\n",
      "step: 9, hamming: 0.33464371916895425\n",
      "step: 10, hamming: 0.29382891080082413\n",
      "step: 11, hamming: 0.25630804132509455\n",
      "step: 12, hamming: 0.22238058991015805\n",
      "step: 13, hamming: 0.1921244069073419\n",
      "step: 14, hamming: 0.1654581795748105\n",
      "step: 15, hamming: 0.14219255890384128\n",
      "step: 16, hamming: 0.12206981536725257\n",
      "step: 17, hamming: 0.10479308912740204\n",
      "step: 18, hamming: 0.09004682418382233\n",
      "step: 19, hamming: 0.07751094005916007\n",
      "step: 20, hamming: 0.06687044755283782\n",
      "step: 21, hamming: 0.057823710017225756\n",
      "step: 22, hamming: 0.05009090857316944\n",
      "step: 23, hamming: 0.04342352724644388\n",
      "step: 24, hamming: 0.03761374284334517\n",
      "step: 25, hamming: 0.032500717866233104\n",
      "step: 26, hamming: 0.027970567103492504\n",
      "step: 27, hamming: 0.023949145946046777\n",
      "step: 28, hamming: 0.020389814594208796\n",
      "step: 29, hamming: 0.01726038967332538\n",
      "step: 30, hamming: 0.014532948325301727\n",
      "step: 31, hamming: 0.012178073966408829\n",
      "step: 32, hamming: 0.010163053860804863\n",
      "step: 33, hamming: 0.00845257088969216\n",
      "step: 34, hamming: 0.007010420899916855\n",
      "step: 35, hamming: 0.005801277650771889\n",
      "step: 36, hamming: 0.0047920442777004526\n",
      "step: 37, hamming: 0.003952679304999426\n",
      "step: 38, hamming: 0.0032565646292526955\n",
      "step: 39, hamming: 0.0026805410775311805\n",
      "step: 40, hamming: 0.002204732881227192\n",
      "step: 41, hamming: 0.001812256795704004\n",
      "step: 42, hamming: 0.0014888823408190444\n",
      "step: 43, hamming: 0.0012226851824785948\n",
      "step: 44, hamming: 0.0010037179959729236\n",
      "step: 45, hamming: 0.0008237110939490818\n",
      "Running panda took: 1.40 seconds!\n",
      "  Elapsed time: 1.40 sec.\n",
      "Saving LIONESS network 43 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 44:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6455018706675433\n",
      "step: 1, hamming: 0.6066771889037847\n",
      "step: 2, hamming: 0.6104927458207806\n",
      "step: 3, hamming: 0.5874724098807874\n",
      "step: 4, hamming: 0.5535381852036291\n",
      "step: 5, hamming: 0.5130412181815369\n",
      "step: 6, hamming: 0.4688357436456271\n",
      "step: 7, hamming: 0.4232060516712834\n",
      "step: 8, hamming: 0.3779651681944427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 9, hamming: 0.3344757818245871\n",
      "step: 10, hamming: 0.29368699908851514\n",
      "step: 11, hamming: 0.25618810836906636\n",
      "step: 12, hamming: 0.22227915239937626\n",
      "step: 13, hamming: 0.19203863188744116\n",
      "step: 14, hamming: 0.16538554120286303\n",
      "step: 15, hamming: 0.1421309400195572\n",
      "step: 16, hamming: 0.12201736565539252\n",
      "step: 17, hamming: 0.10474825814801286\n",
      "step: 18, hamming: 0.09000844987911948\n",
      "step: 19, hamming: 0.07747797083444699\n",
      "step: 20, hamming: 0.06684205040520512\n",
      "step: 21, hamming: 0.057799246297848224\n",
      "step: 22, hamming: 0.05006983470329826\n",
      "step: 23, hamming: 0.0434054377789933\n",
      "step: 24, hamming: 0.03759830798077482\n",
      "step: 25, hamming: 0.032487644623876356\n",
      "step: 26, hamming: 0.027959585227725074\n",
      "step: 27, hamming: 0.02393999328159403\n",
      "step: 28, hamming: 0.020382235303793003\n",
      "step: 29, hamming: 0.017254142326846075\n",
      "step: 30, hamming: 0.0145278165923739\n",
      "step: 31, hamming: 0.012173865559254836\n",
      "step: 32, hamming: 0.010159605065697939\n",
      "step: 33, hamming: 0.008449745022944615\n",
      "step: 34, hamming: 0.0070081047551232775\n",
      "step: 35, hamming: 0.005799378660948919\n",
      "step: 36, hamming: 0.004790486921682077\n",
      "step: 37, hamming: 0.003951402165785128\n",
      "step: 38, hamming: 0.00325551714323014\n",
      "step: 39, hamming: 0.002679681888018422\n",
      "step: 40, hamming: 0.002204028114908212\n",
      "step: 41, hamming: 0.0018116787055630656\n",
      "step: 42, hamming: 0.0014884081765578659\n",
      "step: 43, hamming: 0.0012222962914605212\n",
      "step: 44, hamming: 0.0010033990729403487\n",
      "step: 45, hamming: 0.0008234495793931098\n",
      "Running panda took: 1.11 seconds!\n",
      "  Elapsed time: 1.11 sec.\n",
      "Saving LIONESS network 44 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 45:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451609513913849\n",
      "step: 1, hamming: 0.60682301694336\n",
      "step: 2, hamming: 0.6106083346123937\n",
      "step: 3, hamming: 0.5875838988028538\n",
      "step: 4, hamming: 0.5536474918555494\n",
      "step: 5, hamming: 0.5131464111654228\n",
      "step: 6, hamming: 0.46893531465023547\n",
      "step: 7, hamming: 0.4232953952543082\n",
      "step: 8, hamming: 0.3780435892681105\n",
      "step: 9, hamming: 0.33454365128594754\n",
      "step: 10, hamming: 0.29374508748769934\n",
      "step: 11, hamming: 0.25623779090891435\n",
      "step: 12, hamming: 0.22232178821340307\n",
      "step: 13, hamming: 0.1920751280646255\n",
      "step: 14, hamming: 0.16541683124420956\n",
      "step: 15, hamming: 0.1421577806735096\n",
      "step: 16, hamming: 0.12204047685598642\n",
      "step: 17, hamming: 0.10476822857697907\n",
      "step: 18, hamming: 0.09002567471379645\n",
      "step: 19, hamming: 0.07749288793781219\n",
      "step: 20, hamming: 0.06685499297134434\n",
      "step: 21, hamming: 0.05781044528986368\n",
      "step: 22, hamming: 0.050079503532131836\n",
      "step: 23, hamming: 0.043413743732357984\n",
      "step: 24, hamming: 0.037605400712792725\n",
      "step: 25, hamming: 0.032493655647963006\n",
      "step: 26, hamming: 0.027964640608281126\n",
      "step: 27, hamming: 0.02394421340556936\n",
      "step: 28, hamming: 0.02038573503818736\n",
      "step: 29, hamming: 0.01725703100015745\n",
      "step: 30, hamming: 0.014530192108341453\n",
      "step: 31, hamming: 0.012175817632282378\n",
      "step: 32, hamming: 0.010161208009430479\n",
      "step: 33, hamming: 0.008451061098622103\n",
      "step: 34, hamming: 0.007009185612316628\n",
      "step: 35, hamming: 0.00580026658609169\n",
      "step: 36, hamming: 0.004791216556223985\n",
      "step: 37, hamming: 0.003952001550810154\n",
      "step: 38, hamming: 0.003256009551690285\n",
      "step: 39, hamming: 0.0026800863984453036\n",
      "step: 40, hamming: 0.0022043604008479102\n",
      "step: 41, hamming: 0.0018119516331865852\n",
      "step: 42, hamming: 0.001488632316391934\n",
      "step: 43, hamming: 0.001222480334695352\n",
      "step: 44, hamming: 0.001003550166649101\n",
      "step: 45, hamming: 0.0008235735965779479\n",
      "Running panda took: 1.10 seconds!\n",
      "  Elapsed time: 1.10 sec.\n",
      "Saving LIONESS network 45 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 46:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6444561373205833\n",
      "step: 1, hamming: 0.6065980545917278\n",
      "step: 2, hamming: 0.6105128971327527\n",
      "step: 3, hamming: 0.5875110985863032\n",
      "step: 4, hamming: 0.553588190940493\n",
      "step: 5, hamming: 0.513091592106618\n",
      "step: 6, hamming: 0.46888363850009035\n",
      "step: 7, hamming: 0.42324816999840126\n",
      "step: 8, hamming: 0.3780016536091072\n",
      "step: 9, hamming: 0.3345068620657478\n",
      "step: 10, hamming: 0.2937130015631493\n",
      "step: 11, hamming: 0.2562101677520667\n",
      "step: 12, hamming: 0.22229782461584252\n",
      "step: 13, hamming: 0.19205431495200018\n",
      "step: 14, hamming: 0.16539873221832643\n",
      "step: 15, hamming: 0.14214201379337854\n",
      "step: 16, hamming: 0.1220266912618745\n",
      "step: 17, hamming: 0.10475621059832578\n",
      "step: 18, hamming: 0.09001529971582535\n",
      "step: 19, hamming: 0.07748388708610993\n",
      "step: 20, hamming: 0.06684718843182245\n",
      "step: 21, hamming: 0.05780369052361184\n",
      "step: 22, hamming: 0.05007369620781047\n",
      "step: 23, hamming: 0.04340879060521527\n",
      "step: 24, hamming: 0.0376012098195711\n",
      "step: 25, hamming: 0.03249013651071767\n",
      "step: 26, hamming: 0.027961701647830434\n",
      "step: 27, hamming: 0.023941767943218586\n",
      "step: 28, hamming: 0.020383707109450016\n",
      "step: 29, hamming: 0.01725535251678685\n",
      "step: 30, hamming: 0.014528803438411068\n",
      "step: 31, hamming: 0.01217466719343373\n",
      "step: 32, hamming: 0.010160255136051028\n",
      "step: 33, hamming: 0.008450271975505554\n",
      "step: 34, hamming: 0.0070085325302855425\n",
      "step: 35, hamming: 0.005799726752184737\n",
      "step: 36, hamming: 0.004790770756328508\n",
      "step: 37, hamming: 0.003951633741495327\n",
      "step: 38, hamming: 0.003255706284492107\n",
      "step: 39, hamming: 0.002679836512915268\n",
      "step: 40, hamming: 0.0022041546289165003\n",
      "step: 41, hamming: 0.001811782285088026\n",
      "step: 42, hamming: 0.0014884930203235185\n",
      "step: 43, hamming: 0.0012223658117307841\n",
      "step: 44, hamming: 0.001003456049299198\n",
      "step: 45, hamming: 0.0008234962794548331\n",
      "Running panda took: 1.19 seconds!\n",
      "  Elapsed time: 1.19 sec.\n",
      "Saving LIONESS network 46 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 47:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.01 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6451094435014055\n",
      "step: 1, hamming: 0.6067224879320419\n",
      "step: 2, hamming: 0.6104855876766591\n",
      "step: 3, hamming: 0.5874302260560214\n",
      "step: 4, hamming: 0.5534812095958548\n",
      "step: 5, hamming: 0.5129821856517685\n",
      "step: 6, hamming: 0.46878098398796686\n",
      "step: 7, hamming: 0.42315657041730403\n",
      "step: 8, hamming: 0.3779239978979505\n",
      "step: 9, hamming: 0.3344427227132656\n",
      "step: 10, hamming: 0.29366102731476706\n",
      "step: 11, hamming: 0.25616821630801284\n",
      "step: 12, hamming: 0.22226395137122443\n",
      "step: 13, hamming: 0.19202681658475132\n",
      "step: 14, hamming: 0.16537625986764548\n",
      "step: 15, hamming: 0.1421235728034832\n",
      "step: 16, hamming: 0.12201150665910697\n",
      "step: 17, hamming: 0.10474353103231468\n",
      "step: 18, hamming: 0.09000454950906786\n",
      "step: 19, hamming: 0.07747470953904385\n",
      "step: 20, hamming: 0.06683930742299432\n",
      "step: 21, hamming: 0.0577968871587979\n",
      "step: 22, hamming: 0.050067790022936765\n",
      "step: 23, hamming: 0.04340365566162159\n",
      "step: 24, hamming: 0.03759676223186337\n",
      "step: 25, hamming: 0.03248631716762015\n",
      "step: 26, hamming: 0.027958452078804107\n",
      "step: 27, hamming: 0.023939028616903366\n",
      "step: 28, hamming: 0.020381419449167232\n",
      "step: 29, hamming: 0.017253456558720872\n",
      "step: 30, hamming: 0.014527240970954446\n",
      "step: 31, hamming: 0.012173384845798182\n",
      "step: 32, hamming: 0.010159205498888376\n",
      "step: 33, hamming: 0.008449414288126912\n",
      "step: 34, hamming: 0.007007832514285143\n",
      "step: 35, hamming: 0.005799155320694112\n",
      "step: 36, hamming: 0.0047903042325213785\n",
      "step: 37, hamming: 0.003951252628490434\n",
      "step: 38, hamming: 0.0032553948817023257\n",
      "step: 39, hamming: 0.0026795820184424913\n",
      "step: 40, hamming: 0.0022039466024797332\n",
      "step: 41, hamming: 0.0018116122118621097\n",
      "step: 42, hamming: 0.001488353952046178\n",
      "step: 43, hamming: 0.0012222520794900526\n",
      "step: 44, hamming: 0.0010033630290072102\n",
      "step: 45, hamming: 0.0008234201909991941\n",
      "Running panda took: 1.47 seconds!\n",
      "  Elapsed time: 1.47 sec.\n",
      "Saving LIONESS network 47 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 48:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450729208857102\n",
      "step: 1, hamming: 0.6063501512150873\n",
      "step: 2, hamming: 0.6103715026046174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 3, hamming: 0.5874284484527396\n",
      "step: 4, hamming: 0.5535259596516843\n",
      "step: 5, hamming: 0.513054704775382\n",
      "step: 6, hamming: 0.4688708582148043\n",
      "step: 7, hamming: 0.42325992634308324\n",
      "step: 8, hamming: 0.37803721253202954\n",
      "step: 9, hamming: 0.33456092176953284\n",
      "step: 10, hamming: 0.29377908789293194\n",
      "step: 11, hamming: 0.25628124480181685\n",
      "step: 12, hamming: 0.22236874382375207\n",
      "step: 13, hamming: 0.19212159497551798\n",
      "step: 14, hamming: 0.165460467724724\n",
      "step: 15, hamming: 0.1421972940894272\n",
      "step: 16, hamming: 0.12207523597375464\n",
      "step: 17, hamming: 0.10479810759599201\n",
      "step: 18, hamming: 0.09005092233539186\n",
      "step: 19, hamming: 0.07751386224984219\n",
      "step: 20, hamming: 0.06687216133585415\n",
      "step: 21, hamming: 0.05782432168099327\n",
      "step: 22, hamming: 0.050090599604048575\n",
      "step: 23, hamming: 0.04342254449049325\n",
      "step: 24, hamming: 0.03761235473029859\n",
      "step: 25, hamming: 0.03249915130287423\n",
      "step: 26, hamming: 0.027968991909988243\n",
      "step: 27, hamming: 0.023947673742928513\n",
      "step: 28, hamming: 0.02038850792929859\n",
      "step: 29, hamming: 0.01725926821592624\n",
      "step: 30, hamming: 0.01453200904050933\n",
      "step: 31, hamming: 0.012177299988630045\n",
      "step: 32, hamming: 0.010162422517800813\n",
      "step: 33, hamming: 0.008452059415929542\n",
      "step: 34, hamming: 0.007010008256905795\n",
      "step: 35, hamming: 0.005800945322716098\n",
      "step: 36, hamming: 0.004791776880461754\n",
      "step: 37, hamming: 0.003952464204685849\n",
      "step: 38, hamming: 0.003256391557054846\n",
      "step: 39, hamming: 0.002680401761524582\n",
      "step: 40, hamming: 0.0022046206611944417\n",
      "step: 41, hamming: 0.0018121663562190118\n",
      "step: 42, hamming: 0.0014888093898348762\n",
      "step: 43, hamming: 0.0012226262844574846\n",
      "step: 44, hamming: 0.0010036704002436745\n",
      "step: 45, hamming: 0.0008236725986009702\n",
      "Running panda took: 1.82 seconds!\n",
      "  Elapsed time: 1.82 sec.\n",
      "Saving LIONESS network 48 to lioness_output using npy format:\n",
      "  Elapsed time: 0.10 sec.\n",
      "Running LIONESS for sample 49:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6450184972229365\n",
      "step: 1, hamming: 0.6068240235253974\n",
      "step: 2, hamming: 0.6105965948530254\n",
      "step: 3, hamming: 0.5875567675582667\n",
      "step: 4, hamming: 0.5536089018867548\n",
      "step: 5, hamming: 0.5130991941025522\n",
      "step: 6, hamming: 0.4688833696500004\n",
      "step: 7, hamming: 0.42324358671229345\n",
      "step: 8, hamming: 0.3779936855263689\n",
      "step: 9, hamming: 0.334497167054918\n",
      "step: 10, hamming: 0.29370268379408965\n",
      "step: 11, hamming: 0.2561996389154925\n",
      "step: 12, hamming: 0.22228779180603883\n",
      "step: 13, hamming: 0.19204512280533334\n",
      "step: 14, hamming: 0.165390605047534\n",
      "step: 15, hamming: 0.14213514394319682\n",
      "step: 16, hamming: 0.12202099792941379\n",
      "step: 17, hamming: 0.1047515095155968\n",
      "step: 18, hamming: 0.09001138629352734\n",
      "step: 19, hamming: 0.07748073876203371\n",
      "step: 20, hamming: 0.06684470509459198\n",
      "step: 21, hamming: 0.057801763448754456\n",
      "step: 22, hamming: 0.05007220261612099\n",
      "step: 23, hamming: 0.043407622810912906\n",
      "step: 24, hamming: 0.03760028270759244\n",
      "step: 25, hamming: 0.03248939173711593\n",
      "step: 26, hamming: 0.027961098540398806\n",
      "step: 27, hamming: 0.023941274385556592\n",
      "step: 28, hamming: 0.020383301206956795\n",
      "step: 29, hamming: 0.01725501788997063\n",
      "step: 30, hamming: 0.014528528671629803\n",
      "step: 31, hamming: 0.012174442900517382\n",
      "step: 32, hamming: 0.010160072735999443\n",
      "step: 33, hamming: 0.008450124179654742\n",
      "step: 34, hamming: 0.007008412715800017\n",
      "step: 35, hamming: 0.005799629423267298\n",
      "step: 36, hamming: 0.004790691480475807\n",
      "step: 37, hamming: 0.003951568960336357\n",
      "step: 38, hamming: 0.0032556533190094545\n",
      "step: 39, hamming: 0.002679793182593864\n",
      "step: 40, hamming: 0.0022041191628976997\n",
      "step: 41, hamming: 0.0018117532364711271\n",
      "step: 42, hamming: 0.0014884692131398446\n",
      "step: 43, hamming: 0.0012223462917056505\n",
      "step: 44, hamming: 0.0010034400411097599\n",
      "step: 45, hamming: 0.0008234831485799553\n",
      "Running panda took: 1.69 seconds!\n",
      "  Elapsed time: 1.69 sec.\n",
      "Saving LIONESS network 49 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Running LIONESS for sample 50:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "step: 0, hamming: 0.6452671619076494\n",
      "step: 1, hamming: 0.6066848794849177\n",
      "step: 2, hamming: 0.610527587312023\n",
      "step: 3, hamming: 0.5875201390790639\n",
      "step: 4, hamming: 0.5535905216591732\n",
      "step: 5, hamming: 0.5130937685955773\n",
      "step: 6, hamming: 0.46888589482515114\n",
      "step: 7, hamming: 0.42325035103333314\n",
      "step: 8, hamming: 0.3780020754008558\n",
      "step: 9, hamming: 0.33450587862635867\n",
      "step: 10, hamming: 0.29371071766895035\n",
      "step: 11, hamming: 0.2562069560043886\n",
      "step: 12, hamming: 0.22229411692149156\n",
      "step: 13, hamming: 0.1920505169749237\n",
      "step: 14, hamming: 0.16539508199998465\n",
      "step: 15, hamming: 0.14213872037052777\n",
      "step: 16, hamming: 0.12202392388371457\n",
      "step: 17, hamming: 0.10475390584465225\n",
      "step: 18, hamming: 0.09001338394595736\n",
      "step: 19, hamming: 0.07748237123447736\n",
      "step: 20, hamming: 0.0668460176169171\n",
      "step: 21, hamming: 0.0578028088348542\n",
      "step: 22, hamming: 0.050073032857744684\n",
      "step: 23, hamming: 0.04340828693081746\n",
      "step: 24, hamming: 0.03760082097488881\n",
      "step: 25, hamming: 0.03248982737352425\n",
      "step: 26, hamming: 0.027961447158616017\n",
      "step: 27, hamming: 0.02394155483809075\n",
      "step: 28, hamming: 0.020383526490059086\n",
      "step: 29, hamming: 0.017255198122503472\n",
      "step: 30, hamming: 0.014528673059252924\n",
      "step: 31, hamming: 0.01217455915612517\n",
      "step: 32, hamming: 0.01016016655328855\n",
      "step: 33, hamming: 0.008450200171028089\n",
      "step: 34, hamming: 0.00700847469606477\n",
      "step: 35, hamming: 0.005799679944846653\n",
      "step: 36, hamming: 0.004790732721068612\n",
      "step: 37, hamming: 0.003951602686458481\n",
      "step: 38, hamming: 0.003255680919601843\n",
      "step: 39, hamming: 0.002679815808328271\n",
      "step: 40, hamming: 0.0022041377219136423\n",
      "step: 41, hamming: 0.0018117684679712833\n",
      "step: 42, hamming: 0.0014884817149678213\n",
      "step: 43, hamming: 0.001222356555124916\n",
      "step: 44, hamming: 0.0010034484651433174\n",
      "step: 45, hamming: 0.0008234900629489587\n",
      "Running panda took: 1.50 seconds!\n",
      "  Elapsed time: 1.50 sec.\n",
      "Saving LIONESS network 50 to lioness_output using npy format:\n",
      "  Elapsed time: 0.02 sec.\n"
     ]
    }
   ],
   "source": [
    "lioness_obj = Lioness(panda_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Lioness with co-expression matrix\n",
    "Lioness can work with co-expression matrix. To compute Lioness with coexpression matrix, we can set motif data to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading expression data ...\n",
      "Expression matrix: (1000, 50)\n",
      "  Elapsed time: 0.02 sec.\n",
      "Loading PPI data ...\n",
      "Number of PPIs: 238\n",
      "  Elapsed time: 0.01 sec.\n",
      "Calculating coexpression network ...\n",
      "  Elapsed time: 0.03 sec.\n",
      "Returning the correlation matrix of expression data in <Panda_obj>.correlation_matrix\n",
      "Loading input data ...\n",
      "  Elapsed time: 0.00 sec.\n",
      "Running LIONESS for sample 1:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 1 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 2:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.18 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 2 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 3:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 3 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 4:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 4 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 5:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 5 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 6:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 6 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 7:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.09 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 7 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 8:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 8 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 9:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 9 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 10:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 10 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 11:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 11 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 12:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 12 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 13:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 13 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 14:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 14 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 15:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 15 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 16:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 16 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 17:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 17 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 18:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 18 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 19:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 19 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 20:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 20 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 21:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 21 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 22:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 22 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 23:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 23 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 24:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 24 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 25:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 25 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 26:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 26 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 27:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 27 to lioness_output using npy format:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Running LIONESS for sample 28:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 28 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 29:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 29 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LIONESS for sample 30:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 30 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 31:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 31 to lioness_output using npy format:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Running LIONESS for sample 32:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 32 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 33:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 33 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 34:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 34 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 35:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 35 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 36:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 36 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 37:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 37 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 38:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 38 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 39:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 39 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 40:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 40 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 41:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 41 to lioness_output using npy format:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Running LIONESS for sample 42:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 42 to lioness_output using npy format:\n",
      "  Elapsed time: 0.09 sec.\n",
      "Running LIONESS for sample 43:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.07 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 43 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 44:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 44 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 45:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 45 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 46:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 46 to lioness_output using npy format:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Running LIONESS for sample 47:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.05 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 47 to lioness_output using npy format:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Running LIONESS for sample 48:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 48 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 49:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.02 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.04 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 49 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n",
      "Running LIONESS for sample 50:\n",
      "Computing coexpression network:\n",
      "  Elapsed time: 0.03 sec.\n",
      "Normalizing networks:\n",
      "  Elapsed time: 0.06 sec.\n",
      "Inferring LIONESS network:\n",
      "  Elapsed time: 0.00 sec.\n",
      "Saving LIONESS network 50 to lioness_output using npy format:\n",
      "  Elapsed time: 0.08 sec.\n"
     ]
    }
   ],
   "source": [
    "motif = None\n",
    "\n",
    "# Make sure to keep epxression matrix for next step\n",
    "panda_obj = Panda('netZooPy/tests/ToyData/ToyExpressionData.txt',\n",
    "                  None,\n",
    "                  'netZooPy/tests/ToyData/ToyPPIData.txt',\n",
    "                  save_tmp=True,\n",
    "                  remove_missing=False,\n",
    "                  keep_expression_matrix=True)\n",
    "lioness_obj = Lioness(panda_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Lioness results\n",
    "AnalyzeLioness() can be used to visualize lioness network. You may select only the `top` genes to be visualized in the graph. In current version of Lioness. Only the network of the first sample will be visualized using `.top_network_plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "analyze_lioness_obj = AnalyzeLioness(lioness_obj)\n",
    "analyze_lioness_obj.top_network_plot(top = 10, file = \"lioness_top_10.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Lioness results\n",
    "We can save Lioness results by using `save_lioness_results()` method of the `Lioness` object. The edge weights of Lioness predictions will be saved into output file. We can get TF and target IDs from the `.export_panda_results` property of `Panda` object. Each row correspond to a row in the Lioness output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>gene</th>\n",
       "      <th>motif</th>\n",
       "      <th>force</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AHR</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-53.984356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.276521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARID3A</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-64.531519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ARNT</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-70.183704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BRCA1</td>\n",
       "      <td>AACSL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-57.854191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86995</th>\n",
       "      <td>TLX1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.673701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86996</th>\n",
       "      <td>TP53</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.789647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86997</th>\n",
       "      <td>USF1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.855873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86998</th>\n",
       "      <td>VDR</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.885728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86999</th>\n",
       "      <td>YY1</td>\n",
       "      <td>ZWILCH</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-80.408914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf    gene  motif      force\n",
       "0         AHR   AACSL    0.0 -53.984356\n",
       "1          AR   AACSL    0.0  27.276521\n",
       "2      ARID3A   AACSL    1.0 -64.531519\n",
       "3        ARNT   AACSL    1.0 -70.183704\n",
       "4       BRCA1   AACSL    0.0 -57.854191\n",
       "...       ...     ...    ...        ...\n",
       "86995    TLX1  ZWILCH    0.0  15.673701\n",
       "86996    TP53  ZWILCH    0.0  23.789647\n",
       "86997    USF1  ZWILCH    0.0  -6.855873\n",
       "86998     VDR  ZWILCH    0.0  20.885728\n",
       "86999     YY1  ZWILCH    1.0 -80.408914\n",
       "\n",
       "[87000 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panda_obj.export_panda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lioness_obj.save_lioness_results(file = 'lioness.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "Kuijjer ML, Tung MG, Yuan GC, Quackenbush J, Glass K: Estimating Sample-Specific Regulatory Networks. iScience 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
